Total: 14.26s
ROUTINE ======================== github.com/tevino/abool/v2.(*AtomicBool).IsSet in /home/samore/go/pkg/mod/github.com/tevino/abool/v2@v2.1.0/bool.go
     180ms      180ms (flat, cum)  1.26% of Total
         .          .     40:func (ab *AtomicBool) IsSet() bool {
     180ms      180ms     41:	return atomic.LoadInt32((*int32)(ab)) == 1
         .          .     42:}
         .          .     43:
         .          .     44:// IsNotSet returns whether the Boolean is false.
         .          .     45:func (ab *AtomicBool) IsNotSet() bool {
         .          .     46:	return !ab.IsSet()
ROUTINE ======================== indexbytebody in /usr/local/go/src/internal/bytealg/indexbyte_amd64.s
      40ms       40ms (flat, cum)  0.28% of Total
         .          .     27:TEXT	indexbytebody<>(SB), NOSPLIT, $0
         .          .     28:	// Shuffle X0 around so that each byte contains
         .          .     29:	// the character we're looking for.
         .          .     30:	MOVD AX, X0
         .          .     31:	PUNPCKLBW X0, X0
         .          .     32:	PUNPCKLBW X0, X0
         .          .     33:	PSHUFL $0, X0, X0
         .          .     34:
         .          .     35:	CMPQ BX, $16
         .          .     36:	JLT small
         .          .     37:
         .          .     38:	MOVQ SI, DI
         .          .     39:
         .          .     40:	CMPQ BX, $32
         .          .     41:	JA avx2
         .          .     42:sse:
         .          .     43:	LEAQ	-16(SI)(BX*1), AX	// AX = address of last 16 bytes
         .          .     44:	JMP	sseloopentry
         .          .     45:
         .          .     46:sseloop:
         .          .     47:	// Move the next 16-byte chunk of the data into X1.
         .          .     48:	MOVOU	(DI), X1
         .          .     49:	// Compare bytes in X0 to X1.
         .          .     50:	PCMPEQB	X0, X1
         .          .     51:	// Take the top bit of each byte in X1 and put the result in DX.
         .          .     52:	PMOVMSKB X1, DX
         .          .     53:	// Find first set bit, if any.
         .          .     54:	BSFL	DX, DX
         .          .     55:	JNZ	ssesuccess
         .          .     56:	// Advance to next block.
         .          .     57:	ADDQ	$16, DI
         .          .     58:sseloopentry:
         .          .     59:	CMPQ	DI, AX
         .          .     60:	JB	sseloop
         .          .     61:
         .          .     62:	// Search the last 16-byte chunk. This chunk may overlap with the
         .          .     63:	// chunks we've already searched, but that's ok.
         .          .     64:	MOVQ	AX, DI
         .          .     65:	MOVOU	(AX), X1
         .          .     66:	PCMPEQB	X0, X1
         .          .     67:	PMOVMSKB X1, DX
         .          .     68:	BSFL	DX, DX
         .          .     69:	JNZ	ssesuccess
         .          .     70:
         .          .     71:failure:
         .          .     72:	MOVQ $-1, (R8)
         .          .     73:	RET
         .          .     74:
         .          .     75:// We've found a chunk containing the byte.
         .          .     76:// The chunk was loaded from DI.
         .          .     77:// The index of the matching byte in the chunk is DX.
         .          .     78:// The start of the data is SI.
         .          .     79:ssesuccess:
         .          .     80:	SUBQ SI, DI	// Compute offset of chunk within data.
         .          .     81:	ADDQ DX, DI	// Add offset of byte within chunk.
         .          .     82:	MOVQ DI, (R8)
         .          .     83:	RET
         .          .     84:
         .          .     85:// handle for lengths < 16
         .          .     86:small:
         .          .     87:	TESTQ	BX, BX
         .          .     88:	JEQ	failure
         .          .     89:
         .          .     90:	// Check if we'll load across a page boundary.
         .          .     91:	LEAQ	16(SI), AX
         .          .     92:	TESTW	$0xff0, AX
         .          .     93:	JEQ	endofpage
         .          .     94:
         .          .     95:	MOVOU	(SI), X1 // Load data
         .          .     96:	PCMPEQB	X0, X1	// Compare target byte with each byte in data.
         .          .     97:	PMOVMSKB X1, DX	// Move result bits to integer register.
         .          .     98:	BSFL	DX, DX	// Find first set bit.
         .          .     99:	JZ	failure	// No set bit, failure.
         .          .    100:	CMPL	DX, BX
         .          .    101:	JAE	failure	// Match is past end of data.
         .          .    102:	MOVQ	DX, (R8)
         .          .    103:	RET
         .          .    104:
         .          .    105:endofpage:
         .          .    106:	MOVOU	-16(SI)(BX*1), X1	// Load data into the high end of X1.
         .          .    107:	PCMPEQB	X0, X1	// Compare target byte with each byte in data.
         .          .    108:	PMOVMSKB X1, DX	// Move result bits to integer register.
         .          .    109:	MOVL	BX, CX
         .          .    110:	SHLL	CX, DX
         .          .    111:	SHRL	$16, DX	// Shift desired bits down to bottom of register.
         .          .    112:	BSFL	DX, DX	// Find first set bit.
         .          .    113:	JZ	failure	// No set bit, failure.
         .          .    114:	MOVQ	DX, (R8)
         .          .    115:	RET
         .          .    116:
         .          .    117:avx2:
         .          .    118:#ifndef hasAVX2
         .          .    119:	CMPB   internal∕cpu·X86+const_offsetX86HasAVX2(SB), $1
      10ms       10ms    120:	JNE sse
         .          .    121:#endif
         .          .    122:	MOVD AX, X0
         .          .    123:	LEAQ -32(SI)(BX*1), R11
         .          .    124:	VPBROADCASTB  X0, Y1
         .          .    125:avx2_loop:
         .          .    126:	VMOVDQU (DI), Y2
         .          .    127:	VPCMPEQB Y1, Y2, Y3
         .          .    128:	VPTEST Y3, Y3
      10ms       10ms    129:	JNZ avx2success
         .          .    130:	ADDQ $32, DI
         .          .    131:	CMPQ DI, R11
         .          .    132:	JLT avx2_loop
         .          .    133:	MOVQ R11, DI
         .          .    134:	VMOVDQU (DI), Y2
         .          .    135:	VPCMPEQB Y1, Y2, Y3
         .          .    136:	VPTEST Y3, Y3
         .          .    137:	JNZ avx2success
         .          .    138:	VZEROUPPER
         .          .    139:	MOVQ $-1, (R8)
         .          .    140:	RET
         .          .    141:
         .          .    142:avx2success:
      20ms       20ms    143:	VPMOVMSKB Y3, DX
         .          .    144:	BSFL DX, DX
         .          .    145:	SUBQ SI, DI
         .          .    146:	ADDQ DI, DX
         .          .    147:	MOVQ DX, (R8)
         .          .    148:	VZEROUPPER
ROUTINE ======================== main.BinarySearch in /home/samore/go-path/main.go
     8.78s      8.78s (flat, cum) 61.57% of Total
         .          .     20:func BinarySearch(a uint32, x uint32) bool {
         .          .     21:	start := 0
     2.98s      2.98s     22:	end := len(outgoingLinks[a]) - 1
     290ms      290ms     23:	for start <= end {
     100ms      100ms     24:		mid := (start + end) / 2
     5.21s      5.21s     25:		if outgoingLinks[a][mid] == x {
         .          .     26:			return true
         .          .     27:		} else if outgoingLinks[a][mid] < x {
     170ms      170ms     28:			start = mid + 1
         .          .     29:		} else if outgoingLinks[a][mid] > x {
      30ms       30ms     30:			end = mid - 1
         .          .     31:		}
         .          .     32:	}
         .          .     33:	return false
         .          .     34:}
         .          .     35:
ROUTINE ======================== main.findOutgoingLink in /home/samore/go-path/main.go
     620ms     22.22s (flat, cum) 155.82% of Total
      30ms       30ms     41:func findOutgoingLink(startLink uint32, goal uint32, currentDepth uint32, returnCh chan<- uint32, stopCh <-chan struct{}, isClosed *abool.AtomicBool) {
         .       30ms     42:	if isClosed.IsSet() {
     250ms      250ms     43:		return
         .          .     44:	}
         .          .     45:	send := func(data uint32) {
         .          .     46:		select {
         .          .     47:		case <-stopCh:
         .          .     48:			return
         .          .     49:		case returnCh <- data:
         .          .     50:		}
         .          .     51:	}
         .          .     52:
         .          .     53:	// visited = append(visited, startLink)
         .      8.78s     54:	if hasNeighbour(startLink, goal) {
         .      350ms     55:		send(startLink)
         .          .     56:		return
         .          .     57:	}
         .          .     58:
     180ms      180ms     59:	if currentDepth == MAX_DEPTH {
      90ms       90ms     60:		return
         .          .     61:	}
         .          .     62:
      40ms       40ms     63:	for _, l := range outgoingLinks[startLink] {
         .      150ms     64:		if isClosed.IsSet() {
         .          .     65:			return
         .          .     66:		}
         .          .     67:
         .          .     68:		if currentDepth+2 == MAX_DEPTH {
         .      1.43s     69:			go findOutgoingLink(l, goal, currentDepth+1, returnCh, stopCh, isClosed)
         .          .     70:		} else {
      20ms     10.88s     71:			findOutgoingLink(l, goal, currentDepth+1, returnCh, stopCh, isClosed)
         .          .     72:		}
         .          .     73:
         .          .     74:	}
      10ms       10ms     75:}
         .          .     76:
         .          .     77:func main() {
         .          .     78:	defer utils.Timer("main")()
         .          .     79:	println("Starts reading file.")
         .          .     80:	bytes := file.ReadFile()
ROUTINE ======================== main.findOutgoingLink.func1 in /home/samore/go-path/main.go
         0      350ms (flat, cum)  2.45% of Total
         .          .     45:	send := func(data uint32) {
         .      350ms     46:		select {
         .          .     47:		case <-stopCh:
         .          .     48:			return
         .          .     49:		case returnCh <- data:
         .          .     50:		}
         .          .     51:	}
ROUTINE ======================== main.hasNeighbour in /home/samore/go-path/main.go
         0      8.78s (flat, cum) 61.57% of Total
         .          .     36:func hasNeighbour(startLink, goal uint32) bool {
         .          .     37:	// outgoingNeighbours := outgoingLinks[startLink]
         .      8.78s     38:	return BinarySearch(startLink, goal)
         .          .     39:}
         .          .     40:
         .          .     41:func findOutgoingLink(startLink uint32, goal uint32, currentDepth uint32, returnCh chan<- uint32, stopCh <-chan struct{}, isClosed *abool.AtomicBool) {
         .          .     42:	if isClosed.IsSet() {
         .          .     43:		return
ROUTINE ======================== main.main in /home/samore/go-path/main.go
         0       10ms (flat, cum)  0.07% of Total
         .          .     77:func main() {
         .          .     78:	defer utils.Timer("main")()
         .          .     79:	println("Starts reading file.")
         .          .     80:	bytes := file.ReadFile()
         .          .     81:	println("Finished reading file.")
         .          .     82:	outgoingLinks = file.UnmarshalMessage(bytes)
         .          .     83:	println("Finished json parse.")
         .          .     84:
         .          .     85:	i := 1
         .          .     86:
         .          .     87:	f, _ := os.Create("cpu_profile.prof")
         .          .     88:	pprof.StartCPUProfile(f)
         .          .     89:	defer pprof.StopCPUProfile()
         .          .     90:
         .          .     91:	defer utils.Timer("0-50")()
         .          .     92:	for i < 5000 {
         .          .     93:		cond := abool.New()
         .          .     94:		nodeCh := make(chan uint32)
         .          .     95:		stopCh := make(chan struct{})
         .          .     96:
         .          .     97:		go findOutgoingLink(0, uint32(i), 0, nodeCh, stopCh, cond)
         .          .     98:
         .          .     99:		node := <-nodeCh
         .          .    100:		cond.Set()
         .       10ms    101:		close(stopCh)
         .          .    102:
         .          .    103:		if node != MAX_VAL {
         .          .    104:			fmt.Printf("Found response %v -> %v \n", i, node)
         .          .    105:			// if !slices.Contains(outgoingLinks[node], uint32(i)) {
         .          .    106:			// 	fmt.Printf("node %v does not have an outgoing link to %v", node, i)
ROUTINE ======================== runtime.(*gList).pop in /usr/local/go/src/runtime/proc.go
     660ms      660ms (flat, cum)  4.63% of Total
         .          .   6544:func (l *gList) pop() *g {
         .          .   6545:	gp := l.head.ptr()
         .          .   6546:	if gp != nil {
     660ms      660ms   6547:		l.head = gp.schedlink
         .          .   6548:	}
         .          .   6549:	return gp
         .          .   6550:}
         .          .   6551:
         .          .   6552://go:linkname setMaxThreads runtime/debug.setMaxThreads
ROUTINE ======================== runtime.(*gQueue).pop in /usr/local/go/src/runtime/proc.go
     460ms      460ms (flat, cum)  3.23% of Total
         .          .   6500:func (q *gQueue) pop() *g {
         .          .   6501:	gp := q.head.ptr()
         .          .   6502:	if gp != nil {
     460ms      460ms   6503:		q.head = gp.schedlink
         .          .   6504:		if q.head == 0 {
         .          .   6505:			q.tail = 0
         .          .   6506:		}
         .          .   6507:	}
         .          .   6508:	return gp
ROUTINE ======================== runtime.(*gcBits).bytep in /usr/local/go/src/runtime/mheap.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .   2089:func (b *gcBits) bytep(n uintptr) *uint8 {
      10ms       10ms   2090:	return addb(&b.x, n)
         .          .   2091:}
         .          .   2092:
         .          .   2093:// bitp returns a pointer to the byte containing bit n and a mask for
         .          .   2094:// selecting that bit from *bytep.
         .          .   2095:func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8) {
ROUTINE ======================== runtime.(*gcControllerState).addScannableStack in /usr/local/go/src/runtime/mgcpacer.go
      50ms       60ms (flat, cum)  0.42% of Total
         .          .    877:func (c *gcControllerState) addScannableStack(pp *p, amount int64) {
         .          .    878:	if pp == nil {
         .          .    879:		c.maxStackScan.Add(amount)
         .          .    880:		return
         .          .    881:	}
         .          .    882:	pp.maxStackScanDelta += amount
         .          .    883:	if pp.maxStackScanDelta >= maxStackScanSlack || pp.maxStackScanDelta <= -maxStackScanSlack {
         .       10ms    884:		c.maxStackScan.Add(pp.maxStackScanDelta)
      50ms       50ms    885:		pp.maxStackScanDelta = 0
         .          .    886:	}
         .          .    887:}
         .          .    888:
         .          .    889:func (c *gcControllerState) addGlobals(amount int64) {
         .          .    890:	c.globalsScan.Add(amount)
ROUTINE ======================== runtime.(*gcControllerState).enlistWorker in /usr/local/go/src/runtime/mgcpacer.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    691:func (c *gcControllerState) enlistWorker() {
         .          .    692:	// If there are idle Ps, wake one so it will run an idle worker.
         .          .    693:	// NOTE: This is suspected of causing deadlocks. See golang.org/issue/19112.
         .          .    694:	//
         .          .    695:	//	if sched.npidle.Load() != 0 && sched.nmspinning.Load() == 0 {
         .          .    696:	//		wakep()
         .          .    697:	//		return
         .          .    698:	//	}
         .          .    699:
         .          .    700:	// There are no idle Ps. If we need more dedicated workers,
         .          .    701:	// try to preempt a running P so it will switch to a worker.
         .          .    702:	if c.dedicatedMarkWorkersNeeded.Load() <= 0 {
         .          .    703:		return
         .          .    704:	}
         .          .    705:	// Pick a random other P to preempt.
         .          .    706:	if gomaxprocs <= 1 {
         .          .    707:		return
         .          .    708:	}
         .          .    709:	gp := getg()
         .          .    710:	if gp == nil || gp.m == nil || gp.m.p == 0 {
         .          .    711:		return
         .          .    712:	}
         .          .    713:	myID := gp.m.p.ptr().id
         .          .    714:	for tries := 0; tries < 5; tries++ {
         .          .    715:		id := int32(fastrandn(uint32(gomaxprocs - 1)))
         .          .    716:		if id >= myID {
         .          .    717:			id++
         .          .    718:		}
         .          .    719:		p := allp[id]
         .          .    720:		if p.status != _Prunning {
         .          .    721:			continue
         .          .    722:		}
         .       10ms    723:		if preemptone(p) {
         .          .    724:			return
         .          .    725:		}
         .          .    726:	}
         .          .    727:}
         .          .    728:
ROUTINE ======================== runtime.(*gcWork).balance in /usr/local/go/src/runtime/mgcwork.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    288:func (w *gcWork) balance() {
         .          .    289:	if w.wbuf1 == nil {
         .          .    290:		return
         .          .    291:	}
         .          .    292:	if wbuf := w.wbuf2; wbuf.nobj != 0 {
         .          .    293:		putfull(wbuf)
         .          .    294:		w.flushedWork = true
         .          .    295:		w.wbuf2 = getempty()
         .          .    296:	} else if wbuf := w.wbuf1; wbuf.nobj > 4 {
         .          .    297:		w.wbuf1 = handoff(wbuf)
         .          .    298:		w.flushedWork = true // handoff did putfull
         .          .    299:	} else {
         .          .    300:		return
         .          .    301:	}
         .          .    302:	// We flushed a buffer to the full list, so wake a worker.
         .          .    303:	if gcphase == _GCmark {
         .       10ms    304:		gcController.enlistWorker()
         .          .    305:	}
         .          .    306:}
         .          .    307:
         .          .    308:// empty reports whether w has no mark work available.
         .          .    309://
ROUTINE ======================== runtime.(*guintptr).cas in /usr/local/go/src/runtime/runtime2.go
      60ms       60ms (flat, cum)  0.42% of Total
         .          .    271:func (gp *guintptr) cas(old, new guintptr) bool {
      60ms       60ms    272:	return atomic.Casuintptr((*uintptr)(unsafe.Pointer(gp)), uintptr(old), uintptr(new))
         .          .    273:}
         .          .    274:
         .          .    275://go:nosplit
         .          .    276:func (gp *g) guintptr() guintptr {
         .          .    277:	return guintptr(unsafe.Pointer(gp))
ROUTINE ======================== runtime.(*mcache).nextFree in /usr/local/go/src/runtime/malloc.go
         0       40ms (flat, cum)  0.28% of Total
         .          .    915:func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool) {
         .          .    916:	s = c.alloc[spc]
         .          .    917:	shouldhelpgc = false
         .          .    918:	freeIndex := s.nextFreeIndex()
         .          .    919:	if freeIndex == s.nelems {
         .          .    920:		// The span is full.
         .          .    921:		if uintptr(s.allocCount) != s.nelems {
         .          .    922:			println("runtime: s.allocCount=", s.allocCount, "s.nelems=", s.nelems)
         .          .    923:			throw("s.allocCount != s.nelems && freeIndex == s.nelems")
         .          .    924:		}
         .       40ms    925:		c.refill(spc)
         .          .    926:		shouldhelpgc = true
         .          .    927:		s = c.alloc[spc]
         .          .    928:
         .          .    929:		freeIndex = s.nextFreeIndex()
         .          .    930:	}
ROUTINE ======================== runtime.(*mcache).refill in /usr/local/go/src/runtime/mcache.go
         0       40ms (flat, cum)  0.28% of Total
         .          .    147:func (c *mcache) refill(spc spanClass) {
         .          .    148:	// Return the current cached span to the central lists.
         .          .    149:	s := c.alloc[spc]
         .          .    150:
         .          .    151:	if uintptr(s.allocCount) != s.nelems {
         .          .    152:		throw("refill of span with free space remaining")
         .          .    153:	}
         .          .    154:	if s != &emptymspan {
         .          .    155:		// Mark this span as no longer cached.
         .          .    156:		if s.sweepgen != mheap_.sweepgen+3 {
         .          .    157:			throw("bad sweepgen in refill")
         .          .    158:		}
         .          .    159:		mheap_.central[spc].mcentral.uncacheSpan(s)
         .          .    160:
         .          .    161:		// Count up how many slots were used and record it.
         .          .    162:		stats := memstats.heapStats.acquire()
         .          .    163:		slotsUsed := int64(s.allocCount) - int64(s.allocCountBeforeCache)
         .          .    164:		atomic.Xadd64(&stats.smallAllocCount[spc.sizeclass()], slotsUsed)
         .          .    165:
         .          .    166:		// Flush tinyAllocs.
         .          .    167:		if spc == tinySpanClass {
         .          .    168:			atomic.Xadd64(&stats.tinyAllocCount, int64(c.tinyAllocs))
         .          .    169:			c.tinyAllocs = 0
         .          .    170:		}
         .          .    171:		memstats.heapStats.release()
         .          .    172:
         .          .    173:		// Count the allocs in inconsistent, internal stats.
         .          .    174:		bytesAllocated := slotsUsed * int64(s.elemsize)
         .          .    175:		gcController.totalAlloc.Add(bytesAllocated)
         .          .    176:
         .          .    177:		// Clear the second allocCount just to be safe.
         .          .    178:		s.allocCountBeforeCache = 0
         .          .    179:	}
         .          .    180:
         .          .    181:	// Get a new cached span from the central lists.
         .       40ms    182:	s = mheap_.central[spc].mcentral.cacheSpan()
         .          .    183:	if s == nil {
         .          .    184:		throw("out of memory")
         .          .    185:	}
         .          .    186:
         .          .    187:	if uintptr(s.allocCount) == s.nelems {
ROUTINE ======================== runtime.(*mcentral).cacheSpan in /usr/local/go/src/runtime/mcentral.go
      10ms       40ms (flat, cum)  0.28% of Total
         .          .     81:func (c *mcentral) cacheSpan() *mspan {
         .          .     82:	// Deduct credit for this span allocation and sweep if necessary.
         .          .     83:	spanBytes := uintptr(class_to_allocnpages[c.spanclass.sizeclass()]) * _PageSize
         .          .     84:	deductSweepCredit(spanBytes, 0)
         .          .     85:
         .          .     86:	traceDone := false
         .          .     87:	if traceEnabled() {
         .          .     88:		traceGCSweepStart()
         .          .     89:	}
         .          .     90:
         .          .     91:	// If we sweep spanBudget spans without finding any free
         .          .     92:	// space, just allocate a fresh span. This limits the amount
         .          .     93:	// of time we can spend trying to find free space and
         .          .     94:	// amortizes the cost of small object sweeping over the
         .          .     95:	// benefit of having a full free span to allocate from. By
         .          .     96:	// setting this to 100, we limit the space overhead to 1%.
         .          .     97:	//
         .          .     98:	// TODO(austin,mknyszek): This still has bad worst-case
         .          .     99:	// throughput. For example, this could find just one free slot
         .          .    100:	// on the 100th swept span. That limits allocation latency, but
         .          .    101:	// still has very poor throughput. We could instead keep a
         .          .    102:	// running free-to-used budget and switch to fresh span
         .          .    103:	// allocation if the budget runs low.
         .          .    104:	spanBudget := 100
         .          .    105:
         .          .    106:	var s *mspan
         .          .    107:	var sl sweepLocker
         .          .    108:
         .          .    109:	// Try partial swept spans first.
         .          .    110:	sg := mheap_.sweepgen
         .          .    111:	if s = c.partialSwept(sg).pop(); s != nil {
         .          .    112:		goto havespan
         .          .    113:	}
         .          .    114:
         .          .    115:	sl = sweep.active.begin()
         .          .    116:	if sl.valid {
         .          .    117:		// Now try partial unswept spans.
         .          .    118:		for ; spanBudget >= 0; spanBudget-- {
         .          .    119:			s = c.partialUnswept(sg).pop()
         .          .    120:			if s == nil {
         .          .    121:				break
         .          .    122:			}
         .          .    123:			if s, ok := sl.tryAcquire(s); ok {
         .          .    124:				// We got ownership of the span, so let's sweep it and use it.
         .          .    125:				s.sweep(true)
         .          .    126:				sweep.active.end(sl)
         .          .    127:				goto havespan
         .          .    128:			}
         .          .    129:			// We failed to get ownership of the span, which means it's being or
         .          .    130:			// has been swept by an asynchronous sweeper that just couldn't remove it
         .          .    131:			// from the unswept list. That sweeper took ownership of the span and
         .          .    132:			// responsibility for either freeing it to the heap or putting it on the
         .          .    133:			// right swept list. Either way, we should just ignore it (and it's unsafe
         .          .    134:			// for us to do anything else).
         .          .    135:		}
         .          .    136:		// Now try full unswept spans, sweeping them and putting them into the
         .          .    137:		// right list if we fail to get a span.
         .          .    138:		for ; spanBudget >= 0; spanBudget-- {
         .          .    139:			s = c.fullUnswept(sg).pop()
         .          .    140:			if s == nil {
         .          .    141:				break
         .          .    142:			}
         .          .    143:			if s, ok := sl.tryAcquire(s); ok {
         .          .    144:				// We got ownership of the span, so let's sweep it.
         .          .    145:				s.sweep(true)
         .          .    146:				// Check if there's any free space.
         .          .    147:				freeIndex := s.nextFreeIndex()
         .          .    148:				if freeIndex != s.nelems {
         .          .    149:					s.freeindex = freeIndex
         .          .    150:					sweep.active.end(sl)
         .          .    151:					goto havespan
         .          .    152:				}
         .          .    153:				// Add it to the swept list, because sweeping didn't give us any free space.
         .          .    154:				c.fullSwept(sg).push(s.mspan)
         .          .    155:			}
         .          .    156:			// See comment for partial unswept spans.
         .          .    157:		}
         .          .    158:		sweep.active.end(sl)
         .          .    159:	}
         .          .    160:	if traceEnabled() {
         .          .    161:		traceGCSweepDone()
         .          .    162:		traceDone = true
         .          .    163:	}
         .          .    164:
         .          .    165:	// We failed to get a span from the mcentral so get one from mheap.
         .       10ms    166:	s = c.grow()
         .          .    167:	if s == nil {
         .          .    168:		return nil
         .          .    169:	}
         .          .    170:
         .          .    171:	// At this point s is a span that should have free slots.
         .          .    172:havespan:
         .       10ms    173:	if traceEnabled() && !traceDone {
         .          .    174:		traceGCSweepDone()
         .          .    175:	}
      10ms       10ms    176:	n := int(s.nelems) - int(s.allocCount)
         .          .    177:	if n == 0 || s.freeindex == s.nelems || uintptr(s.allocCount) == s.nelems {
         .          .    178:		throw("span has no free objects")
         .          .    179:	}
         .          .    180:	freeByteBase := s.freeindex &^ (64 - 1)
         .          .    181:	whichByte := freeByteBase / 8
         .          .    182:	// Init alloc bits cache.
         .       10ms    183:	s.refillAllocCache(whichByte)
         .          .    184:
         .          .    185:	// Adjust the allocCache so that s.freeindex corresponds to the low bit in
         .          .    186:	// s.allocCache.
         .          .    187:	s.allocCache >>= s.freeindex % 64
         .          .    188:
ROUTINE ======================== runtime.(*mcentral).grow in /usr/local/go/src/runtime/mcentral.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    242:func (c *mcentral) grow() *mspan {
         .          .    243:	npages := uintptr(class_to_allocnpages[c.spanclass.sizeclass()])
         .          .    244:	size := uintptr(class_to_size[c.spanclass.sizeclass()])
         .          .    245:
         .       10ms    246:	s := mheap_.alloc(npages, c.spanclass)
         .          .    247:	if s == nil {
         .          .    248:		return nil
         .          .    249:	}
         .          .    250:
         .          .    251:	// Use division by multiplication and shifts to quickly compute:
ROUTINE ======================== runtime.(*mheap).alloc in /usr/local/go/src/runtime/mheap.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    957:func (h *mheap) alloc(npages uintptr, spanclass spanClass) *mspan {
         .          .    958:	// Don't do any operations that lock the heap on the G stack.
         .          .    959:	// It might trigger stack growth, and the stack growth code needs
         .          .    960:	// to be able to allocate heap.
         .          .    961:	var s *mspan
         .       10ms    962:	systemstack(func() {
         .          .    963:		// To prevent excessive heap growth, before allocating n pages
         .          .    964:		// we need to sweep and reclaim at least n pages.
         .          .    965:		if !isSweepDone() {
         .          .    966:			h.reclaim(npages)
         .          .    967:		}
ROUTINE ======================== runtime.(*mheap).alloc.func1 in /usr/local/go/src/runtime/mheap.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    962:	systemstack(func() {
         .          .    963:		// To prevent excessive heap growth, before allocating n pages
         .          .    964:		// we need to sweep and reclaim at least n pages.
         .          .    965:		if !isSweepDone() {
         .          .    966:			h.reclaim(npages)
         .          .    967:		}
         .       10ms    968:		s = h.allocSpan(npages, spanAllocHeap, spanclass)
         .          .    969:	})
         .          .    970:	return s
         .          .    971:}
         .          .    972:
         .          .    973:// allocManual allocates a manually-managed span of npage pages.
ROUTINE ======================== runtime.(*mheap).allocSpan in /usr/local/go/src/runtime/mheap.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   1175:func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan) {
         .          .   1176:	// Function-global state.
         .          .   1177:	gp := getg()
         .          .   1178:	base, scav := uintptr(0), uintptr(0)
         .          .   1179:	growth := uintptr(0)
         .          .   1180:
         .          .   1181:	// On some platforms we need to provide physical page aligned stack
         .          .   1182:	// allocations. Where the page size is less than the physical page
         .          .   1183:	// size, we already manage to do this by default.
         .          .   1184:	needPhysPageAlign := physPageAlignedStacks && typ == spanAllocStack && pageSize < physPageSize
         .          .   1185:
         .          .   1186:	// If the allocation is small enough, try the page cache!
         .          .   1187:	// The page cache does not support aligned allocations, so we cannot use
         .          .   1188:	// it if we need to provide a physical page aligned stack allocation.
         .          .   1189:	pp := gp.m.p.ptr()
         .          .   1190:	if !needPhysPageAlign && pp != nil && npages < pageCachePages/4 {
         .          .   1191:		c := &pp.pcache
         .          .   1192:
         .          .   1193:		// If the cache is empty, refill it.
         .          .   1194:		if c.empty() {
         .          .   1195:			lock(&h.lock)
         .          .   1196:			*c = h.pages.allocToCache()
         .          .   1197:			unlock(&h.lock)
         .          .   1198:		}
         .          .   1199:
         .          .   1200:		// Try to allocate from the cache.
         .          .   1201:		base, scav = c.alloc(npages)
         .          .   1202:		if base != 0 {
         .          .   1203:			s = h.tryAllocMSpan()
         .          .   1204:			if s != nil {
         .          .   1205:				goto HaveSpan
         .          .   1206:			}
         .          .   1207:			// We have a base but no mspan, so we need
         .          .   1208:			// to lock the heap.
         .          .   1209:		}
         .          .   1210:	}
         .          .   1211:
         .          .   1212:	// For one reason or another, we couldn't get the
         .          .   1213:	// whole job done without the heap lock.
         .          .   1214:	lock(&h.lock)
         .          .   1215:
         .          .   1216:	if needPhysPageAlign {
         .          .   1217:		// Overallocate by a physical page to allow for later alignment.
         .          .   1218:		extraPages := physPageSize / pageSize
         .          .   1219:
         .          .   1220:		// Find a big enough region first, but then only allocate the
         .          .   1221:		// aligned portion. We can't just allocate and then free the
         .          .   1222:		// edges because we need to account for scavenged memory, and
         .          .   1223:		// that's difficult with alloc.
         .          .   1224:		//
         .          .   1225:		// Note that we skip updates to searchAddr here. It's OK if
         .          .   1226:		// it's stale and higher than normal; it'll operate correctly,
         .          .   1227:		// just come with a performance cost.
         .          .   1228:		base, _ = h.pages.find(npages + extraPages)
         .          .   1229:		if base == 0 {
         .          .   1230:			var ok bool
         .          .   1231:			growth, ok = h.grow(npages + extraPages)
         .          .   1232:			if !ok {
         .          .   1233:				unlock(&h.lock)
         .          .   1234:				return nil
         .          .   1235:			}
         .          .   1236:			base, _ = h.pages.find(npages + extraPages)
         .          .   1237:			if base == 0 {
         .          .   1238:				throw("grew heap, but no adequate free space found")
         .          .   1239:			}
         .          .   1240:		}
         .          .   1241:		base = alignUp(base, physPageSize)
         .          .   1242:		scav = h.pages.allocRange(base, npages)
         .          .   1243:	}
         .          .   1244:
         .          .   1245:	if base == 0 {
         .          .   1246:		// Try to acquire a base address.
         .          .   1247:		base, scav = h.pages.alloc(npages)
         .          .   1248:		if base == 0 {
         .          .   1249:			var ok bool
         .          .   1250:			growth, ok = h.grow(npages)
         .          .   1251:			if !ok {
         .          .   1252:				unlock(&h.lock)
         .          .   1253:				return nil
         .          .   1254:			}
         .          .   1255:			base, scav = h.pages.alloc(npages)
         .          .   1256:			if base == 0 {
         .          .   1257:				throw("grew heap, but no adequate free space found")
         .          .   1258:			}
         .          .   1259:		}
         .          .   1260:	}
         .          .   1261:	if s == nil {
         .          .   1262:		// We failed to get an mspan earlier, so grab
         .          .   1263:		// one now that we have the heap lock.
         .          .   1264:		s = h.allocMSpanLocked()
         .          .   1265:	}
         .          .   1266:	unlock(&h.lock)
         .          .   1267:
         .          .   1268:HaveSpan:
         .          .   1269:	// Decide if we need to scavenge in response to what we just allocated.
         .          .   1270:	// Specifically, we track the maximum amount of memory to scavenge of all
         .          .   1271:	// the alternatives below, assuming that the maximum satisfies *all*
         .          .   1272:	// conditions we check (e.g. if we need to scavenge X to satisfy the
         .          .   1273:	// memory limit and Y to satisfy heap-growth scavenging, and Y > X, then
         .          .   1274:	// it's fine to pick Y, because the memory limit is still satisfied).
         .          .   1275:	//
         .          .   1276:	// It's fine to do this after allocating because we expect any scavenged
         .          .   1277:	// pages not to get touched until we return. Simultaneously, it's important
         .          .   1278:	// to do this before calling sysUsed because that may commit address space.
         .          .   1279:	bytesToScavenge := uintptr(0)
         .          .   1280:	forceScavenge := false
         .          .   1281:	if limit := gcController.memoryLimit.Load(); !gcCPULimiter.limiting() {
         .          .   1282:		// Assist with scavenging to maintain the memory limit by the amount
         .          .   1283:		// that we expect to page in.
         .          .   1284:		inuse := gcController.mappedReady.Load()
         .          .   1285:		// Be careful about overflow, especially with uintptrs. Even on 32-bit platforms
         .          .   1286:		// someone can set a really big memory limit that isn't maxInt64.
         .          .   1287:		if uint64(scav)+inuse > uint64(limit) {
         .          .   1288:			bytesToScavenge = uintptr(uint64(scav) + inuse - uint64(limit))
         .          .   1289:			forceScavenge = true
         .          .   1290:		}
         .          .   1291:	}
         .          .   1292:	if goal := scavenge.gcPercentGoal.Load(); goal != ^uint64(0) && growth > 0 {
         .          .   1293:		// We just caused a heap growth, so scavenge down what will soon be used.
         .          .   1294:		// By scavenging inline we deal with the failure to allocate out of
         .          .   1295:		// memory fragments by scavenging the memory fragments that are least
         .          .   1296:		// likely to be re-used.
         .          .   1297:		//
         .          .   1298:		// Only bother with this because we're not using a memory limit. We don't
         .          .   1299:		// care about heap growths as long as we're under the memory limit, and the
         .          .   1300:		// previous check for scaving already handles that.
         .          .   1301:		if retained := heapRetained(); retained+uint64(growth) > goal {
         .          .   1302:			// The scavenging algorithm requires the heap lock to be dropped so it
         .          .   1303:			// can acquire it only sparingly. This is a potentially expensive operation
         .          .   1304:			// so it frees up other goroutines to allocate in the meanwhile. In fact,
         .          .   1305:			// they can make use of the growth we just created.
         .          .   1306:			todo := growth
         .          .   1307:			if overage := uintptr(retained + uint64(growth) - goal); todo > overage {
         .          .   1308:				todo = overage
         .          .   1309:			}
         .          .   1310:			if todo > bytesToScavenge {
         .          .   1311:				bytesToScavenge = todo
         .          .   1312:			}
         .          .   1313:		}
         .          .   1314:	}
         .          .   1315:	// There are a few very limited circumstances where we won't have a P here.
         .          .   1316:	// It's OK to simply skip scavenging in these cases. Something else will notice
         .          .   1317:	// and pick up the tab.
         .          .   1318:	var now int64
         .          .   1319:	if pp != nil && bytesToScavenge > 0 {
         .          .   1320:		// Measure how long we spent scavenging and add that measurement to the assist
         .          .   1321:		// time so we can track it for the GC CPU limiter.
         .          .   1322:		//
         .          .   1323:		// Limiter event tracking might be disabled if we end up here
         .          .   1324:		// while on a mark worker.
         .          .   1325:		start := nanotime()
         .          .   1326:		track := pp.limiterEvent.start(limiterEventScavengeAssist, start)
         .          .   1327:
         .          .   1328:		// Scavenge, but back out if the limiter turns on.
         .          .   1329:		released := h.pages.scavenge(bytesToScavenge, func() bool {
         .          .   1330:			return gcCPULimiter.limiting()
         .          .   1331:		}, forceScavenge)
         .          .   1332:
         .          .   1333:		mheap_.pages.scav.releasedEager.Add(released)
         .          .   1334:
         .          .   1335:		// Finish up accounting.
         .          .   1336:		now = nanotime()
         .          .   1337:		if track {
         .          .   1338:			pp.limiterEvent.stop(limiterEventScavengeAssist, now)
         .          .   1339:		}
         .          .   1340:		scavenge.assistTime.Add(now - start)
         .          .   1341:	}
         .          .   1342:
         .          .   1343:	// Initialize the span.
         .       10ms   1344:	h.initSpan(s, typ, spanclass, base, npages)
         .          .   1345:
         .          .   1346:	// Commit and account for any scavenged memory that the span now owns.
         .          .   1347:	nbytes := npages * pageSize
         .          .   1348:	if scav != 0 {
         .          .   1349:		// sysUsed all the pages that are actually available
ROUTINE ======================== runtime.(*mheap).freeSpan in /usr/local/go/src/runtime/mheap.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   1549:func (h *mheap) freeSpan(s *mspan) {
         .       10ms   1550:	systemstack(func() {
         .          .   1551:		pageTraceFree(getg().m.p.ptr(), 0, s.base(), s.npages)
         .          .   1552:
         .          .   1553:		lock(&h.lock)
         .          .   1554:		if msanenabled {
         .          .   1555:			// Tell msan that this entire span is no longer in use.
ROUTINE ======================== runtime.(*mheap).freeSpanLocked in /usr/local/go/src/runtime/mheap.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   1591:func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType) {
         .          .   1592:	assertLockHeld(&h.lock)
         .          .   1593:
         .          .   1594:	switch s.state.get() {
         .          .   1595:	case mSpanManual:
         .          .   1596:		if s.allocCount != 0 {
         .          .   1597:			throw("mheap.freeSpanLocked - invalid stack free")
         .          .   1598:		}
         .          .   1599:	case mSpanInUse:
         .          .   1600:		if s.isUserArenaChunk {
         .          .   1601:			throw("mheap.freeSpanLocked - invalid free of user arena chunk")
         .          .   1602:		}
         .          .   1603:		if s.allocCount != 0 || s.sweepgen != h.sweepgen {
         .          .   1604:			print("mheap.freeSpanLocked - span ", s, " ptr ", hex(s.base()), " allocCount ", s.allocCount, " sweepgen ", s.sweepgen, "/", h.sweepgen, "\n")
         .          .   1605:			throw("mheap.freeSpanLocked - invalid free")
         .          .   1606:		}
         .          .   1607:		h.pagesInUse.Add(-s.npages)
         .          .   1608:
         .          .   1609:		// Clear in-use bit in arena page bitmap.
         .          .   1610:		arena, pageIdx, pageMask := pageIndexOf(s.base())
         .          .   1611:		atomic.And8(&arena.pageInUse[pageIdx], ^pageMask)
         .          .   1612:	default:
         .          .   1613:		throw("mheap.freeSpanLocked - invalid span state")
         .          .   1614:	}
         .          .   1615:
         .          .   1616:	// Update stats.
         .          .   1617:	//
         .          .   1618:	// Mirrors the code in allocSpan.
         .          .   1619:	nbytes := s.npages * pageSize
         .          .   1620:	gcController.heapFree.add(int64(nbytes))
         .          .   1621:	if typ == spanAllocHeap {
         .          .   1622:		gcController.heapInUse.add(-int64(nbytes))
         .          .   1623:	}
         .          .   1624:	// Update consistent stats.
         .          .   1625:	stats := memstats.heapStats.acquire()
         .          .   1626:	switch typ {
         .          .   1627:	case spanAllocHeap:
         .          .   1628:		atomic.Xaddint64(&stats.inHeap, -int64(nbytes))
         .          .   1629:	case spanAllocStack:
         .          .   1630:		atomic.Xaddint64(&stats.inStacks, -int64(nbytes))
         .          .   1631:	case spanAllocPtrScalarBits:
         .          .   1632:		atomic.Xaddint64(&stats.inPtrScalarBits, -int64(nbytes))
         .          .   1633:	case spanAllocWorkBuf:
         .          .   1634:		atomic.Xaddint64(&stats.inWorkBufs, -int64(nbytes))
         .          .   1635:	}
         .          .   1636:	memstats.heapStats.release()
         .          .   1637:
         .          .   1638:	// Mark the space as free.
         .       10ms   1639:	h.pages.free(s.base(), s.npages)
         .          .   1640:
         .          .   1641:	// Free the span structure. We no longer have a use for it.
         .          .   1642:	s.state.set(mSpanDead)
         .          .   1643:	h.freeMSpanLocked(s)
         .          .   1644:}
ROUTINE ======================== runtime.(*mheap).initSpan in /usr/local/go/src/runtime/mheap.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .   1381:func (h *mheap) initSpan(s *mspan, typ spanAllocType, spanclass spanClass, base, npages uintptr) {
         .          .   1382:	// At this point, both s != nil and base != 0, and the heap
         .          .   1383:	// lock is no longer held. Initialize the span.
         .          .   1384:	s.init(base, npages)
         .          .   1385:	if h.allocNeedsZero(base, npages) {
         .          .   1386:		s.needzero = 1
         .          .   1387:	}
         .          .   1388:	nbytes := npages * pageSize
         .          .   1389:	if typ.manual() {
         .          .   1390:		s.manualFreeList = 0
         .          .   1391:		s.nelems = 0
         .          .   1392:		s.limit = s.base() + s.npages*pageSize
         .          .   1393:		s.state.set(mSpanManual)
         .          .   1394:	} else {
         .          .   1395:		// We must set span properties before the span is published anywhere
         .          .   1396:		// since we're not holding the heap lock.
         .          .   1397:		s.spanclass = spanclass
         .          .   1398:		if sizeclass := spanclass.sizeclass(); sizeclass == 0 {
         .          .   1399:			s.elemsize = nbytes
         .          .   1400:			s.nelems = 1
         .          .   1401:			s.divMul = 0
         .          .   1402:		} else {
         .          .   1403:			s.elemsize = uintptr(class_to_size[sizeclass])
         .          .   1404:			s.nelems = nbytes / s.elemsize
         .          .   1405:			s.divMul = class_to_divmagic[sizeclass]
         .          .   1406:		}
         .          .   1407:
         .          .   1408:		// Initialize mark and allocation structures.
         .          .   1409:		s.freeindex = 0
         .          .   1410:		s.freeIndexForScan = 0
         .          .   1411:		s.allocCache = ^uint64(0) // all 1s indicating all free.
         .          .   1412:		s.gcmarkBits = newMarkBits(s.nelems)
         .          .   1413:		s.allocBits = newAllocBits(s.nelems)
         .          .   1414:
         .          .   1415:		// It's safe to access h.sweepgen without the heap lock because it's
         .          .   1416:		// only ever updated with the world stopped and we run on the
         .          .   1417:		// systemstack which blocks a STW transition.
         .          .   1418:		atomic.Store(&s.sweepgen, h.sweepgen)
         .          .   1419:
         .          .   1420:		// Now that the span is filled in, set its state. This
         .          .   1421:		// is a publication barrier for the other fields in
         .          .   1422:		// the span. While valid pointers into this span
         .          .   1423:		// should never be visible until the span is returned,
         .          .   1424:		// if the garbage collector finds an invalid pointer,
         .          .   1425:		// access to the span may race with initialization of
         .          .   1426:		// the span. We resolve this race by atomically
         .          .   1427:		// setting the state after the span is fully
         .          .   1428:		// initialized, and atomically checking the state in
         .          .   1429:		// any situation where a pointer is suspect.
         .          .   1430:		s.state.set(mSpanInUse)
         .          .   1431:	}
         .          .   1432:
         .          .   1433:	// Publish the span in various locations.
         .          .   1434:
         .          .   1435:	// This is safe to call without the lock held because the slots
         .          .   1436:	// related to this span will only ever be read or modified by
         .          .   1437:	// this thread until pointers into the span are published (and
         .          .   1438:	// we execute a publication barrier at the end of this function
         .          .   1439:	// before that happens) or pageInUse is updated.
         .          .   1440:	h.setSpans(s.base(), npages, s)
         .          .   1441:
         .          .   1442:	if !typ.manual() {
         .          .   1443:		// Mark in-use span in arena page bitmap.
         .          .   1444:		//
         .          .   1445:		// This publishes the span to the page sweeper, so
         .          .   1446:		// it's imperative that the span be completely initialized
         .          .   1447:		// prior to this line.
         .          .   1448:		arena, pageIdx, pageMask := pageIndexOf(s.base())
         .          .   1449:		atomic.Or8(&arena.pageInUse[pageIdx], pageMask)
         .          .   1450:
         .          .   1451:		// Update related page sweeper stats.
      10ms       10ms   1452:		h.pagesInUse.Add(npages)
         .          .   1453:	}
         .          .   1454:
         .          .   1455:	// Make sure the newly allocated span will be observed
         .          .   1456:	// by the GC before pointers into the span are published.
         .          .   1457:	publicationBarrier()
ROUTINE ======================== runtime.(*moduledata).funcName in /usr/local/go/src/runtime/symtab.go
      10ms       90ms (flat, cum)  0.63% of Total
      10ms       10ms    639:func (md *moduledata) funcName(nameOff int32) string {
         .          .    640:	if nameOff == 0 {
         .          .    641:		return ""
         .          .    642:	}
         .       80ms    643:	return gostringnocopy(&md.funcnametab[nameOff])
         .          .    644:}
         .          .    645:
         .          .    646:// FuncForPC returns a *Func describing the function that contains the
         .          .    647:// given program counter address, or else nil.
         .          .    648://
ROUTINE ======================== runtime.(*moduledata).textOff in /usr/local/go/src/runtime/symtab.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    616:func (md *moduledata) textOff(pc uintptr) (uint32, bool) {
         .          .    617:	res := uint32(pc - md.text)
      10ms       10ms    618:	if len(md.textsectmap) > 1 {
         .          .    619:		for i, sect := range md.textsectmap {
         .          .    620:			if sect.baseaddr > pc {
         .          .    621:				// pc is not in any section.
         .          .    622:				return 0, false
         .          .    623:			}
ROUTINE ======================== runtime.(*mspan).refillAllocCache in /usr/local/go/src/runtime/mbitmap.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    120:func (s *mspan) refillAllocCache(whichByte uintptr) {
         .       10ms    121:	bytes := (*[8]uint8)(unsafe.Pointer(s.allocBits.bytep(whichByte)))
         .          .    122:	aCache := uint64(0)
         .          .    123:	aCache |= uint64(bytes[0])
         .          .    124:	aCache |= uint64(bytes[1]) << (1 * 8)
         .          .    125:	aCache |= uint64(bytes[2]) << (2 * 8)
         .          .    126:	aCache |= uint64(bytes[3]) << (3 * 8)
ROUTINE ======================== runtime.(*pageAlloc).free in /usr/local/go/src/runtime/mpagealloc.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    932:func (p *pageAlloc) free(base, npages uintptr) {
         .          .    933:	assertLockHeld(p.mheapLock)
         .          .    934:
         .          .    935:	// If we're freeing pages below the p.searchAddr, update searchAddr.
         .          .    936:	if b := (offAddr{base}); b.lessThan(p.searchAddr) {
         .          .    937:		p.searchAddr = b
         .          .    938:	}
         .          .    939:	limit := base + npages*pageSize - 1
         .          .    940:	if npages == 1 {
         .          .    941:		// Fast path: we're clearing a single bit, and we know exactly
         .          .    942:		// where it is, so mark it directly.
         .          .    943:		i := chunkIndex(base)
         .          .    944:		pi := chunkPageIndex(base)
         .          .    945:		p.chunkOf(i).free1(pi)
         .          .    946:		p.scav.index.free(i, pi, 1)
         .          .    947:	} else {
         .          .    948:		// Slow path: we're clearing more bits so we may need to iterate.
         .          .    949:		sc, ec := chunkIndex(base), chunkIndex(limit)
         .          .    950:		si, ei := chunkPageIndex(base), chunkPageIndex(limit)
         .          .    951:
         .          .    952:		if sc == ec {
         .          .    953:			// The range doesn't cross any chunk boundaries.
         .          .    954:			p.chunkOf(sc).free(si, ei+1-si)
         .          .    955:			p.scav.index.free(sc, si, ei+1-si)
         .          .    956:		} else {
         .          .    957:			// The range crosses at least one chunk boundary.
         .          .    958:			p.chunkOf(sc).free(si, pallocChunkPages-si)
         .          .    959:			p.scav.index.free(sc, si, pallocChunkPages-si)
         .          .    960:			for c := sc + 1; c < ec; c++ {
         .          .    961:				p.chunkOf(c).freeAll()
         .          .    962:				p.scav.index.free(c, 0, pallocChunkPages)
         .          .    963:			}
         .          .    964:			p.chunkOf(ec).free(0, ei+1)
         .          .    965:			p.scav.index.free(ec, 0, ei+1)
         .          .    966:		}
         .          .    967:	}
         .       10ms    968:	p.update(base, npages, true, false)
         .          .    969:}
         .          .    970:
         .          .    971:const (
         .          .    972:	pallocSumBytes = unsafe.Sizeof(pallocSum(0))
         .          .    973:
ROUTINE ======================== runtime.(*pageAlloc).update in /usr/local/go/src/runtime/mpagealloc.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    481:func (p *pageAlloc) update(base, npages uintptr, contig, alloc bool) {
         .          .    482:	assertLockHeld(p.mheapLock)
         .          .    483:
         .          .    484:	// base, limit, start, and end are inclusive.
         .          .    485:	limit := base + npages*pageSize - 1
         .          .    486:	sc, ec := chunkIndex(base), chunkIndex(limit)
         .          .    487:
         .          .    488:	// Handle updating the lowest level first.
         .          .    489:	if sc == ec {
         .          .    490:		// Fast path: the allocation doesn't span more than one chunk,
         .          .    491:		// so update this one and if the summary didn't change, return.
         .          .    492:		x := p.summary[len(p.summary)-1][sc]
         .       10ms    493:		y := p.chunkOf(sc).summarize()
         .          .    494:		if x == y {
         .          .    495:			return
         .          .    496:		}
         .          .    497:		p.summary[len(p.summary)-1][sc] = y
         .          .    498:	} else if contig {
ROUTINE ======================== runtime.(*pallocBits).summarize in /usr/local/go/src/runtime/mpallocbits.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    136:func (b *pallocBits) summarize() pallocSum {
         .          .    137:	var start, max, cur uint
         .          .    138:	const notSetYet = ^uint(0) // sentinel for start value
         .          .    139:	start = notSetYet
         .          .    140:	for i := 0; i < len(b); i++ {
         .          .    141:		x := b[i]
         .          .    142:		if x == 0 {
         .          .    143:			cur += 64
         .          .    144:			continue
         .          .    145:		}
         .          .    146:		t := uint(sys.TrailingZeros64(x))
         .          .    147:		l := uint(sys.LeadingZeros64(x))
         .          .    148:
         .          .    149:		// Finish any region spanning the uint64s
         .          .    150:		cur += t
         .          .    151:		if start == notSetYet {
         .          .    152:			start = cur
         .          .    153:		}
         .          .    154:		if cur > max {
         .          .    155:			max = cur
         .          .    156:		}
         .          .    157:		// Final region that might span to next uint64
         .          .    158:		cur = l
         .          .    159:	}
         .          .    160:	if start == notSetYet {
         .          .    161:		// Made it all the way through without finding a single 1 bit.
         .          .    162:		const n = uint(64 * len(b))
         .          .    163:		return packPallocSum(n, n, n)
         .          .    164:	}
         .          .    165:	if cur > max {
         .          .    166:		max = cur
         .          .    167:	}
         .          .    168:	if max >= 64-2 {
         .          .    169:		// There is no way an internal run of zeros could beat max.
         .          .    170:		return packPallocSum(start, max, cur)
         .          .    171:	}
         .          .    172:	// Now look inside each uint64 for runs of zeros.
         .          .    173:	// All uint64s must be nonzero, or we would have aborted above.
         .          .    174:outer:
         .          .    175:	for i := 0; i < len(b); i++ {
         .          .    176:		x := b[i]
         .          .    177:
         .          .    178:		// Look inside this uint64. We have a pattern like
         .          .    179:		// 000000 1xxxxx1 000000
         .          .    180:		// We need to look inside the 1xxxxx1 for any contiguous
         .          .    181:		// region of zeros.
         .          .    182:
         .          .    183:		// We already know the trailing zeros are no larger than max. Remove them.
         .          .    184:		x >>= sys.TrailingZeros64(x) & 63
         .          .    185:		if x&(x+1) == 0 { // no more zeros (except at the top).
         .          .    186:			continue
         .          .    187:		}
         .          .    188:
         .          .    189:		// Strategy: shrink all runs of zeros by max. If any runs of zero
         .          .    190:		// remain, then we've identified a larger maximum zero run.
         .          .    191:		p := max     // number of zeros we still need to shrink by.
         .          .    192:		k := uint(1) // current minimum length of runs of ones in x.
         .          .    193:		for {
         .          .    194:			// Shrink all runs of zeros by p places (except the top zeros).
         .          .    195:			for p > 0 {
         .          .    196:				if p <= k {
         .          .    197:					// Shift p ones down into the top of each run of zeros.
         .          .    198:					x |= x >> (p & 63)
      10ms       10ms    199:					if x&(x+1) == 0 { // no more zeros (except at the top).
         .          .    200:						continue outer
         .          .    201:					}
         .          .    202:					break
         .          .    203:				}
         .          .    204:				// Shift k ones down into the top of each run of zeros.
ROUTINE ======================== runtime.(*sweepLocked).sweep in /usr/local/go/src/runtime/mgcsweep.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    502:func (sl *sweepLocked) sweep(preserve bool) bool {
         .          .    503:	// It's critical that we enter this function with preemption disabled,
         .          .    504:	// GC must not start while we are in the middle of this function.
         .          .    505:	gp := getg()
         .          .    506:	if gp.m.locks == 0 && gp.m.mallocing == 0 && gp != gp.m.g0 {
         .          .    507:		throw("mspan.sweep: m is not locked")
         .          .    508:	}
         .          .    509:
         .          .    510:	s := sl.mspan
         .          .    511:	if !preserve {
         .          .    512:		// We'll release ownership of this span. Nil it out to
         .          .    513:		// prevent the caller from accidentally using it.
         .          .    514:		sl.mspan = nil
         .          .    515:	}
         .          .    516:
         .          .    517:	sweepgen := mheap_.sweepgen
         .          .    518:	if state := s.state.get(); state != mSpanInUse || s.sweepgen != sweepgen-1 {
         .          .    519:		print("mspan.sweep: state=", state, " sweepgen=", s.sweepgen, " mheap.sweepgen=", sweepgen, "\n")
         .          .    520:		throw("mspan.sweep: bad span state")
         .          .    521:	}
         .          .    522:
         .          .    523:	if traceEnabled() {
         .          .    524:		traceGCSweepSpan(s.npages * _PageSize)
         .          .    525:	}
         .          .    526:
         .          .    527:	mheap_.pagesSwept.Add(int64(s.npages))
         .          .    528:
         .          .    529:	spc := s.spanclass
         .          .    530:	size := s.elemsize
         .          .    531:
         .          .    532:	// The allocBits indicate which unmarked objects don't need to be
         .          .    533:	// processed since they were free at the end of the last GC cycle
         .          .    534:	// and were not allocated since then.
         .          .    535:	// If the allocBits index is >= s.freeindex and the bit
         .          .    536:	// is not marked then the object remains unallocated
         .          .    537:	// since the last GC.
         .          .    538:	// This situation is analogous to being on a freelist.
         .          .    539:
         .          .    540:	// Unlink & free special records for any objects we're about to free.
         .          .    541:	// Two complications here:
         .          .    542:	// 1. An object can have both finalizer and profile special records.
         .          .    543:	//    In such case we need to queue finalizer for execution,
         .          .    544:	//    mark the object as live and preserve the profile special.
         .          .    545:	// 2. A tiny object can have several finalizers setup for different offsets.
         .          .    546:	//    If such object is not marked, we need to queue all finalizers at once.
         .          .    547:	// Both 1 and 2 are possible at the same time.
         .          .    548:	hadSpecials := s.specials != nil
         .          .    549:	siter := newSpecialsIter(s)
         .          .    550:	for siter.valid() {
         .          .    551:		// A finalizer can be set for an inner byte of an object, find object beginning.
         .          .    552:		objIndex := uintptr(siter.s.offset) / size
         .          .    553:		p := s.base() + objIndex*size
         .          .    554:		mbits := s.markBitsForIndex(objIndex)
         .          .    555:		if !mbits.isMarked() {
         .          .    556:			// This object is not marked and has at least one special record.
         .          .    557:			// Pass 1: see if it has at least one finalizer.
         .          .    558:			hasFin := false
         .          .    559:			endOffset := p - s.base() + size
         .          .    560:			for tmp := siter.s; tmp != nil && uintptr(tmp.offset) < endOffset; tmp = tmp.next {
         .          .    561:				if tmp.kind == _KindSpecialFinalizer {
         .          .    562:					// Stop freeing of object if it has a finalizer.
         .          .    563:					mbits.setMarkedNonAtomic()
         .          .    564:					hasFin = true
         .          .    565:					break
         .          .    566:				}
         .          .    567:			}
         .          .    568:			// Pass 2: queue all finalizers _or_ handle profile record.
         .          .    569:			for siter.valid() && uintptr(siter.s.offset) < endOffset {
         .          .    570:				// Find the exact byte for which the special was setup
         .          .    571:				// (as opposed to object beginning).
         .          .    572:				special := siter.s
         .          .    573:				p := s.base() + uintptr(special.offset)
         .          .    574:				if special.kind == _KindSpecialFinalizer || !hasFin {
         .          .    575:					siter.unlinkAndNext()
         .          .    576:					freeSpecial(special, unsafe.Pointer(p), size)
         .          .    577:				} else {
         .          .    578:					// The object has finalizers, so we're keeping it alive.
         .          .    579:					// All other specials only apply when an object is freed,
         .          .    580:					// so just keep the special record.
         .          .    581:					siter.next()
         .          .    582:				}
         .          .    583:			}
         .          .    584:		} else {
         .          .    585:			// object is still live
         .          .    586:			if siter.s.kind == _KindSpecialReachable {
         .          .    587:				special := siter.unlinkAndNext()
         .          .    588:				(*specialReachable)(unsafe.Pointer(special)).reachable = true
         .          .    589:				freeSpecial(special, unsafe.Pointer(p), size)
         .          .    590:			} else {
         .          .    591:				// keep special record
         .          .    592:				siter.next()
         .          .    593:			}
         .          .    594:		}
         .          .    595:	}
         .          .    596:	if hadSpecials && s.specials == nil {
         .          .    597:		spanHasNoSpecials(s)
         .          .    598:	}
         .          .    599:
         .          .    600:	if debug.allocfreetrace != 0 || debug.clobberfree != 0 || raceenabled || msanenabled || asanenabled {
         .          .    601:		// Find all newly freed objects. This doesn't have to
         .          .    602:		// efficient; allocfreetrace has massive overhead.
         .          .    603:		mbits := s.markBitsForBase()
         .          .    604:		abits := s.allocBitsForIndex(0)
         .          .    605:		for i := uintptr(0); i < s.nelems; i++ {
         .          .    606:			if !mbits.isMarked() && (abits.index < s.freeindex || abits.isMarked()) {
         .          .    607:				x := s.base() + i*s.elemsize
         .          .    608:				if debug.allocfreetrace != 0 {
         .          .    609:					tracefree(unsafe.Pointer(x), size)
         .          .    610:				}
         .          .    611:				if debug.clobberfree != 0 {
         .          .    612:					clobberfree(unsafe.Pointer(x), size)
         .          .    613:				}
         .          .    614:				// User arenas are handled on explicit free.
         .          .    615:				if raceenabled && !s.isUserArenaChunk {
         .          .    616:					racefree(unsafe.Pointer(x), size)
         .          .    617:				}
         .          .    618:				if msanenabled && !s.isUserArenaChunk {
         .          .    619:					msanfree(unsafe.Pointer(x), size)
         .          .    620:				}
         .          .    621:				if asanenabled && !s.isUserArenaChunk {
         .          .    622:					asanpoison(unsafe.Pointer(x), size)
         .          .    623:				}
         .          .    624:			}
         .          .    625:			mbits.advance()
         .          .    626:			abits.advance()
         .          .    627:		}
         .          .    628:	}
         .          .    629:
         .          .    630:	// Check for zombie objects.
         .          .    631:	if s.freeindex < s.nelems {
         .          .    632:		// Everything < freeindex is allocated and hence
         .          .    633:		// cannot be zombies.
         .          .    634:		//
         .          .    635:		// Check the first bitmap byte, where we have to be
         .          .    636:		// careful with freeindex.
         .          .    637:		obj := s.freeindex
         .          .    638:		if (*s.gcmarkBits.bytep(obj / 8)&^*s.allocBits.bytep(obj / 8))>>(obj%8) != 0 {
         .          .    639:			s.reportZombies()
         .          .    640:		}
         .          .    641:		// Check remaining bytes.
         .          .    642:		for i := obj/8 + 1; i < divRoundUp(s.nelems, 8); i++ {
         .          .    643:			if *s.gcmarkBits.bytep(i)&^*s.allocBits.bytep(i) != 0 {
         .          .    644:				s.reportZombies()
         .          .    645:			}
         .          .    646:		}
         .          .    647:	}
         .          .    648:
         .          .    649:	// Count the number of free objects in this span.
         .          .    650:	nalloc := uint16(s.countAlloc())
         .          .    651:	nfreed := s.allocCount - nalloc
         .          .    652:	if nalloc > s.allocCount {
         .          .    653:		// The zombie check above should have caught this in
         .          .    654:		// more detail.
         .          .    655:		print("runtime: nelems=", s.nelems, " nalloc=", nalloc, " previous allocCount=", s.allocCount, " nfreed=", nfreed, "\n")
         .          .    656:		throw("sweep increased allocation count")
         .          .    657:	}
         .          .    658:
         .          .    659:	s.allocCount = nalloc
         .          .    660:	s.freeindex = 0 // reset allocation index to start of span.
         .          .    661:	s.freeIndexForScan = 0
         .          .    662:	if traceEnabled() {
         .          .    663:		getg().m.p.ptr().trace.reclaimed += uintptr(nfreed) * s.elemsize
         .          .    664:	}
         .          .    665:
         .          .    666:	// gcmarkBits becomes the allocBits.
         .          .    667:	// get a fresh cleared gcmarkBits in preparation for next GC
         .          .    668:	s.allocBits = s.gcmarkBits
         .          .    669:	s.gcmarkBits = newMarkBits(s.nelems)
         .          .    670:
         .          .    671:	// refresh pinnerBits if they exists
         .          .    672:	if s.pinnerBits != nil {
         .          .    673:		s.refreshPinnerBits()
         .          .    674:	}
         .          .    675:
         .          .    676:	// Initialize alloc bits cache.
         .          .    677:	s.refillAllocCache(0)
         .          .    678:
         .          .    679:	// The span must be in our exclusive ownership until we update sweepgen,
         .          .    680:	// check for potential races.
         .          .    681:	if state := s.state.get(); state != mSpanInUse || s.sweepgen != sweepgen-1 {
         .          .    682:		print("mspan.sweep: state=", state, " sweepgen=", s.sweepgen, " mheap.sweepgen=", sweepgen, "\n")
         .          .    683:		throw("mspan.sweep: bad span state after sweep")
         .          .    684:	}
         .          .    685:	if s.sweepgen == sweepgen+1 || s.sweepgen == sweepgen+3 {
         .          .    686:		throw("swept cached span")
         .          .    687:	}
         .          .    688:
         .          .    689:	// We need to set s.sweepgen = h.sweepgen only when all blocks are swept,
         .          .    690:	// because of the potential for a concurrent free/SetFinalizer.
         .          .    691:	//
         .          .    692:	// But we need to set it before we make the span available for allocation
         .          .    693:	// (return it to heap or mcentral), because allocation code assumes that a
         .          .    694:	// span is already swept if available for allocation.
         .          .    695:	//
         .          .    696:	// Serialization point.
         .          .    697:	// At this point the mark bits are cleared and allocation ready
         .          .    698:	// to go so release the span.
         .          .    699:	atomic.Store(&s.sweepgen, sweepgen)
         .          .    700:
         .          .    701:	if s.isUserArenaChunk {
         .          .    702:		if preserve {
         .          .    703:			// This is a case that should never be handled by a sweeper that
         .          .    704:			// preserves the span for reuse.
         .          .    705:			throw("sweep: tried to preserve a user arena span")
         .          .    706:		}
         .          .    707:		if nalloc > 0 {
         .          .    708:			// There still exist pointers into the span or the span hasn't been
         .          .    709:			// freed yet. It's not ready to be reused. Put it back on the
         .          .    710:			// full swept list for the next cycle.
         .          .    711:			mheap_.central[spc].mcentral.fullSwept(sweepgen).push(s)
         .          .    712:			return false
         .          .    713:		}
         .          .    714:
         .          .    715:		// It's only at this point that the sweeper doesn't actually need to look
         .          .    716:		// at this arena anymore, so subtract from pagesInUse now.
         .          .    717:		mheap_.pagesInUse.Add(-s.npages)
         .          .    718:		s.state.set(mSpanDead)
         .          .    719:
         .          .    720:		// The arena is ready to be recycled. Remove it from the quarantine list
         .          .    721:		// and place it on the ready list. Don't add it back to any sweep lists.
         .          .    722:		systemstack(func() {
         .          .    723:			// It's the arena code's responsibility to get the chunk on the quarantine
         .          .    724:			// list by the time all references to the chunk are gone.
         .          .    725:			if s.list != &mheap_.userArena.quarantineList {
         .          .    726:				throw("user arena span is on the wrong list")
         .          .    727:			}
         .          .    728:			lock(&mheap_.lock)
         .          .    729:			mheap_.userArena.quarantineList.remove(s)
         .          .    730:			mheap_.userArena.readyList.insert(s)
         .          .    731:			unlock(&mheap_.lock)
         .          .    732:		})
         .          .    733:		return false
         .          .    734:	}
         .          .    735:
         .          .    736:	if spc.sizeclass() != 0 {
         .          .    737:		// Handle spans for small objects.
         .          .    738:		if nfreed > 0 {
         .          .    739:			// Only mark the span as needing zeroing if we've freed any
         .          .    740:			// objects, because a fresh span that had been allocated into,
         .          .    741:			// wasn't totally filled, but then swept, still has all of its
         .          .    742:			// free slots zeroed.
         .          .    743:			s.needzero = 1
         .          .    744:			stats := memstats.heapStats.acquire()
         .          .    745:			atomic.Xadd64(&stats.smallFreeCount[spc.sizeclass()], int64(nfreed))
         .          .    746:			memstats.heapStats.release()
         .          .    747:
         .          .    748:			// Count the frees in the inconsistent, internal stats.
         .          .    749:			gcController.totalFree.Add(int64(nfreed) * int64(s.elemsize))
         .          .    750:		}
         .          .    751:		if !preserve {
         .          .    752:			// The caller may not have removed this span from whatever
         .          .    753:			// unswept set its on but taken ownership of the span for
         .          .    754:			// sweeping by updating sweepgen. If this span still is in
         .          .    755:			// an unswept set, then the mcentral will pop it off the
         .          .    756:			// set, check its sweepgen, and ignore it.
         .          .    757:			if nalloc == 0 {
         .          .    758:				// Free totally free span directly back to the heap.
         .       10ms    759:				mheap_.freeSpan(s)
         .          .    760:				return true
         .          .    761:			}
         .          .    762:			// Return span back to the right mcentral list.
         .          .    763:			if uintptr(nalloc) == s.nelems {
         .          .    764:				mheap_.central[spc].mcentral.fullSwept(sweepgen).push(s)
ROUTINE ======================== runtime.(*sweepLocked).sweep.(*mheap).freeSpan.func2 in /usr/local/go/src/runtime/mheap.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   1550:	systemstack(func() {
         .          .   1551:		pageTraceFree(getg().m.p.ptr(), 0, s.base(), s.npages)
         .          .   1552:
         .          .   1553:		lock(&h.lock)
         .          .   1554:		if msanenabled {
         .          .   1555:			// Tell msan that this entire span is no longer in use.
         .          .   1556:			base := unsafe.Pointer(s.base())
         .          .   1557:			bytes := s.npages << _PageShift
         .          .   1558:			msanfree(base, bytes)
         .          .   1559:		}
         .          .   1560:		if asanenabled {
         .          .   1561:			// Tell asan that this entire span is no longer in use.
         .          .   1562:			base := unsafe.Pointer(s.base())
         .          .   1563:			bytes := s.npages << _PageShift
         .          .   1564:			asanpoison(base, bytes)
         .          .   1565:		}
         .       10ms   1566:		h.freeSpanLocked(s, spanAllocHeap)
         .          .   1567:		unlock(&h.lock)
         .          .   1568:	})
         .          .   1569:}
         .          .   1570:
         .          .   1571:// freeManual frees a manually-managed span returned by allocManual.
ROUTINE ======================== runtime.(*timeHistogram).record in /usr/local/go/src/runtime/histogram.go
      20ms       20ms (flat, cum)  0.14% of Total
         .          .    105:func (h *timeHistogram) record(duration int64) {
         .          .    106:	// If the duration is negative, capture that in underflow.
         .          .    107:	if duration < 0 {
         .          .    108:		h.underflow.Add(1)
         .          .    109:		return
         .          .    110:	}
         .          .    111:	// bucketBit is the target bit for the bucket which is usually the
         .          .    112:	// highest 1 bit, but if we're less than the minimum, is the highest
         .          .    113:	// 1 bit of the minimum (which will be zero in the duration).
         .          .    114:	//
         .          .    115:	// bucket is the bucket index, which is the bucketBit minus the
         .          .    116:	// highest bit of the minimum, plus one to leave room for the catch-all
         .          .    117:	// bucket for samples lower than the minimum.
         .          .    118:	var bucketBit, bucket uint
      10ms       10ms    119:	if l := sys.Len64(uint64(duration)); l < timeHistMinBucketBits {
         .          .    120:		bucketBit = timeHistMinBucketBits
         .          .    121:		bucket = 0 // bucketBit - timeHistMinBucketBits
         .          .    122:	} else {
         .          .    123:		bucketBit = uint(l)
         .          .    124:		bucket = bucketBit - timeHistMinBucketBits + 1
         .          .    125:	}
         .          .    126:	// If the bucket we computed is greater than the number of buckets,
         .          .    127:	// count that in overflow.
         .          .    128:	if bucket >= timeHistNumBuckets {
         .          .    129:		h.overflow.Add(1)
         .          .    130:		return
         .          .    131:	}
         .          .    132:	// The sub-bucket index is just next timeHistSubBucketBits after the bucketBit.
         .          .    133:	subBucket := uint(duration>>(bucketBit-1-timeHistSubBucketBits)) % timeHistNumSubBuckets
         .          .    134:	h.counts[bucket*timeHistNumSubBuckets+subBucket].Add(1)
      10ms       10ms    135:}
         .          .    136:
         .          .    137:const (
         .          .    138:	fInf    = 0x7FF0000000000000
         .          .    139:	fNegInf = 0xFFF0000000000000
         .          .    140:)
ROUTINE ======================== runtime.(*unwinder).initAt in /usr/local/go/src/runtime/traceback.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    134:func (u *unwinder) initAt(pc0, sp0, lr0 uintptr, gp *g, flags unwindFlags) {
         .          .    135:	// Don't call this "g"; it's too easy get "g" and "gp" confused.
         .          .    136:	if ourg := getg(); ourg == gp && ourg == ourg.m.curg {
         .          .    137:		// The starting sp has been passed in as a uintptr, and the caller may
         .          .    138:		// have other uintptr-typed stack references as well.
         .          .    139:		// If during one of the calls that got us here or during one of the
         .          .    140:		// callbacks below the stack must be grown, all these uintptr references
         .          .    141:		// to the stack will not be updated, and traceback will continue
         .          .    142:		// to inspect the old stack memory, which may no longer be valid.
         .          .    143:		// Even if all the variables were updated correctly, it is not clear that
         .          .    144:		// we want to expose a traceback that begins on one stack and ends
         .          .    145:		// on another stack. That could confuse callers quite a bit.
         .          .    146:		// Instead, we require that initAt and any other function that
         .          .    147:		// accepts an sp for the current goroutine (typically obtained by
         .          .    148:		// calling getcallersp) must not run on that goroutine's stack but
         .          .    149:		// instead on the g0 stack.
         .          .    150:		throw("cannot trace user goroutine on its own stack")
         .          .    151:	}
         .          .    152:
         .          .    153:	if pc0 == ^uintptr(0) && sp0 == ^uintptr(0) { // Signal to fetch saved values from gp.
         .          .    154:		if gp.syscallsp != 0 {
         .          .    155:			pc0 = gp.syscallpc
         .          .    156:			sp0 = gp.syscallsp
         .          .    157:			if usesLR {
         .          .    158:				lr0 = 0
         .          .    159:			}
         .          .    160:		} else {
         .          .    161:			pc0 = gp.sched.pc
         .          .    162:			sp0 = gp.sched.sp
         .          .    163:			if usesLR {
         .          .    164:				lr0 = gp.sched.lr
         .          .    165:			}
         .          .    166:		}
         .          .    167:	}
         .          .    168:
         .          .    169:	var frame stkframe
         .          .    170:	frame.pc = pc0
         .          .    171:	frame.sp = sp0
         .          .    172:	if usesLR {
         .          .    173:		frame.lr = lr0
         .          .    174:	}
         .          .    175:
         .          .    176:	// If the PC is zero, it's likely a nil function call.
         .          .    177:	// Start in the caller's frame.
         .          .    178:	if frame.pc == 0 {
         .          .    179:		if usesLR {
         .          .    180:			frame.pc = *(*uintptr)(unsafe.Pointer(frame.sp))
         .          .    181:			frame.lr = 0
         .          .    182:		} else {
         .          .    183:			frame.pc = uintptr(*(*uintptr)(unsafe.Pointer(frame.sp)))
         .          .    184:			frame.sp += goarch.PtrSize
         .          .    185:		}
         .          .    186:	}
         .          .    187:
         .          .    188:	// runtime/internal/atomic functions call into kernel helpers on
         .          .    189:	// arm < 7. See runtime/internal/atomic/sys_linux_arm.s.
         .          .    190:	//
         .          .    191:	// Start in the caller's frame.
         .          .    192:	if GOARCH == "arm" && goarm < 7 && GOOS == "linux" && frame.pc&0xffff0000 == 0xffff0000 {
         .          .    193:		// Note that the calls are simple BL without pushing the return
         .          .    194:		// address, so we use LR directly.
         .          .    195:		//
         .          .    196:		// The kernel helpers are frameless leaf functions, so SP and
         .          .    197:		// LR are not touched.
         .          .    198:		frame.pc = frame.lr
         .          .    199:		frame.lr = 0
         .          .    200:	}
         .          .    201:
         .          .    202:	f := findfunc(frame.pc)
         .          .    203:	if !f.valid() {
         .          .    204:		if flags&unwindSilentErrors == 0 {
         .          .    205:			print("runtime: g ", gp.goid, ": unknown pc ", hex(frame.pc), "\n")
         .          .    206:			tracebackHexdump(gp.stack, &frame, 0)
         .          .    207:		}
         .          .    208:		if flags&(unwindPrintErrors|unwindSilentErrors) == 0 {
         .          .    209:			throw("unknown pc")
         .          .    210:		}
         .          .    211:		*u = unwinder{}
         .          .    212:		return
         .          .    213:	}
         .          .    214:	frame.fn = f
         .          .    215:
         .          .    216:	// Populate the unwinder.
         .          .    217:	*u = unwinder{
         .          .    218:		frame:        frame,
         .          .    219:		g:            gp.guintptr(),
         .          .    220:		cgoCtxt:      len(gp.cgoCtxt) - 1,
         .          .    221:		calleeFuncID: abi.FuncIDNormal,
         .          .    222:		flags:        flags,
         .          .    223:	}
         .          .    224:
         .          .    225:	isSyscall := frame.pc == pc0 && frame.sp == sp0 && pc0 == gp.syscallpc && sp0 == gp.syscallsp
         .       10ms    226:	u.resolveInternal(true, isSyscall)
         .          .    227:}
         .          .    228:
         .          .    229:func (u *unwinder) valid() bool {
         .          .    230:	return u.frame.pc != 0
         .          .    231:}
ROUTINE ======================== runtime.(*unwinder).resolveInternal in /usr/local/go/src/runtime/traceback.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    254:func (u *unwinder) resolveInternal(innermost, isSyscall bool) {
         .          .    255:	frame := &u.frame
         .          .    256:	gp := u.g.ptr()
         .          .    257:
         .          .    258:	f := frame.fn
         .          .    259:	if f.pcsp == 0 {
         .          .    260:		// No frame information, must be external function, like race support.
         .          .    261:		// See golang.org/issue/13568.
         .          .    262:		u.finishInternal()
         .          .    263:		return
         .          .    264:	}
         .          .    265:
         .          .    266:	// Compute function info flags.
         .          .    267:	flag := f.flag
         .          .    268:	if f.funcID == abi.FuncID_cgocallback {
         .          .    269:		// cgocallback does write SP to switch from the g0 to the curg stack,
         .          .    270:		// but it carefully arranges that during the transition BOTH stacks
         .          .    271:		// have cgocallback frame valid for unwinding through.
         .          .    272:		// So we don't need to exclude it with the other SP-writing functions.
         .          .    273:		flag &^= abi.FuncFlagSPWrite
         .          .    274:	}
         .          .    275:	if isSyscall {
         .          .    276:		// Some Syscall functions write to SP, but they do so only after
         .          .    277:		// saving the entry PC/SP using entersyscall.
         .          .    278:		// Since we are using the entry PC/SP, the later SP write doesn't matter.
         .          .    279:		flag &^= abi.FuncFlagSPWrite
         .          .    280:	}
         .          .    281:
         .          .    282:	// Found an actual function.
         .          .    283:	// Derive frame pointer.
         .          .    284:	if frame.fp == 0 {
         .          .    285:		// Jump over system stack transitions. If we're on g0 and there's a user
         .          .    286:		// goroutine, try to jump. Otherwise this is a regular call.
         .          .    287:		// We also defensively check that this won't switch M's on us,
         .          .    288:		// which could happen at critical points in the scheduler.
         .          .    289:		// This ensures gp.m doesn't change from a stack jump.
         .          .    290:		if u.flags&unwindJumpStack != 0 && gp == gp.m.g0 && gp.m.curg != nil && gp.m.curg.m == gp.m {
         .          .    291:			switch f.funcID {
         .          .    292:			case abi.FuncID_morestack:
         .          .    293:				// morestack does not return normally -- newstack()
         .          .    294:				// gogo's to curg.sched. Match that.
         .          .    295:				// This keeps morestack() from showing up in the backtrace,
         .          .    296:				// but that makes some sense since it'll never be returned
         .          .    297:				// to.
         .          .    298:				gp = gp.m.curg
         .          .    299:				u.g.set(gp)
         .          .    300:				frame.pc = gp.sched.pc
         .          .    301:				frame.fn = findfunc(frame.pc)
         .          .    302:				f = frame.fn
         .          .    303:				flag = f.flag
         .          .    304:				frame.lr = gp.sched.lr
         .          .    305:				frame.sp = gp.sched.sp
         .          .    306:				u.cgoCtxt = len(gp.cgoCtxt) - 1
         .          .    307:			case abi.FuncID_systemstack:
         .          .    308:				// systemstack returns normally, so just follow the
         .          .    309:				// stack transition.
         .          .    310:				if usesLR && funcspdelta(f, frame.pc, &u.cache) == 0 {
         .          .    311:					// We're at the function prologue and the stack
         .          .    312:					// switch hasn't happened, or epilogue where we're
         .          .    313:					// about to return. Just unwind normally.
         .          .    314:					// Do this only on LR machines because on x86
         .          .    315:					// systemstack doesn't have an SP delta (the CALL
         .          .    316:					// instruction opens the frame), therefore no way
         .          .    317:					// to check.
         .          .    318:					flag &^= abi.FuncFlagSPWrite
         .          .    319:					break
         .          .    320:				}
         .          .    321:				gp = gp.m.curg
         .          .    322:				u.g.set(gp)
         .          .    323:				frame.sp = gp.sched.sp
         .          .    324:				u.cgoCtxt = len(gp.cgoCtxt) - 1
         .          .    325:				flag &^= abi.FuncFlagSPWrite
         .          .    326:			}
         .          .    327:		}
         .       10ms    328:		frame.fp = frame.sp + uintptr(funcspdelta(f, frame.pc, &u.cache))
         .          .    329:		if !usesLR {
         .          .    330:			// On x86, call instruction pushes return PC before entering new function.
         .          .    331:			frame.fp += goarch.PtrSize
         .          .    332:		}
         .          .    333:	}
ROUTINE ======================== runtime.(*waitq).dequeueSudoG in /usr/local/go/src/runtime/select.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    600:func (q *waitq) dequeueSudoG(sgp *sudog) {
         .          .    601:	x := sgp.prev
         .          .    602:	y := sgp.next
         .          .    603:	if x != nil {
         .          .    604:		if y != nil {
         .          .    605:			// middle of queue
         .          .    606:			x.next = y
         .          .    607:			y.prev = x
         .          .    608:			sgp.next = nil
         .          .    609:			sgp.prev = nil
         .          .    610:			return
         .          .    611:		}
         .          .    612:		// end of queue
      10ms       10ms    613:		x.next = nil
         .          .    614:		q.last = x
         .          .    615:		sgp.prev = nil
         .          .    616:		return
         .          .    617:	}
         .          .    618:	if y != nil {
ROUTINE ======================== runtime.acquireSudog in /usr/local/go/src/runtime/proc.go
      20ms       20ms (flat, cum)  0.14% of Total
         .          .    414:func acquireSudog() *sudog {
         .          .    415:	// Delicate dance: the semaphore implementation calls
         .          .    416:	// acquireSudog, acquireSudog calls new(sudog),
         .          .    417:	// new calls malloc, malloc can call the garbage collector,
         .          .    418:	// and the garbage collector calls the semaphore implementation
         .          .    419:	// in stopTheWorld.
         .          .    420:	// Break the cycle by doing acquirem/releasem around new(sudog).
         .          .    421:	// The acquirem/releasem increments m.locks during new(sudog),
         .          .    422:	// which keeps the garbage collector from being invoked.
         .          .    423:	mp := acquirem()
         .          .    424:	pp := mp.p.ptr()
         .          .    425:	if len(pp.sudogcache) == 0 {
         .          .    426:		lock(&sched.sudoglock)
         .          .    427:		// First, try to grab a batch from central cache.
         .          .    428:		for len(pp.sudogcache) < cap(pp.sudogcache)/2 && sched.sudogcache != nil {
         .          .    429:			s := sched.sudogcache
      10ms       10ms    430:			sched.sudogcache = s.next
         .          .    431:			s.next = nil
         .          .    432:			pp.sudogcache = append(pp.sudogcache, s)
         .          .    433:		}
         .          .    434:		unlock(&sched.sudoglock)
         .          .    435:		// If the central cache is empty, allocate a new one.
         .          .    436:		if len(pp.sudogcache) == 0 {
         .          .    437:			pp.sudogcache = append(pp.sudogcache, new(sudog))
         .          .    438:		}
         .          .    439:	}
         .          .    440:	n := len(pp.sudogcache)
         .          .    441:	s := pp.sudogcache[n-1]
         .          .    442:	pp.sudogcache[n-1] = nil
         .          .    443:	pp.sudogcache = pp.sudogcache[:n-1]
      10ms       10ms    444:	if s.elem != nil {
         .          .    445:		throw("acquireSudog: found s.elem != nil in cache")
         .          .    446:	}
         .          .    447:	releasem(mp)
         .          .    448:	return s
         .          .    449:}
ROUTINE ======================== runtime.bgsweep in /usr/local/go/src/runtime/mgcsweep.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    273:func bgsweep(c chan int) {
         .          .    274:	sweep.g = getg()
         .          .    275:
         .          .    276:	lockInit(&sweep.lock, lockRankSweep)
         .          .    277:	lock(&sweep.lock)
         .          .    278:	sweep.parked = true
         .          .    279:	c <- 1
         .          .    280:	goparkunlock(&sweep.lock, waitReasonGCSweepWait, traceBlockGCSweep, 1)
         .          .    281:
         .          .    282:	for {
         .          .    283:		// bgsweep attempts to be a "low priority" goroutine by intentionally
         .          .    284:		// yielding time. It's OK if it doesn't run, because goroutines allocating
         .          .    285:		// memory will sweep and ensure that all spans are swept before the next
         .          .    286:		// GC cycle. We really only want to run when we're idle.
         .          .    287:		//
         .          .    288:		// However, calling Gosched after each span swept produces a tremendous
         .          .    289:		// amount of tracing events, sometimes up to 50% of events in a trace. It's
         .          .    290:		// also inefficient to call into the scheduler so much because sweeping a
         .          .    291:		// single span is in general a very fast operation, taking as little as 30 ns
         .          .    292:		// on modern hardware. (See #54767.)
         .          .    293:		//
         .          .    294:		// As a result, bgsweep sweeps in batches, and only calls into the scheduler
         .          .    295:		// at the end of every batch. Furthermore, it only yields its time if there
         .          .    296:		// isn't spare idle time available on other cores. If there's available idle
         .          .    297:		// time, helping to sweep can reduce allocation latencies by getting ahead of
         .          .    298:		// the proportional sweeper and having spans ready to go for allocation.
         .          .    299:		const sweepBatchSize = 10
         .          .    300:		nSwept := 0
         .       10ms    301:		for sweepone() != ^uintptr(0) {
         .          .    302:			sweep.nbgsweep++
         .          .    303:			nSwept++
         .          .    304:			if nSwept%sweepBatchSize == 0 {
         .          .    305:				goschedIfBusy()
         .          .    306:			}
ROUTINE ======================== runtime.callers in /usr/local/go/src/runtime/traceback.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   1092:func callers(skip int, pcbuf []uintptr) int {
         .          .   1093:	sp := getcallersp()
         .          .   1094:	pc := getcallerpc()
         .          .   1095:	gp := getg()
         .          .   1096:	var n int
         .       10ms   1097:	systemstack(func() {
         .          .   1098:		var u unwinder
         .          .   1099:		u.initAt(pc, sp, 0, gp, unwindSilentErrors)
         .          .   1100:		n = tracebackPCs(&u, skip, pcbuf)
         .          .   1101:	})
         .          .   1102:	return n
ROUTINE ======================== runtime.callers.func1 in /usr/local/go/src/runtime/traceback.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   1097:	systemstack(func() {
         .          .   1098:		var u unwinder
         .       10ms   1099:		u.initAt(pc, sp, 0, gp, unwindSilentErrors)
         .          .   1100:		n = tracebackPCs(&u, skip, pcbuf)
         .          .   1101:	})
         .          .   1102:	return n
         .          .   1103:}
         .          .   1104:
ROUTINE ======================== runtime.casgstatus in /usr/local/go/src/runtime/proc.go
      80ms      120ms (flat, cum)  0.84% of Total
         .          .   1042:func casgstatus(gp *g, oldval, newval uint32) {
         .          .   1043:	if (oldval&_Gscan != 0) || (newval&_Gscan != 0) || oldval == newval {
         .          .   1044:		systemstack(func() {
         .          .   1045:			print("runtime: casgstatus: oldval=", hex(oldval), " newval=", hex(newval), "\n")
         .          .   1046:			throw("casgstatus: bad incoming values")
         .          .   1047:		})
         .          .   1048:	}
         .          .   1049:
         .          .   1050:	acquireLockRank(lockRankGscan)
         .          .   1051:	releaseLockRank(lockRankGscan)
         .          .   1052:
         .          .   1053:	// See https://golang.org/cl/21503 for justification of the yield delay.
         .          .   1054:	const yieldDelay = 5 * 1000
         .          .   1055:	var nextYield int64
         .          .   1056:
         .          .   1057:	// loop if gp->atomicstatus is in a scan state giving
         .          .   1058:	// GC time to finish and change the state to oldval.
      70ms       70ms   1059:	for i := 0; !gp.atomicstatus.CompareAndSwap(oldval, newval); i++ {
         .          .   1060:		if oldval == _Gwaiting && gp.atomicstatus.Load() == _Grunnable {
         .          .   1061:			throw("casgstatus: waiting for Gwaiting but is Grunnable")
         .          .   1062:		}
         .          .   1063:		if i == 0 {
         .          .   1064:			nextYield = nanotime() + yieldDelay
         .          .   1065:		}
         .          .   1066:		if nanotime() < nextYield {
         .          .   1067:			for x := 0; x < 10 && gp.atomicstatus.Load() != oldval; x++ {
         .          .   1068:				procyield(1)
         .          .   1069:			}
         .          .   1070:		} else {
         .          .   1071:			osyield()
         .          .   1072:			nextYield = nanotime() + yieldDelay/2
         .          .   1073:		}
         .          .   1074:	}
         .          .   1075:
         .          .   1076:	if oldval == _Grunning {
         .          .   1077:		// Track every gTrackingPeriod time a goroutine transitions out of running.
         .          .   1078:		if casgstatusAlwaysTrack || gp.trackingSeq%gTrackingPeriod == 0 {
         .          .   1079:			gp.tracking = true
         .          .   1080:		}
         .          .   1081:		gp.trackingSeq++
         .          .   1082:	}
         .          .   1083:	if !gp.tracking {
         .          .   1084:		return
         .          .   1085:	}
         .          .   1086:
         .          .   1087:	// Handle various kinds of tracking.
         .          .   1088:	//
         .          .   1089:	// Currently:
         .          .   1090:	// - Time spent in runnable.
         .          .   1091:	// - Time spent blocked on a sync.Mutex or sync.RWMutex.
         .          .   1092:	switch oldval {
      10ms       10ms   1093:	case _Grunnable:
         .          .   1094:		// We transitioned out of runnable, so measure how much
         .          .   1095:		// time we spent in this state and add it to
         .          .   1096:		// runnableTime.
         .          .   1097:		now := nanotime()
         .          .   1098:		gp.runnableTime += now - gp.trackingStamp
         .          .   1099:		gp.trackingStamp = 0
         .          .   1100:	case _Gwaiting:
         .          .   1101:		if !gp.waitreason.isMutexWait() {
         .          .   1102:			// Not blocking on a lock.
         .          .   1103:			break
         .          .   1104:		}
         .          .   1105:		// Blocking on a lock, measure it. Note that because we're
         .          .   1106:		// sampling, we have to multiply by our sampling period to get
         .          .   1107:		// a more representative estimate of the absolute value.
         .          .   1108:		// gTrackingPeriod also represents an accurate sampling period
         .          .   1109:		// because we can only enter this state from _Grunning.
         .          .   1110:		now := nanotime()
         .          .   1111:		sched.totalMutexWaitTime.Add((now - gp.trackingStamp) * gTrackingPeriod)
         .          .   1112:		gp.trackingStamp = 0
         .          .   1113:	}
         .          .   1114:	switch newval {
         .          .   1115:	case _Gwaiting:
         .          .   1116:		if !gp.waitreason.isMutexWait() {
         .          .   1117:			// Not blocking on a lock.
         .          .   1118:			break
         .          .   1119:		}
         .          .   1120:		// Blocking on a lock. Write down the timestamp.
         .          .   1121:		now := nanotime()
         .          .   1122:		gp.trackingStamp = now
         .          .   1123:	case _Grunnable:
         .          .   1124:		// We just transitioned into runnable, so record what
         .          .   1125:		// time that happened.
         .       20ms   1126:		now := nanotime()
         .          .   1127:		gp.trackingStamp = now
         .          .   1128:	case _Grunning:
         .          .   1129:		// We're transitioning into running, so turn off
         .          .   1130:		// tracking and record how much time we spent in
         .          .   1131:		// runnable.
         .          .   1132:		gp.tracking = false
         .       20ms   1133:		sched.timeToRun.record(gp.runnableTime)
         .          .   1134:		gp.runnableTime = 0
         .          .   1135:	}
         .          .   1136:}
         .          .   1137:
         .          .   1138:// casGToWaiting transitions gp from old to _Gwaiting, and sets the wait reason.
ROUTINE ======================== runtime.checkTimers in /usr/local/go/src/runtime/proc.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   3659:func checkTimers(pp *p, now int64) (rnow, pollUntil int64, ran bool) {
         .          .   3660:	// If it's not yet time for the first timer, or the first adjusted
         .          .   3661:	// timer, then there is nothing to do.
         .          .   3662:	next := pp.timer0When.Load()
         .          .   3663:	nextAdj := pp.timerModifiedEarliest.Load()
         .          .   3664:	if next == 0 || (nextAdj != 0 && nextAdj < next) {
         .          .   3665:		next = nextAdj
         .          .   3666:	}
         .          .   3667:
         .          .   3668:	if next == 0 {
         .          .   3669:		// No timers to run or adjust.
         .          .   3670:		return now, 0, false
         .          .   3671:	}
         .          .   3672:
         .          .   3673:	if now == 0 {
         .       10ms   3674:		now = nanotime()
         .          .   3675:	}
         .          .   3676:	if now < next {
         .          .   3677:		// Next timer is not ready to run, but keep going
         .          .   3678:		// if we would clear deleted timers.
         .          .   3679:		// This corresponds to the condition below where
ROUTINE ======================== runtime.closechan in /usr/local/go/src/runtime/chan.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    357:func closechan(c *hchan) {
         .          .    358:	if c == nil {
         .          .    359:		panic(plainError("close of nil channel"))
         .          .    360:	}
         .          .    361:
         .          .    362:	lock(&c.lock)
         .          .    363:	if c.closed != 0 {
         .          .    364:		unlock(&c.lock)
         .          .    365:		panic(plainError("close of closed channel"))
         .          .    366:	}
         .          .    367:
         .          .    368:	if raceenabled {
         .          .    369:		callerpc := getcallerpc()
         .          .    370:		racewritepc(c.raceaddr(), callerpc, abi.FuncPCABIInternal(closechan))
         .          .    371:		racerelease(c.raceaddr())
         .          .    372:	}
         .          .    373:
         .          .    374:	c.closed = 1
         .          .    375:
         .          .    376:	var glist gList
         .          .    377:
         .          .    378:	// release all readers
         .          .    379:	for {
         .          .    380:		sg := c.recvq.dequeue()
         .          .    381:		if sg == nil {
         .          .    382:			break
         .          .    383:		}
         .          .    384:		if sg.elem != nil {
         .          .    385:			typedmemclr(c.elemtype, sg.elem)
         .          .    386:			sg.elem = nil
         .          .    387:		}
         .          .    388:		if sg.releasetime != 0 {
         .          .    389:			sg.releasetime = cputicks()
         .          .    390:		}
         .          .    391:		gp := sg.g
         .          .    392:		gp.param = unsafe.Pointer(sg)
         .          .    393:		sg.success = false
         .          .    394:		if raceenabled {
         .          .    395:			raceacquireg(gp, c.raceaddr())
         .          .    396:		}
         .          .    397:		glist.push(gp)
         .          .    398:	}
         .          .    399:
         .          .    400:	// release all writers (they will panic)
         .          .    401:	for {
         .          .    402:		sg := c.sendq.dequeue()
         .          .    403:		if sg == nil {
         .          .    404:			break
         .          .    405:		}
         .          .    406:		sg.elem = nil
         .          .    407:		if sg.releasetime != 0 {
         .          .    408:			sg.releasetime = cputicks()
         .          .    409:		}
         .          .    410:		gp := sg.g
         .          .    411:		gp.param = unsafe.Pointer(sg)
         .          .    412:		sg.success = false
         .          .    413:		if raceenabled {
         .          .    414:			raceacquireg(gp, c.raceaddr())
         .          .    415:		}
         .          .    416:		glist.push(gp)
         .          .    417:	}
         .          .    418:	unlock(&c.lock)
         .          .    419:
         .          .    420:	// Ready all Gs now that we've dropped the channel lock.
         .          .    421:	for !glist.empty() {
         .       10ms    422:		gp := glist.pop()
         .          .    423:		gp.schedlink = 0
         .          .    424:		goready(gp, 3)
         .          .    425:	}
         .          .    426:}
         .          .    427:
ROUTINE ======================== runtime.execute in /usr/local/go/src/runtime/proc.go
     170ms      250ms (flat, cum)  1.75% of Total
         .          .   2847:func execute(gp *g, inheritTime bool) {
         .          .   2848:	mp := getg().m
         .          .   2849:
         .          .   2850:	if goroutineProfile.active {
         .          .   2851:		// Make sure that gp has had its stack written out to the goroutine
         .          .   2852:		// profile, exactly as it was when the goroutine profiler first stopped
         .          .   2853:		// the world.
         .          .   2854:		tryRecordGoroutineProfile(gp, osyield)
         .          .   2855:	}
         .          .   2856:
         .          .   2857:	// Assign gp.m before entering _Grunning so running Gs have an
         .          .   2858:	// M.
         .          .   2859:	mp.curg = gp
     170ms      170ms   2860:	gp.m = mp
         .       60ms   2861:	casgstatus(gp, _Grunnable, _Grunning)
         .          .   2862:	gp.waitsince = 0
         .          .   2863:	gp.preempt = false
         .          .   2864:	gp.stackguard0 = gp.stack.lo + stackGuard
         .          .   2865:	if !inheritTime {
         .          .   2866:		mp.p.ptr().schedtick++
         .          .   2867:	}
         .          .   2868:
         .          .   2869:	// Check whether the profiler needs to be turned on or off.
         .          .   2870:	hz := sched.profilehz
         .          .   2871:	if mp.profilehz != hz {
         .          .   2872:		setThreadCPUProfiler(hz)
         .          .   2873:	}
         .          .   2874:
         .          .   2875:	if traceEnabled() {
         .          .   2876:		// GoSysExit has to happen when we have a P, but before GoStart.
         .          .   2877:		// So we emit it here.
         .          .   2878:		if gp.syscallsp != 0 {
         .          .   2879:			traceGoSysExit()
         .          .   2880:		}
         .          .   2881:		traceGoStart()
         .          .   2882:	}
         .          .   2883:
         .       20ms   2884:	gogo(&gp.sched)
         .          .   2885:}
         .          .   2886:
         .          .   2887:// Finds a runnable goroutine to execute.
         .          .   2888:// Tries to steal from other P's, get g from local or global queue, poll network.
         .          .   2889:// tryWakeP indicates that the returned goroutine is not normal (GC worker, trace
ROUTINE ======================== runtime.fastrand in /usr/local/go/src/runtime/stubs.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    124:func fastrand() uint32 {
         .          .    125:	mp := getg().m
         .          .    126:	// Implement wyrand: https://github.com/wangyi-fudan/wyhash
         .          .    127:	// Only the platform that math.Mul64 can be lowered
         .          .    128:	// by the compiler should be in this list.
         .          .    129:	if goarch.IsAmd64|goarch.IsArm64|goarch.IsPpc64|
         .          .    130:		goarch.IsPpc64le|goarch.IsMips64|goarch.IsMips64le|
         .          .    131:		goarch.IsS390x|goarch.IsRiscv64|goarch.IsLoong64 == 1 {
         .          .    132:		mp.fastrand += 0xa0761d6478bd642f
         .          .    133:		hi, lo := math.Mul64(mp.fastrand, mp.fastrand^0xe7037ed1a0b428db)
      10ms       10ms    134:		return uint32(hi ^ lo)
         .          .    135:	}
         .          .    136:
         .          .    137:	// Implement xorshift64+: 2 32-bit xorshift sequences added together.
         .          .    138:	// Shift triplet [17,7,16] was calculated as indicated in Marsaglia's
         .          .    139:	// Xorshift paper: https://www.jstatsoft.org/article/view/v008i14/xorshift.pdf
ROUTINE ======================== runtime.findObject in /usr/local/go/src/runtime/mbitmap.go
      10ms       50ms (flat, cum)  0.35% of Total
         .          .    339:func findObject(p, refBase, refOff uintptr) (base uintptr, s *mspan, objIndex uintptr) {
         .       40ms    340:	s = spanOf(p)
         .          .    341:	// If s is nil, the virtual address has never been part of the heap.
         .          .    342:	// This pointer may be to some mmap'd region, so we allow it.
         .          .    343:	if s == nil {
         .          .    344:		if (GOARCH == "amd64" || GOARCH == "arm64") && p == clobberdeadPtr && debug.invalidptr != 0 {
         .          .    345:			// Crash if clobberdeadPtr is seen. Only on AMD64 and ARM64 for now,
         .          .    346:			// as they are the only platform where compiler's clobberdead mode is
         .          .    347:			// implemented. On these platforms clobberdeadPtr cannot be a valid address.
         .          .    348:			badPointer(s, p, refBase, refOff)
         .          .    349:		}
         .          .    350:		return
         .          .    351:	}
         .          .    352:	// If p is a bad pointer, it may not be in s's bounds.
         .          .    353:	//
         .          .    354:	// Check s.state to synchronize with span initialization
         .          .    355:	// before checking other fields. See also spanOfHeap.
      10ms       10ms    356:	if state := s.state.get(); state != mSpanInUse || p < s.base() || p >= s.limit {
         .          .    357:		// Pointers into stacks are also ok, the runtime manages these explicitly.
         .          .    358:		if state == mSpanManual {
         .          .    359:			return
         .          .    360:		}
         .          .    361:		// The following ensures that we are rigorous about what data
ROUTINE ======================== runtime.findRunnable in /usr/local/go/src/runtime/proc.go
      60ms      1.42s (flat, cum)  9.96% of Total
         .          .   2891:func findRunnable() (gp *g, inheritTime, tryWakeP bool) {
         .          .   2892:	mp := getg().m
         .          .   2893:
         .          .   2894:	// The conditions here and in handoffp must agree: if
         .          .   2895:	// findrunnable would return a G to run, handoffp must start
         .          .   2896:	// an M.
         .          .   2897:
         .          .   2898:top:
         .          .   2899:	pp := mp.p.ptr()
         .          .   2900:	if sched.gcwaiting.Load() {
         .          .   2901:		gcstopm()
         .          .   2902:		goto top
         .          .   2903:	}
         .          .   2904:	if pp.runSafePointFn != 0 {
         .          .   2905:		runSafePointFn()
         .          .   2906:	}
         .          .   2907:
         .          .   2908:	// now and pollUntil are saved for work stealing later,
         .          .   2909:	// which may steal timers. It's important that between now
         .          .   2910:	// and then, nothing blocks, so these numbers remain mostly
         .          .   2911:	// relevant.
         .       10ms   2912:	now, pollUntil, _ := checkTimers(pp, 0)
         .          .   2913:
         .          .   2914:	// Try to schedule the trace reader.
         .          .   2915:	if traceEnabled() || traceShuttingDown() {
         .          .   2916:		gp := traceReader()
         .          .   2917:		if gp != nil {
         .          .   2918:			casgstatus(gp, _Gwaiting, _Grunnable)
         .          .   2919:			traceGoUnpark(gp, 0)
         .          .   2920:			return gp, false, true
         .          .   2921:		}
         .          .   2922:	}
         .          .   2923:
         .          .   2924:	// Try to schedule a GC worker.
         .          .   2925:	if gcBlackenEnabled != 0 {
         .          .   2926:		gp, tnow := gcController.findRunnableGCWorker(pp, now)
         .          .   2927:		if gp != nil {
         .          .   2928:			return gp, false, true
         .          .   2929:		}
         .          .   2930:		now = tnow
         .          .   2931:	}
         .          .   2932:
         .          .   2933:	// Check the global runnable queue once in a while to ensure fairness.
         .          .   2934:	// Otherwise two goroutines can completely occupy the local runqueue
         .          .   2935:	// by constantly respawning each other.
      10ms       10ms   2936:	if pp.schedtick%61 == 0 && sched.runqsize > 0 {
         .      220ms   2937:		lock(&sched.lock)
         .       10ms   2938:		gp := globrunqget(pp, 1)
         .      150ms   2939:		unlock(&sched.lock)
         .          .   2940:		if gp != nil {
         .          .   2941:			return gp, false, false
         .          .   2942:		}
         .          .   2943:	}
         .          .   2944:
         .          .   2945:	// Wake up the finalizer G.
         .          .   2946:	if fingStatus.Load()&(fingWait|fingWake) == fingWait|fingWake {
         .          .   2947:		if gp := wakefing(); gp != nil {
         .          .   2948:			ready(gp, 0, true)
         .          .   2949:		}
         .          .   2950:	}
      40ms       40ms   2951:	if *cgo_yield != nil {
         .          .   2952:		asmcgocall(*cgo_yield, nil)
         .          .   2953:	}
         .          .   2954:
         .          .   2955:	// local runq
         .       20ms   2956:	if gp, inheritTime := runqget(pp); gp != nil {
         .          .   2957:		return gp, inheritTime, false
         .          .   2958:	}
         .          .   2959:
         .          .   2960:	// global runq
         .          .   2961:	if sched.runqsize != 0 {
         .       60ms   2962:		lock(&sched.lock)
         .      500ms   2963:		gp := globrunqget(pp, 0)
         .      390ms   2964:		unlock(&sched.lock)
         .          .   2965:		if gp != nil {
         .          .   2966:			return gp, false, false
         .          .   2967:		}
         .          .   2968:	}
         .          .   2969:
         .          .   2970:	// Poll network.
         .          .   2971:	// This netpoll is only an optimization before we resort to stealing.
         .          .   2972:	// We can safely skip it if there are no waiters or a thread is blocked
         .          .   2973:	// in netpoll already. If there is any kind of logical race with that
         .          .   2974:	// blocked thread (e.g. it has already returned from netpoll, but does
         .          .   2975:	// not set lastpoll yet), this thread will do blocking netpoll below
         .          .   2976:	// anyway.
         .          .   2977:	if netpollinited() && netpollWaiters.Load() > 0 && sched.lastpoll.Load() != 0 {
         .          .   2978:		if list := netpoll(0); !list.empty() { // non-blocking
         .          .   2979:			gp := list.pop()
         .          .   2980:			injectglist(&list)
         .          .   2981:			casgstatus(gp, _Gwaiting, _Grunnable)
         .          .   2982:			if traceEnabled() {
         .          .   2983:				traceGoUnpark(gp, 0)
         .          .   2984:			}
         .          .   2985:			return gp, false, false
         .          .   2986:		}
         .          .   2987:	}
         .          .   2988:
         .          .   2989:	// Spinning Ms: steal work from other Ps.
         .          .   2990:	//
         .          .   2991:	// Limit the number of spinning Ms to half the number of busy Ps.
         .          .   2992:	// This is necessary to prevent excessive CPU consumption when
         .          .   2993:	// GOMAXPROCS>>1 but the program parallelism is low.
         .          .   2994:	if mp.spinning || 2*sched.nmspinning.Load() < gomaxprocs-sched.npidle.Load() {
         .          .   2995:		if !mp.spinning {
         .          .   2996:			mp.becomeSpinning()
         .          .   2997:		}
         .          .   2998:
         .          .   2999:		gp, inheritTime, tnow, w, newWork := stealWork(now)
         .          .   3000:		if gp != nil {
         .          .   3001:			// Successfully stole.
         .          .   3002:			return gp, inheritTime, false
         .          .   3003:		}
         .          .   3004:		if newWork {
         .          .   3005:			// There may be new timer or GC work; restart to
         .          .   3006:			// discover.
         .          .   3007:			goto top
         .          .   3008:		}
         .          .   3009:
         .          .   3010:		now = tnow
         .          .   3011:		if w != 0 && (pollUntil == 0 || w < pollUntil) {
         .          .   3012:			// Earlier timer to wait for.
         .          .   3013:			pollUntil = w
         .          .   3014:		}
         .          .   3015:	}
         .          .   3016:
         .          .   3017:	// We have nothing to do.
         .          .   3018:	//
         .          .   3019:	// If we're in the GC mark phase, can safely scan and blacken objects,
         .          .   3020:	// and have work to do, run idle-time marking rather than give up the P.
         .          .   3021:	if gcBlackenEnabled != 0 && gcMarkWorkAvailable(pp) && gcController.addIdleMarkWorker() {
         .          .   3022:		node := (*gcBgMarkWorkerNode)(gcBgMarkWorkerPool.pop())
         .          .   3023:		if node != nil {
         .          .   3024:			pp.gcMarkWorkerMode = gcMarkWorkerIdleMode
         .          .   3025:			gp := node.gp.ptr()
         .          .   3026:			casgstatus(gp, _Gwaiting, _Grunnable)
         .          .   3027:			if traceEnabled() {
         .          .   3028:				traceGoUnpark(gp, 0)
         .          .   3029:			}
         .          .   3030:			return gp, false, false
         .          .   3031:		}
         .          .   3032:		gcController.removeIdleMarkWorker()
         .          .   3033:	}
         .          .   3034:
         .          .   3035:	// wasm only:
         .          .   3036:	// If a callback returned and no other goroutine is awake,
         .          .   3037:	// then wake event handler goroutine which pauses execution
         .          .   3038:	// until a callback was triggered.
      10ms       10ms   3039:	gp, otherReady := beforeIdle(now, pollUntil)
         .          .   3040:	if gp != nil {
         .          .   3041:		casgstatus(gp, _Gwaiting, _Grunnable)
         .          .   3042:		if traceEnabled() {
         .          .   3043:			traceGoUnpark(gp, 0)
         .          .   3044:		}
ROUTINE ======================== runtime.findfunc in /usr/local/go/src/runtime/symtab.go
         0       20ms (flat, cum)  0.14% of Total
         .          .    772:func findfunc(pc uintptr) funcInfo {
         .       10ms    773:	datap := findmoduledatap(pc)
         .          .    774:	if datap == nil {
         .          .    775:		return funcInfo{}
         .          .    776:	}
         .          .    777:	const nsub = uintptr(len(findfuncbucket{}.subbuckets))
         .          .    778:
         .       10ms    779:	pcOff, ok := datap.textOff(pc)
         .          .    780:	if !ok {
         .          .    781:		return funcInfo{}
         .          .    782:	}
         .          .    783:
         .          .    784:	x := uintptr(pcOff) + datap.text - datap.minpc // TODO: are datap.text and datap.minpc always equal?
ROUTINE ======================== runtime.findmoduledatap in /usr/local/go/src/runtime/symtab.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    734:func findmoduledatap(pc uintptr) *moduledata {
         .          .    735:	for datap := &firstmoduledata; datap != nil; datap = datap.next {
      10ms       10ms    736:		if datap.minpc <= pc && pc < datap.maxpc {
         .          .    737:			return datap
         .          .    738:		}
         .          .    739:	}
         .          .    740:	return nil
         .          .    741:}
ROUTINE ======================== runtime.findnull in /usr/local/go/src/runtime/string.go
      40ms       80ms (flat, cum)  0.56% of Total
         .          .    508:func findnull(s *byte) int {
         .          .    509:	if s == nil {
         .          .    510:		return 0
         .          .    511:	}
         .          .    512:
         .          .    513:	// Avoid IndexByteString on Plan 9 because it uses SSE instructions
         .          .    514:	// on x86 machines, and those are classified as floating point instructions,
         .          .    515:	// which are illegal in a note handler.
         .          .    516:	if GOOS == "plan9" {
         .          .    517:		p := (*[maxAlloc/2 - 1]byte)(unsafe.Pointer(s))
         .          .    518:		l := 0
         .          .    519:		for p[l] != 0 {
         .          .    520:			l++
         .          .    521:		}
         .          .    522:		return l
         .          .    523:	}
         .          .    524:
         .          .    525:	// pageSize is the unit we scan at a time looking for NULL.
         .          .    526:	// It must be the minimum page size for any architecture Go
         .          .    527:	// runs on. It's okay (just a minor performance loss) if the
         .          .    528:	// actual system page size is larger than this value.
         .          .    529:	const pageSize = 4096
         .          .    530:
         .          .    531:	offset := 0
         .          .    532:	ptr := unsafe.Pointer(s)
         .          .    533:	// IndexByteString uses wide reads, so we need to be careful
         .          .    534:	// with page boundaries. Call IndexByteString on
         .          .    535:	// [ptr, endOfPage) interval.
         .          .    536:	safeLen := int(pageSize - uintptr(ptr)%pageSize)
         .          .    537:
         .          .    538:	for {
      40ms       40ms    539:		t := *(*string)(unsafe.Pointer(&stringStruct{ptr, safeLen}))
         .          .    540:		// Check one page at a time.
         .       40ms    541:		if i := bytealg.IndexByteString(t, 0); i != -1 {
         .          .    542:			return offset + i
         .          .    543:		}
         .          .    544:		// Move to next page
         .          .    545:		ptr = unsafe.Pointer(uintptr(ptr) + uintptr(safeLen))
         .          .    546:		offset += safeLen
ROUTINE ======================== runtime.funcname in /usr/local/go/src/runtime/symtab.go
         0       90ms (flat, cum)  0.63% of Total
         .          .    938:func funcname(f funcInfo) string {
         .          .    939:	if !f.valid() {
         .          .    940:		return ""
         .          .    941:	}
         .       90ms    942:	return f.datap.funcName(f.nameOff)
         .          .    943:}
         .          .    944:
         .          .    945:func funcpkgpath(f funcInfo) string {
         .          .    946:	name := funcNameForPrint(funcname(f))
         .          .    947:	i := len(name) - 1
ROUTINE ======================== runtime.funcspdelta in /usr/local/go/src/runtime/symtab.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    993:func funcspdelta(f funcInfo, targetpc uintptr, cache *pcvalueCache) int32 {
         .       10ms    994:	x, _ := pcvalue(f, f.pcsp, targetpc, cache, true)
         .          .    995:	if debugPcln && x&(goarch.PtrSize-1) != 0 {
         .          .    996:		print("invalid spdelta ", funcname(f), " ", hex(f.entry()), " ", hex(targetpc), " ", hex(f.pcsp), " ", x, "\n")
         .          .    997:		throw("bad spdelta")
         .          .    998:	}
         .          .    999:	return x
ROUTINE ======================== runtime.futex in /usr/local/go/src/runtime/sys_linux_amd64.s
     1.05s      1.05s (flat, cum)  7.36% of Total
         .          .    549:TEXT runtime·futex(SB),NOSPLIT,$0
         .          .    550:	MOVQ	addr+0(FP), DI
         .          .    551:	MOVL	op+8(FP), SI
         .          .    552:	MOVL	val+12(FP), DX
         .          .    553:	MOVQ	ts+16(FP), R10
         .          .    554:	MOVQ	addr2+24(FP), R8
         .          .    555:	MOVL	val3+32(FP), R9
         .          .    556:	MOVL	$SYS_futex, AX
         .          .    557:	SYSCALL
     1.04s      1.04s    558:	MOVL	AX, ret+40(FP)
      10ms       10ms    559:	RET
         .          .    560:
         .          .    561:// int32 clone(int32 flags, void *stk, M *mp, G *gp, void (*fn)(void));
         .          .    562:TEXT runtime·clone(SB),NOSPLIT|NOFRAME,$0
         .          .    563:	MOVL	flags+0(FP), DI
         .          .    564:	MOVQ	stk+8(FP), SI
ROUTINE ======================== runtime.futexsleep in /usr/local/go/src/runtime/os_linux.go
      20ms      150ms (flat, cum)  1.05% of Total
         .          .     62:func futexsleep(addr *uint32, val uint32, ns int64) {
         .          .     63:	// Some Linux kernels have a bug where futex of
         .          .     64:	// FUTEX_WAIT returns an internal error code
         .          .     65:	// as an errno. Libpthread ignores the return value
         .          .     66:	// here, and so can we: as it says a few lines up,
         .          .     67:	// spurious wakeups are allowed.
         .          .     68:	if ns < 0 {
      10ms      140ms     69:		futex(unsafe.Pointer(addr), _FUTEX_WAIT_PRIVATE, val, nil, nil, 0)
      10ms       10ms     70:		return
         .          .     71:	}
         .          .     72:
         .          .     73:	var ts timespec
         .          .     74:	ts.setNsec(ns)
         .          .     75:	futex(unsafe.Pointer(addr), _FUTEX_WAIT_PRIVATE, val, unsafe.Pointer(&ts), nil, 0)
ROUTINE ======================== runtime.futexwakeup in /usr/local/go/src/runtime/os_linux.go
      10ms      930ms (flat, cum)  6.52% of Total
         .          .     81:func futexwakeup(addr *uint32, cnt uint32) {
      10ms      930ms     82:	ret := futex(unsafe.Pointer(addr), _FUTEX_WAKE_PRIVATE, cnt, nil, nil, 0)
         .          .     83:	if ret >= 0 {
         .          .     84:		return
         .          .     85:	}
         .          .     86:
         .          .     87:	// I don't know that futex wakeup can return
ROUTINE ======================== runtime.gcBgMarkWorker in /usr/local/go/src/runtime/mgc.go
         0      100ms (flat, cum)   0.7% of Total
         .          .   1259:func gcBgMarkWorker() {
         .          .   1260:	gp := getg()
         .          .   1261:
         .          .   1262:	// We pass node to a gopark unlock function, so it can't be on
         .          .   1263:	// the stack (see gopark). Prevent deadlock from recursively
         .          .   1264:	// starting GC by disabling preemption.
         .          .   1265:	gp.m.preemptoff = "GC worker init"
         .          .   1266:	node := new(gcBgMarkWorkerNode)
         .          .   1267:	gp.m.preemptoff = ""
         .          .   1268:
         .          .   1269:	node.gp.set(gp)
         .          .   1270:
         .          .   1271:	node.m.set(acquirem())
         .          .   1272:	notewakeup(&work.bgMarkReady)
         .          .   1273:	// After this point, the background mark worker is generally scheduled
         .          .   1274:	// cooperatively by gcController.findRunnableGCWorker. While performing
         .          .   1275:	// work on the P, preemption is disabled because we are working on
         .          .   1276:	// P-local work buffers. When the preempt flag is set, this puts itself
         .          .   1277:	// into _Gwaiting to be woken up by gcController.findRunnableGCWorker
         .          .   1278:	// at the appropriate time.
         .          .   1279:	//
         .          .   1280:	// When preemption is enabled (e.g., while in gcMarkDone), this worker
         .          .   1281:	// may be preempted and schedule as a _Grunnable G from a runq. That is
         .          .   1282:	// fine; it will eventually gopark again for further scheduling via
         .          .   1283:	// findRunnableGCWorker.
         .          .   1284:	//
         .          .   1285:	// Since we disable preemption before notifying bgMarkReady, we
         .          .   1286:	// guarantee that this G will be in the worker pool for the next
         .          .   1287:	// findRunnableGCWorker. This isn't strictly necessary, but it reduces
         .          .   1288:	// latency between _GCmark starting and the workers starting.
         .          .   1289:
         .          .   1290:	for {
         .          .   1291:		// Go to sleep until woken by
         .          .   1292:		// gcController.findRunnableGCWorker.
         .          .   1293:		gopark(func(g *g, nodep unsafe.Pointer) bool {
         .          .   1294:			node := (*gcBgMarkWorkerNode)(nodep)
         .          .   1295:
         .          .   1296:			if mp := node.m.ptr(); mp != nil {
         .          .   1297:				// The worker G is no longer running; release
         .          .   1298:				// the M.
         .          .   1299:				//
         .          .   1300:				// N.B. it is _safe_ to release the M as soon
         .          .   1301:				// as we are no longer performing P-local mark
         .          .   1302:				// work.
         .          .   1303:				//
         .          .   1304:				// However, since we cooperatively stop work
         .          .   1305:				// when gp.preempt is set, if we releasem in
         .          .   1306:				// the loop then the following call to gopark
         .          .   1307:				// would immediately preempt the G. This is
         .          .   1308:				// also safe, but inefficient: the G must
         .          .   1309:				// schedule again only to enter gopark and park
         .          .   1310:				// again. Thus, we defer the release until
         .          .   1311:				// after parking the G.
         .          .   1312:				releasem(mp)
         .          .   1313:			}
         .          .   1314:
         .          .   1315:			// Release this G to the pool.
         .          .   1316:			gcBgMarkWorkerPool.push(&node.node)
         .          .   1317:			// Note that at this point, the G may immediately be
         .          .   1318:			// rescheduled and may be running.
         .          .   1319:			return true
         .          .   1320:		}, unsafe.Pointer(node), waitReasonGCWorkerIdle, traceBlockSystemGoroutine, 0)
         .          .   1321:
         .          .   1322:		// Preemption must not occur here, or another G might see
         .          .   1323:		// p.gcMarkWorkerMode.
         .          .   1324:
         .          .   1325:		// Disable preemption so we can use the gcw. If the
         .          .   1326:		// scheduler wants to preempt us, we'll stop draining,
         .          .   1327:		// dispose the gcw, and then preempt.
         .          .   1328:		node.m.set(acquirem())
         .          .   1329:		pp := gp.m.p.ptr() // P can't change with preemption disabled.
         .          .   1330:
         .          .   1331:		if gcBlackenEnabled == 0 {
         .          .   1332:			println("worker mode", pp.gcMarkWorkerMode)
         .          .   1333:			throw("gcBgMarkWorker: blackening not enabled")
         .          .   1334:		}
         .          .   1335:
         .          .   1336:		if pp.gcMarkWorkerMode == gcMarkWorkerNotWorker {
         .          .   1337:			throw("gcBgMarkWorker: mode not set")
         .          .   1338:		}
         .          .   1339:
         .          .   1340:		startTime := nanotime()
         .          .   1341:		pp.gcMarkWorkerStartTime = startTime
         .          .   1342:		var trackLimiterEvent bool
         .          .   1343:		if pp.gcMarkWorkerMode == gcMarkWorkerIdleMode {
         .          .   1344:			trackLimiterEvent = pp.limiterEvent.start(limiterEventIdleMarkWork, startTime)
         .          .   1345:		}
         .          .   1346:
         .          .   1347:		decnwait := atomic.Xadd(&work.nwait, -1)
         .          .   1348:		if decnwait == work.nproc {
         .          .   1349:			println("runtime: work.nwait=", decnwait, "work.nproc=", work.nproc)
         .          .   1350:			throw("work.nwait was > work.nproc")
         .          .   1351:		}
         .          .   1352:
         .      100ms   1353:		systemstack(func() {
         .          .   1354:			// Mark our goroutine preemptible so its stack
         .          .   1355:			// can be scanned. This lets two mark workers
         .          .   1356:			// scan each other (otherwise, they would
         .          .   1357:			// deadlock). We must not modify anything on
         .          .   1358:			// the G stack. However, stack shrinking is
ROUTINE ======================== runtime.gcBgMarkWorker.func2 in /usr/local/go/src/runtime/mgc.go
         0      100ms (flat, cum)   0.7% of Total
         .          .   1353:		systemstack(func() {
         .          .   1354:			// Mark our goroutine preemptible so its stack
         .          .   1355:			// can be scanned. This lets two mark workers
         .          .   1356:			// scan each other (otherwise, they would
         .          .   1357:			// deadlock). We must not modify anything on
         .          .   1358:			// the G stack. However, stack shrinking is
         .          .   1359:			// disabled for mark workers, so it is safe to
         .          .   1360:			// read from the G stack.
         .          .   1361:			casGToWaiting(gp, _Grunning, waitReasonGCWorkerActive)
         .          .   1362:			switch pp.gcMarkWorkerMode {
         .          .   1363:			default:
         .          .   1364:				throw("gcBgMarkWorker: unexpected gcMarkWorkerMode")
         .          .   1365:			case gcMarkWorkerDedicatedMode:
         .       70ms   1366:				gcDrain(&pp.gcw, gcDrainUntilPreempt|gcDrainFlushBgCredit)
         .          .   1367:				if gp.preempt {
         .          .   1368:					// We were preempted. This is
         .          .   1369:					// a useful signal to kick
         .          .   1370:					// everything out of the run
         .          .   1371:					// queue so it can run
         .          .   1372:					// somewhere else.
         .          .   1373:					if drainQ, n := runqdrain(pp); n > 0 {
         .          .   1374:						lock(&sched.lock)
         .          .   1375:						globrunqputbatch(&drainQ, int32(n))
         .          .   1376:						unlock(&sched.lock)
         .          .   1377:					}
         .          .   1378:				}
         .          .   1379:				// Go back to draining, this time
         .          .   1380:				// without preemption.
         .       30ms   1381:				gcDrain(&pp.gcw, gcDrainFlushBgCredit)
         .          .   1382:			case gcMarkWorkerFractionalMode:
         .          .   1383:				gcDrain(&pp.gcw, gcDrainFractional|gcDrainUntilPreempt|gcDrainFlushBgCredit)
         .          .   1384:			case gcMarkWorkerIdleMode:
         .          .   1385:				gcDrain(&pp.gcw, gcDrainIdle|gcDrainUntilPreempt|gcDrainFlushBgCredit)
         .          .   1386:			}
ROUTINE ======================== runtime.gcDrain in /usr/local/go/src/runtime/mgcmark.go
      10ms      100ms (flat, cum)   0.7% of Total
         .          .   1036:func gcDrain(gcw *gcWork, flags gcDrainFlags) {
         .          .   1037:	if !writeBarrier.needed {
         .          .   1038:		throw("gcDrain phase incorrect")
         .          .   1039:	}
         .          .   1040:
         .          .   1041:	gp := getg().m.curg
         .          .   1042:	preemptible := flags&gcDrainUntilPreempt != 0
         .          .   1043:	flushBgCredit := flags&gcDrainFlushBgCredit != 0
         .          .   1044:	idle := flags&gcDrainIdle != 0
         .          .   1045:
         .          .   1046:	initScanWork := gcw.heapScanWork
         .          .   1047:
         .          .   1048:	// checkWork is the scan work before performing the next
         .          .   1049:	// self-preempt check.
         .          .   1050:	checkWork := int64(1<<63 - 1)
         .          .   1051:	var check func() bool
         .          .   1052:	if flags&(gcDrainIdle|gcDrainFractional) != 0 {
         .          .   1053:		checkWork = initScanWork + drainCheckThreshold
         .          .   1054:		if idle {
         .          .   1055:			check = pollWork
         .          .   1056:		} else if flags&gcDrainFractional != 0 {
         .          .   1057:			check = pollFractionalWorkerExit
         .          .   1058:		}
         .          .   1059:	}
         .          .   1060:
         .          .   1061:	// Drain root marking jobs.
         .          .   1062:	if work.markrootNext < work.markrootJobs {
         .          .   1063:		// Stop if we're preemptible or if someone wants to STW.
         .          .   1064:		for !(gp.preempt && (preemptible || sched.gcwaiting.Load())) {
         .          .   1065:			job := atomic.Xadd(&work.markrootNext, +1) - 1
      10ms       10ms   1066:			if job >= work.markrootJobs {
         .          .   1067:				break
         .          .   1068:			}
         .          .   1069:			markroot(gcw, job, flushBgCredit)
         .          .   1070:			if check != nil && check() {
         .          .   1071:				goto done
         .          .   1072:			}
         .          .   1073:		}
         .          .   1074:	}
         .          .   1075:
         .          .   1076:	// Drain heap marking jobs.
         .          .   1077:	// Stop if we're preemptible or if someone wants to STW.
         .          .   1078:	for !(gp.preempt && (preemptible || sched.gcwaiting.Load())) {
         .          .   1079:		// Try to keep work available on the global queue. We used to
         .          .   1080:		// check if there were waiting workers, but it's better to
         .          .   1081:		// just keep work available than to make workers wait. In the
         .          .   1082:		// worst case, we'll do O(log(_WorkbufSize)) unnecessary
         .          .   1083:		// balances.
         .          .   1084:		if work.full == 0 {
         .       10ms   1085:			gcw.balance()
         .          .   1086:		}
         .          .   1087:
         .          .   1088:		b := gcw.tryGetFast()
         .          .   1089:		if b == 0 {
         .          .   1090:			b = gcw.tryGet()
         .          .   1091:			if b == 0 {
         .          .   1092:				// Flush the write barrier
         .          .   1093:				// buffer; this may create
         .          .   1094:				// more work.
         .          .   1095:				wbBufFlush()
         .          .   1096:				b = gcw.tryGet()
         .          .   1097:			}
         .          .   1098:		}
         .          .   1099:		if b == 0 {
         .          .   1100:			// Unable to get work.
         .          .   1101:			break
         .          .   1102:		}
         .       80ms   1103:		scanobject(b, gcw)
         .          .   1104:
         .          .   1105:		// Flush background scan work credit to the global
         .          .   1106:		// account if we've accumulated enough locally so
         .          .   1107:		// mutator assists can draw on it.
         .          .   1108:		if gcw.heapScanWork >= gcCreditSlack {
ROUTINE ======================== runtime.gfget in /usr/local/go/src/runtime/proc.go
     240ms      770ms (flat, cum)  5.40% of Total
         .          .   4667:func gfget(pp *p) *g {
         .          .   4668:retry:
         .          .   4669:	if pp.gFree.empty() && (!sched.gFree.stack.empty() || !sched.gFree.noStack.empty()) {
         .          .   4670:		lock(&sched.gFree.lock)
         .          .   4671:		// Move a batch of free Gs to the P.
         .          .   4672:		for pp.gFree.n < 32 {
         .          .   4673:			// Prefer Gs with stacks.
         .      510ms   4674:			gp := sched.gFree.stack.pop()
         .          .   4675:			if gp == nil {
         .       10ms   4676:				gp = sched.gFree.noStack.pop()
         .          .   4677:				if gp == nil {
         .          .   4678:					break
         .          .   4679:				}
         .          .   4680:			}
         .          .   4681:			sched.gFree.n--
         .          .   4682:			pp.gFree.push(gp)
         .          .   4683:			pp.gFree.n++
         .          .   4684:		}
         .          .   4685:		unlock(&sched.gFree.lock)
         .          .   4686:		goto retry
         .          .   4687:	}
         .       10ms   4688:	gp := pp.gFree.pop()
         .          .   4689:	if gp == nil {
         .          .   4690:		return nil
         .          .   4691:	}
         .          .   4692:	pp.gFree.n--
     240ms      240ms   4693:	if gp.stack.lo != 0 && gp.stack.hi-gp.stack.lo != uintptr(startingStackSize) {
         .          .   4694:		// Deallocate old stack. We kept it in gfput because it was the
         .          .   4695:		// right size when the goroutine was put on the free list, but
         .          .   4696:		// the right size has changed since then.
         .          .   4697:		systemstack(func() {
         .          .   4698:			stackfree(gp.stack)
ROUTINE ======================== runtime.gfput in /usr/local/go/src/runtime/proc.go
         0      140ms (flat, cum)  0.98% of Total
         .          .   4624:func gfput(pp *p, gp *g) {
         .          .   4625:	if readgstatus(gp) != _Gdead {
         .          .   4626:		throw("gfput: bad status (not Gdead)")
         .          .   4627:	}
         .          .   4628:
         .          .   4629:	stksize := gp.stack.hi - gp.stack.lo
         .          .   4630:
         .          .   4631:	if stksize != uintptr(startingStackSize) {
         .          .   4632:		// non-standard stack size - free it.
         .          .   4633:		stackfree(gp.stack)
         .          .   4634:		gp.stack.lo = 0
         .          .   4635:		gp.stack.hi = 0
         .          .   4636:		gp.stackguard0 = 0
         .          .   4637:	}
         .          .   4638:
         .          .   4639:	pp.gFree.push(gp)
         .          .   4640:	pp.gFree.n++
         .          .   4641:	if pp.gFree.n >= 64 {
         .          .   4642:		var (
         .          .   4643:			inc      int32
         .          .   4644:			stackQ   gQueue
         .          .   4645:			noStackQ gQueue
         .          .   4646:		)
         .          .   4647:		for pp.gFree.n >= 32 {
         .      120ms   4648:			gp := pp.gFree.pop()
         .          .   4649:			pp.gFree.n--
         .          .   4650:			if gp.stack.lo == 0 {
         .          .   4651:				noStackQ.push(gp)
         .          .   4652:			} else {
         .          .   4653:				stackQ.push(gp)
         .          .   4654:			}
         .          .   4655:			inc++
         .          .   4656:		}
         .       20ms   4657:		lock(&sched.gFree.lock)
         .          .   4658:		sched.gFree.noStack.pushAll(noStackQ)
         .          .   4659:		sched.gFree.stack.pushAll(stackQ)
         .          .   4660:		sched.gFree.n += inc
         .          .   4661:		unlock(&sched.gFree.lock)
         .          .   4662:	}
ROUTINE ======================== runtime.globrunqget in /usr/local/go/src/runtime/proc.go
         0      510ms (flat, cum)  3.58% of Total
         .          .   5992:func globrunqget(pp *p, max int32) *g {
         .          .   5993:	assertLockHeld(&sched.lock)
         .          .   5994:
         .          .   5995:	if sched.runqsize == 0 {
         .          .   5996:		return nil
         .          .   5997:	}
         .          .   5998:
         .          .   5999:	n := sched.runqsize/gomaxprocs + 1
         .          .   6000:	if n > sched.runqsize {
         .          .   6001:		n = sched.runqsize
         .          .   6002:	}
         .          .   6003:	if max > 0 && n > max {
         .          .   6004:		n = max
         .          .   6005:	}
         .          .   6006:	if n > int32(len(pp.runq))/2 {
         .          .   6007:		n = int32(len(pp.runq)) / 2
         .          .   6008:	}
         .          .   6009:
         .          .   6010:	sched.runqsize -= n
         .          .   6011:
         .       10ms   6012:	gp := sched.runq.pop()
         .          .   6013:	n--
         .          .   6014:	for ; n > 0; n-- {
         .      450ms   6015:		gp1 := sched.runq.pop()
         .       50ms   6016:		runqput(pp, gp1, false)
         .          .   6017:	}
         .          .   6018:	return gp
         .          .   6019:}
         .          .   6020:
         .          .   6021:// pMask is an atomic bitstring with one bit per P.
ROUTINE ======================== runtime.goexit0 in /usr/local/go/src/runtime/proc.go
      30ms      2.06s (flat, cum) 14.45% of Total
      10ms       10ms   3861:func goexit0(gp *g) {
         .          .   3862:	mp := getg().m
         .          .   3863:	pp := mp.p.ptr()
         .          .   3864:
         .       10ms   3865:	casgstatus(gp, _Grunning, _Gdead)
         .       10ms   3866:	gcController.addScannableStack(pp, -int64(gp.stack.hi-gp.stack.lo))
      10ms       70ms   3867:	if isSystemGoroutine(gp, false) {
         .          .   3868:		sched.ngsys.Add(-1)
         .          .   3869:	}
         .          .   3870:	gp.m = nil
         .          .   3871:	locked := gp.lockedm != 0
         .          .   3872:	gp.lockedm = 0
         .          .   3873:	mp.lockedg = 0
         .          .   3874:	gp.preemptStop = false
         .          .   3875:	gp.paniconfault = false
         .          .   3876:	gp._defer = nil // should be true already but just in case.
         .          .   3877:	gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data.
         .          .   3878:	gp.writebuf = nil
         .          .   3879:	gp.waitreason = waitReasonZero
         .          .   3880:	gp.param = nil
         .          .   3881:	gp.labels = nil
         .          .   3882:	gp.timer = nil
         .          .   3883:
         .          .   3884:	if gcBlackenEnabled != 0 && gp.gcAssistBytes > 0 {
         .          .   3885:		// Flush assist credit to the global pool. This gives
         .          .   3886:		// better information to pacing if the application is
         .          .   3887:		// rapidly creating an exiting goroutines.
         .          .   3888:		assistWorkPerByte := gcController.assistWorkPerByte.Load()
         .          .   3889:		scanCredit := int64(assistWorkPerByte * float64(gp.gcAssistBytes))
         .          .   3890:		gcController.bgScanCredit.Add(scanCredit)
         .          .   3891:		gp.gcAssistBytes = 0
         .          .   3892:	}
         .          .   3893:
         .          .   3894:	dropg()
         .          .   3895:
         .          .   3896:	if GOARCH == "wasm" { // no threads yet on wasm
         .          .   3897:		gfput(pp, gp)
         .          .   3898:		schedule() // never returns
         .          .   3899:	}
         .          .   3900:
      10ms       10ms   3901:	if mp.lockedInt != 0 {
         .          .   3902:		print("invalid m->lockedInt = ", mp.lockedInt, "\n")
         .          .   3903:		throw("internal lockOSThread error")
         .          .   3904:	}
         .      140ms   3905:	gfput(pp, gp)
         .          .   3906:	if locked {
         .          .   3907:		// The goroutine may have locked this thread because
         .          .   3908:		// it put it in an unusual kernel state. Kill it
         .          .   3909:		// rather than returning it to the thread pool.
         .          .   3910:
         .          .   3911:		// Return to mstart, which will release the P and exit
         .          .   3912:		// the thread.
         .          .   3913:		if GOOS != "plan9" { // See golang.org/issue/22227.
         .          .   3914:			gogo(&mp.g0.sched)
         .          .   3915:		} else {
         .          .   3916:			// Clear lockedExt on plan9 since we may end up re-using
         .          .   3917:			// this thread.
         .          .   3918:			mp.lockedExt = 0
         .          .   3919:		}
         .          .   3920:	}
         .      1.81s   3921:	schedule()
         .          .   3922:}
         .          .   3923:
         .          .   3924:// save updates getg().sched to refer to pc and sp so that a following
         .          .   3925:// gogo will restore pc and sp.
         .          .   3926://
ROUTINE ======================== runtime.gogo in /usr/local/go/src/runtime/asm_amd64.s
      20ms       20ms (flat, cum)  0.14% of Total
         .          .    403:TEXT runtime·gogo(SB), NOSPLIT, $0-8
         .          .    404:	MOVQ	buf+0(FP), BX		// gobuf
         .          .    405:	MOVQ	gobuf_g(BX), DX
      20ms       20ms    406:	MOVQ	0(DX), CX		// make sure g != nil
         .          .    407:	JMP	gogo<>(SB)
         .          .    408:
         .          .    409:TEXT gogo<>(SB), NOSPLIT, $0
         .          .    410:	get_tls(CX)
         .          .    411:	MOVQ	DX, g(CX)
ROUTINE ======================== runtime.gostartcall in /usr/local/go/src/runtime/sys_x86.go
     210ms      210ms (flat, cum)  1.47% of Total
         .          .     16:func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer) {
         .          .     17:	sp := buf.sp
      10ms       10ms     18:	sp -= goarch.PtrSize
         .          .     19:	*(*uintptr)(unsafe.Pointer(sp)) = buf.pc
     200ms      200ms     20:	buf.sp = sp
         .          .     21:	buf.pc = uintptr(fn)
         .          .     22:	buf.ctxt = ctxt
         .          .     23:}
ROUTINE ======================== runtime.gostartcallfn in /usr/local/go/src/runtime/stack.go
         0      210ms (flat, cum)  1.47% of Total
         .          .   1131:func gostartcallfn(gobuf *gobuf, fv *funcval) {
         .          .   1132:	var fn unsafe.Pointer
         .          .   1133:	if fv != nil {
         .          .   1134:		fn = unsafe.Pointer(fv.fn)
         .          .   1135:	} else {
         .          .   1136:		fn = unsafe.Pointer(abi.FuncPCABIInternal(nilfunc))
         .          .   1137:	}
         .      210ms   1138:	gostartcall(gobuf, fn, unsafe.Pointer(fv))
         .          .   1139:}
         .          .   1140:
         .          .   1141:// isShrinkStackSafe returns whether it's safe to attempt to shrink
         .          .   1142:// gp's stack. Shrinking the stack is only safe when we have precise
         .          .   1143:// pointer maps for all frames on the stack.
ROUTINE ======================== runtime.gostringnocopy in /usr/local/go/src/runtime/string.go
         0       80ms (flat, cum)  0.56% of Total
         .          .    564:func gostringnocopy(str *byte) string {
         .       80ms    565:	ss := stringStruct{str: unsafe.Pointer(str), len: findnull(str)}
         .          .    566:	s := *(*string)(unsafe.Pointer(&ss))
         .          .    567:	return s
         .          .    568:}
         .          .    569:
         .          .    570:func gostringw(strw *uint16) string {
ROUTINE ======================== runtime.heapBits.nextFast in /usr/local/go/src/runtime/mbitmap.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    496:func (h heapBits) nextFast() (heapBits, uintptr) {
         .          .    497:	// TESTQ/JEQ
         .          .    498:	if h.mask == 0 {
         .          .    499:		return h, 0
         .          .    500:	}
         .          .    501:	// BSFQ
         .          .    502:	var i int
         .          .    503:	if goarch.PtrSize == 8 {
         .          .    504:		i = sys.TrailingZeros64(uint64(h.mask))
         .          .    505:	} else {
         .          .    506:		i = sys.TrailingZeros32(uint32(h.mask))
         .          .    507:	}
         .          .    508:	// BTCQ
      10ms       10ms    509:	h.mask ^= uintptr(1) << (i & (ptrBits - 1))
         .          .    510:	// LEAQ (XX)(XX*8)
         .          .    511:	return h, h.addr + uintptr(i)*goarch.PtrSize
         .          .    512:}
         .          .    513:
         .          .    514:// bulkBarrierPreWrite executes a write barrier
ROUTINE ======================== runtime.heapBitsSetType in /usr/local/go/src/runtime/mbitmap.go
         0       30ms (flat, cum)  0.21% of Total
         .          .    946:func heapBitsSetType(x, size, dataSize uintptr, typ *_type) {
         .          .    947:	const doubleCheck = false // slow but helpful; enable to test modifications to this code
         .          .    948:
         .          .    949:	if doubleCheck && dataSize%typ.Size_ != 0 {
         .          .    950:		throw("heapBitsSetType: dataSize not a multiple of typ.Size")
         .          .    951:	}
         .          .    952:
         .          .    953:	if goarch.PtrSize == 8 && size == goarch.PtrSize {
         .          .    954:		// It's one word and it has pointers, it must be a pointer.
         .          .    955:		// Since all allocated one-word objects are pointers
         .          .    956:		// (non-pointers are aggregated into tinySize allocations),
         .          .    957:		// (*mspan).initHeapBits sets the pointer bits for us.
         .          .    958:		// Nothing to do here.
         .          .    959:		if doubleCheck {
         .          .    960:			h, addr := heapBitsForAddr(x, size).next()
         .          .    961:			if addr != x {
         .          .    962:				throw("heapBitsSetType: pointer bit missing")
         .          .    963:			}
         .          .    964:			_, addr = h.next()
         .          .    965:			if addr != 0 {
         .          .    966:				throw("heapBitsSetType: second pointer bit found")
         .          .    967:			}
         .          .    968:		}
         .          .    969:		return
         .          .    970:	}
         .          .    971:
         .          .    972:	h := writeHeapBitsForAddr(x)
         .          .    973:
         .          .    974:	// Handle GC program.
         .          .    975:	if typ.Kind_&kindGCProg != 0 {
         .          .    976:		// Expand the gc program into the storage we're going to use for the actual object.
         .          .    977:		obj := (*uint8)(unsafe.Pointer(x))
         .          .    978:		n := runGCProg(addb(typ.GCData, 4), obj)
         .          .    979:		// Use the expanded program to set the heap bits.
         .          .    980:		for i := uintptr(0); true; i += typ.Size_ {
         .          .    981:			// Copy expanded program to heap bitmap.
         .          .    982:			p := obj
         .          .    983:			j := n
         .          .    984:			for j > 8 {
         .          .    985:				h = h.write(uintptr(*p), 8)
         .          .    986:				p = add1(p)
         .          .    987:				j -= 8
         .          .    988:			}
         .          .    989:			h = h.write(uintptr(*p), j)
         .          .    990:
         .          .    991:			if i+typ.Size_ == dataSize {
         .          .    992:				break // no padding after last element
         .          .    993:			}
         .          .    994:
         .          .    995:			// Pad with zeros to the start of the next element.
         .          .    996:			h = h.pad(typ.Size_ - n*goarch.PtrSize)
         .          .    997:		}
         .          .    998:
         .          .    999:		h.flush(x, size)
         .          .   1000:
         .          .   1001:		// Erase the expanded GC program.
         .          .   1002:		memclrNoHeapPointers(unsafe.Pointer(obj), (n+7)/8)
         .          .   1003:		return
         .          .   1004:	}
         .          .   1005:
         .          .   1006:	// Note about sizes:
         .          .   1007:	//
         .          .   1008:	// typ.Size is the number of words in the object,
         .          .   1009:	// and typ.PtrBytes is the number of words in the prefix
         .          .   1010:	// of the object that contains pointers. That is, the final
         .          .   1011:	// typ.Size - typ.PtrBytes words contain no pointers.
         .          .   1012:	// This allows optimization of a common pattern where
         .          .   1013:	// an object has a small header followed by a large scalar
         .          .   1014:	// buffer. If we know the pointers are over, we don't have
         .          .   1015:	// to scan the buffer's heap bitmap at all.
         .          .   1016:	// The 1-bit ptrmasks are sized to contain only bits for
         .          .   1017:	// the typ.PtrBytes prefix, zero padded out to a full byte
         .          .   1018:	// of bitmap. If there is more room in the allocated object,
         .          .   1019:	// that space is pointerless. The noMorePtrs bitmap will prevent
         .          .   1020:	// scanning large pointerless tails of an object.
         .          .   1021:	//
         .          .   1022:	// Replicated copies are not as nice: if there is an array of
         .          .   1023:	// objects with scalar tails, all but the last tail does have to
         .          .   1024:	// be initialized, because there is no way to say "skip forward".
         .          .   1025:
         .          .   1026:	ptrs := typ.PtrBytes / goarch.PtrSize
         .          .   1027:	if typ.Size_ == dataSize { // Single element
         .          .   1028:		if ptrs <= ptrBits { // Single small element
         .          .   1029:			m := readUintptr(typ.GCData)
         .       10ms   1030:			h = h.write(m, ptrs)
         .          .   1031:		} else { // Single large element
         .          .   1032:			p := typ.GCData
         .          .   1033:			for {
         .          .   1034:				h = h.write(readUintptr(p), ptrBits)
         .          .   1035:				p = addb(p, ptrBits/8)
         .          .   1036:				ptrs -= ptrBits
         .          .   1037:				if ptrs <= ptrBits {
         .          .   1038:					break
         .          .   1039:				}
         .          .   1040:			}
         .          .   1041:			m := readUintptr(p)
         .          .   1042:			h = h.write(m, ptrs)
         .          .   1043:		}
         .          .   1044:	} else { // Repeated element
         .          .   1045:		words := typ.Size_ / goarch.PtrSize // total words, including scalar tail
         .          .   1046:		if words <= ptrBits {               // Repeated small element
         .          .   1047:			n := dataSize / typ.Size_
         .          .   1048:			m := readUintptr(typ.GCData)
         .          .   1049:			// Make larger unit to repeat
         .          .   1050:			for words <= ptrBits/2 {
         .          .   1051:				if n&1 != 0 {
         .          .   1052:					h = h.write(m, words)
         .          .   1053:				}
         .          .   1054:				n /= 2
         .          .   1055:				m |= m << words
         .          .   1056:				ptrs += words
         .          .   1057:				words *= 2
         .          .   1058:				if n == 1 {
         .          .   1059:					break
         .          .   1060:				}
         .          .   1061:			}
         .          .   1062:			for n > 1 {
         .          .   1063:				h = h.write(m, words)
         .          .   1064:				n--
         .          .   1065:			}
         .          .   1066:			h = h.write(m, ptrs)
         .          .   1067:		} else { // Repeated large element
         .          .   1068:			for i := uintptr(0); true; i += typ.Size_ {
         .          .   1069:				p := typ.GCData
         .          .   1070:				j := ptrs
         .          .   1071:				for j > ptrBits {
         .          .   1072:					h = h.write(readUintptr(p), ptrBits)
         .          .   1073:					p = addb(p, ptrBits/8)
         .          .   1074:					j -= ptrBits
         .          .   1075:				}
         .          .   1076:				m := readUintptr(p)
         .          .   1077:				h = h.write(m, j)
         .          .   1078:				if i+typ.Size_ == dataSize {
         .          .   1079:					break // don't need the trailing nonptr bits on the last element.
         .          .   1080:				}
         .          .   1081:				// Pad with zeros to the start of the next element.
         .          .   1082:				h = h.pad(typ.Size_ - typ.PtrBytes)
         .          .   1083:			}
         .          .   1084:		}
         .          .   1085:	}
         .       20ms   1086:	h.flush(x, size)
         .          .   1087:
         .          .   1088:	if doubleCheck {
         .          .   1089:		h := heapBitsForAddr(x, size)
         .          .   1090:		for i := uintptr(0); i < size; i += goarch.PtrSize {
         .          .   1091:			// Compute the pointer bit we want at offset i.
ROUTINE ======================== runtime.isSystemGoroutine in /usr/local/go/src/runtime/traceback.go
         0      110ms (flat, cum)  0.77% of Total
         .          .   1304:func isSystemGoroutine(gp *g, fixed bool) bool {
         .          .   1305:	// Keep this in sync with internal/trace.IsSystemGoroutine.
         .       20ms   1306:	f := findfunc(gp.startpc)
         .          .   1307:	if !f.valid() {
         .          .   1308:		return false
         .          .   1309:	}
         .          .   1310:	if f.funcID == abi.FuncID_runtime_main || f.funcID == abi.FuncID_handleAsyncEvent {
         .          .   1311:		return false
         .          .   1312:	}
         .          .   1313:	if f.funcID == abi.FuncID_runfinq {
         .          .   1314:		// We include the finalizer goroutine if it's calling
         .          .   1315:		// back into user code.
         .          .   1316:		if fixed {
         .          .   1317:			// This goroutine can vary. In fixed mode,
         .          .   1318:			// always consider it a user goroutine.
         .          .   1319:			return false
         .          .   1320:		}
         .          .   1321:		return fingStatus.Load()&fingRunningFinalizer == 0
         .          .   1322:	}
         .       90ms   1323:	return hasPrefix(funcname(f), "runtime.")
         .          .   1324:}
         .          .   1325:
         .          .   1326:// SetCgoTraceback records three C functions to use to gather
         .          .   1327:// traceback information from C code and to convert that traceback
         .          .   1328:// information into symbolic information. These are used when printing
ROUTINE ======================== runtime.lock in /usr/local/go/src/runtime/lock_futex.go
         0      580ms (flat, cum)  4.07% of Total
         .          .     47:func lock(l *mutex) {
         .      580ms     48:	lockWithRank(l, getLockRank(l))
         .          .     49:}
         .          .     50:
         .          .     51:func lock2(l *mutex) {
         .          .     52:	gp := getg()
         .          .     53:
ROUTINE ======================== runtime.lock2 in /usr/local/go/src/runtime/lock_futex.go
      40ms      580ms (flat, cum)  4.07% of Total
         .          .     51:func lock2(l *mutex) {
         .          .     52:	gp := getg()
         .          .     53:
         .          .     54:	if gp.m.locks < 0 {
         .          .     55:		throw("runtime·lock: lock count")
         .          .     56:	}
         .          .     57:	gp.m.locks++
         .          .     58:
         .          .     59:	// Speculative grab for lock.
         .          .     60:	v := atomic.Xchg(key32(&l.key), mutex_locked)
      10ms       10ms     61:	if v == mutex_unlocked {
         .          .     62:		return
         .          .     63:	}
         .          .     64:
         .          .     65:	// wait is either MUTEX_LOCKED or MUTEX_SLEEPING
         .          .     66:	// depending on whether there is a thread sleeping
         .          .     67:	// on this mutex. If we ever change l->key from
         .          .     68:	// MUTEX_SLEEPING to some other value, we must be
         .          .     69:	// careful to change it back to MUTEX_SLEEPING before
         .          .     70:	// returning, to ensure that the sleeping thread gets
         .          .     71:	// its wakeup call.
         .          .     72:	wait := v
         .          .     73:
         .          .     74:	// On uniprocessors, no point spinning.
         .          .     75:	// On multiprocessors, spin for ACTIVE_SPIN attempts.
         .          .     76:	spin := 0
         .          .     77:	if ncpu > 1 {
         .          .     78:		spin = active_spin
         .          .     79:	}
         .          .     80:	for {
         .          .     81:		// Try for lock, spinning.
         .          .     82:		for i := 0; i < spin; i++ {
      20ms       20ms     83:			for l.key == mutex_unlocked {
         .          .     84:				if atomic.Cas(key32(&l.key), mutex_unlocked, wait) {
         .          .     85:					return
         .          .     86:				}
         .          .     87:			}
         .      350ms     88:			procyield(active_spin_cnt)
         .          .     89:		}
         .          .     90:
         .          .     91:		// Try for lock, rescheduling.
         .          .     92:		for i := 0; i < passive_spin; i++ {
         .          .     93:			for l.key == mutex_unlocked {
         .          .     94:				if atomic.Cas(key32(&l.key), mutex_unlocked, wait) {
         .          .     95:					return
         .          .     96:				}
         .          .     97:			}
         .       40ms     98:			osyield()
         .          .     99:		}
         .          .    100:
         .          .    101:		// Sleep.
         .          .    102:		v = atomic.Xchg(key32(&l.key), mutex_sleeping)
      10ms       10ms    103:		if v == mutex_unlocked {
         .          .    104:			return
         .          .    105:		}
         .          .    106:		wait = mutex_sleeping
         .      150ms    107:		futexsleep(key32(&l.key), mutex_sleeping, -1)
         .          .    108:	}
         .          .    109:}
         .          .    110:
         .          .    111:func unlock(l *mutex) {
         .          .    112:	unlockWithRank(l)
ROUTINE ======================== runtime.lockWithRank in /usr/local/go/src/runtime/lockrank_off.go
         0      580ms (flat, cum)  4.07% of Total
         .          .     23:func lockWithRank(l *mutex, rank lockRank) {
         .      580ms     24:	lock2(l)
         .          .     25:}
         .          .     26:
         .          .     27:// This function may be called in nosplit context and thus must be nosplit.
         .          .     28://
         .          .     29://go:nosplit
ROUTINE ======================== runtime.mProf_Malloc in /usr/local/go/src/runtime/mprof.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    418:func mProf_Malloc(p unsafe.Pointer, size uintptr) {
         .          .    419:	var stk [maxStack]uintptr
         .       10ms    420:	nstk := callers(4, stk[:])
         .          .    421:
         .          .    422:	index := (mProfCycle.read() + 2) % uint32(len(memRecord{}.future))
         .          .    423:
         .          .    424:	b := stkbucket(memProfile, size, stk[:nstk], true)
         .          .    425:	mp := b.mp()
ROUTINE ======================== runtime.main in /usr/local/go/src/runtime/proc.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    144:func main() {
         .          .    145:	mp := getg().m
         .          .    146:
         .          .    147:	// Racectx of m0->g0 is used only as the parent of the main goroutine.
         .          .    148:	// It must not be used for anything else.
         .          .    149:	mp.g0.racectx = 0
         .          .    150:
         .          .    151:	// Max stack size is 1 GB on 64-bit, 250 MB on 32-bit.
         .          .    152:	// Using decimal instead of binary GB and MB because
         .          .    153:	// they look nicer in the stack overflow failure message.
         .          .    154:	if goarch.PtrSize == 8 {
         .          .    155:		maxstacksize = 1000000000
         .          .    156:	} else {
         .          .    157:		maxstacksize = 250000000
         .          .    158:	}
         .          .    159:
         .          .    160:	// An upper limit for max stack size. Used to avoid random crashes
         .          .    161:	// after calling SetMaxStack and trying to allocate a stack that is too big,
         .          .    162:	// since stackalloc works with 32-bit sizes.
         .          .    163:	maxstackceiling = 2 * maxstacksize
         .          .    164:
         .          .    165:	// Allow newproc to start new Ms.
         .          .    166:	mainStarted = true
         .          .    167:
         .          .    168:	if GOARCH != "wasm" { // no threads on wasm yet, so no sysmon
         .          .    169:		systemstack(func() {
         .          .    170:			newm(sysmon, nil, -1)
         .          .    171:		})
         .          .    172:	}
         .          .    173:
         .          .    174:	// Lock the main goroutine onto this, the main OS thread,
         .          .    175:	// during initialization. Most programs won't care, but a few
         .          .    176:	// do require certain calls to be made by the main thread.
         .          .    177:	// Those can arrange for main.main to run in the main thread
         .          .    178:	// by calling runtime.LockOSThread during initialization
         .          .    179:	// to preserve the lock.
         .          .    180:	lockOSThread()
         .          .    181:
         .          .    182:	if mp != &m0 {
         .          .    183:		throw("runtime.main not on m0")
         .          .    184:	}
         .          .    185:
         .          .    186:	// Record when the world started.
         .          .    187:	// Must be before doInit for tracing init.
         .          .    188:	runtimeInitTime = nanotime()
         .          .    189:	if runtimeInitTime == 0 {
         .          .    190:		throw("nanotime returning zero")
         .          .    191:	}
         .          .    192:
         .          .    193:	if debug.inittrace != 0 {
         .          .    194:		inittrace.id = getg().goid
         .          .    195:		inittrace.active = true
         .          .    196:	}
         .          .    197:
         .          .    198:	doInit(runtime_inittasks) // Must be before defer.
         .          .    199:
         .          .    200:	// Defer unlock so that runtime.Goexit during init does the unlock too.
         .          .    201:	needUnlock := true
         .          .    202:	defer func() {
         .          .    203:		if needUnlock {
         .          .    204:			unlockOSThread()
         .          .    205:		}
         .          .    206:	}()
         .          .    207:
         .          .    208:	gcenable()
         .          .    209:
         .          .    210:	main_init_done = make(chan bool)
         .          .    211:	if iscgo {
         .          .    212:		if _cgo_pthread_key_created == nil {
         .          .    213:			throw("_cgo_pthread_key_created missing")
         .          .    214:		}
         .          .    215:
         .          .    216:		if _cgo_thread_start == nil {
         .          .    217:			throw("_cgo_thread_start missing")
         .          .    218:		}
         .          .    219:		if GOOS != "windows" {
         .          .    220:			if _cgo_setenv == nil {
         .          .    221:				throw("_cgo_setenv missing")
         .          .    222:			}
         .          .    223:			if _cgo_unsetenv == nil {
         .          .    224:				throw("_cgo_unsetenv missing")
         .          .    225:			}
         .          .    226:		}
         .          .    227:		if _cgo_notify_runtime_init_done == nil {
         .          .    228:			throw("_cgo_notify_runtime_init_done missing")
         .          .    229:		}
         .          .    230:
         .          .    231:		// Set the x_crosscall2_ptr C function pointer variable point to crosscall2.
         .          .    232:		if set_crosscall2 == nil {
         .          .    233:			throw("set_crosscall2 missing")
         .          .    234:		}
         .          .    235:		set_crosscall2()
         .          .    236:
         .          .    237:		// Start the template thread in case we enter Go from
         .          .    238:		// a C-created thread and need to create a new thread.
         .          .    239:		startTemplateThread()
         .          .    240:		cgocall(_cgo_notify_runtime_init_done, nil)
         .          .    241:	}
         .          .    242:
         .          .    243:	// Run the initializing tasks. Depending on build mode this
         .          .    244:	// list can arrive a few different ways, but it will always
         .          .    245:	// contain the init tasks computed by the linker for all the
         .          .    246:	// packages in the program (excluding those added at runtime
         .          .    247:	// by package plugin).
         .          .    248:	for _, m := range activeModules() {
         .          .    249:		doInit(m.inittasks)
         .          .    250:	}
         .          .    251:
         .          .    252:	// Disable init tracing after main init done to avoid overhead
         .          .    253:	// of collecting statistics in malloc and newproc
         .          .    254:	inittrace.active = false
         .          .    255:
         .          .    256:	close(main_init_done)
         .          .    257:
         .          .    258:	needUnlock = false
         .          .    259:	unlockOSThread()
         .          .    260:
         .          .    261:	if isarchive || islibrary {
         .          .    262:		// A program compiled with -buildmode=c-archive or c-shared
         .          .    263:		// has a main, but it is not executed.
         .          .    264:		return
         .          .    265:	}
         .          .    266:	fn := main_main // make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime
         .       10ms    267:	fn()
         .          .    268:	if raceenabled {
         .          .    269:		runExitHooks(0) // run hooks now, since racefini does not return
         .          .    270:		racefini()
         .          .    271:	}
         .          .    272:
ROUTINE ======================== runtime.malg in /usr/local/go/src/runtime/proc.go
         0       20ms (flat, cum)  0.14% of Total
         .          .   4458:func malg(stacksize int32) *g {
         .       10ms   4459:	newg := new(g)
         .          .   4460:	if stacksize >= 0 {
         .          .   4461:		stacksize = round2(stackSystem + stacksize)
         .       10ms   4462:		systemstack(func() {
         .          .   4463:			newg.stack = stackalloc(uint32(stacksize))
         .          .   4464:		})
         .          .   4465:		newg.stackguard0 = newg.stack.lo + stackGuard
         .          .   4466:		newg.stackguard1 = ^uintptr(0)
         .          .   4467:		// Clear the bottom word of the stack. We record g
ROUTINE ======================== runtime.malg.func1 in /usr/local/go/src/runtime/proc.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .   4462:		systemstack(func() {
      10ms       10ms   4463:			newg.stack = stackalloc(uint32(stacksize))
         .          .   4464:		})
         .          .   4465:		newg.stackguard0 = newg.stack.lo + stackGuard
         .          .   4466:		newg.stackguard1 = ^uintptr(0)
         .          .   4467:		// Clear the bottom word of the stack. We record g
         .          .   4468:		// there on gsignal stack during VDSO on ARM and ARM64.
ROUTINE ======================== runtime.mallocgc in /usr/local/go/src/runtime/malloc.go
      10ms       90ms (flat, cum)  0.63% of Total
         .          .    948:func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {
         .          .    949:	if gcphase == _GCmarktermination {
         .          .    950:		throw("mallocgc called with gcphase == _GCmarktermination")
         .          .    951:	}
         .          .    952:
         .          .    953:	if size == 0 {
         .          .    954:		return unsafe.Pointer(&zerobase)
         .          .    955:	}
         .          .    956:
         .          .    957:	// It's possible for any malloc to trigger sweeping, which may in
         .          .    958:	// turn queue finalizers. Record this dynamic lock edge.
         .          .    959:	lockRankMayQueueFinalizer()
         .          .    960:
         .          .    961:	userSize := size
         .          .    962:	if asanenabled {
         .          .    963:		// Refer to ASAN runtime library, the malloc() function allocates extra memory,
         .          .    964:		// the redzone, around the user requested memory region. And the redzones are marked
         .          .    965:		// as unaddressable. We perform the same operations in Go to detect the overflows or
         .          .    966:		// underflows.
         .          .    967:		size += computeRZlog(size)
         .          .    968:	}
         .          .    969:
         .          .    970:	if debug.malloc {
         .          .    971:		if debug.sbrk != 0 {
         .          .    972:			align := uintptr(16)
         .          .    973:			if typ != nil {
         .          .    974:				// TODO(austin): This should be just
         .          .    975:				//   align = uintptr(typ.align)
         .          .    976:				// but that's only 4 on 32-bit platforms,
         .          .    977:				// even if there's a uint64 field in typ (see #599).
         .          .    978:				// This causes 64-bit atomic accesses to panic.
         .          .    979:				// Hence, we use stricter alignment that matches
         .          .    980:				// the normal allocator better.
         .          .    981:				if size&7 == 0 {
         .          .    982:					align = 8
         .          .    983:				} else if size&3 == 0 {
         .          .    984:					align = 4
         .          .    985:				} else if size&1 == 0 {
         .          .    986:					align = 2
         .          .    987:				} else {
         .          .    988:					align = 1
         .          .    989:				}
         .          .    990:			}
         .          .    991:			return persistentalloc(size, align, &memstats.other_sys)
         .          .    992:		}
         .          .    993:
         .          .    994:		if inittrace.active && inittrace.id == getg().goid {
         .          .    995:			// Init functions are executed sequentially in a single goroutine.
         .          .    996:			inittrace.allocs += 1
         .          .    997:		}
         .          .    998:	}
         .          .    999:
         .          .   1000:	// assistG is the G to charge for this allocation, or nil if
         .          .   1001:	// GC is not currently active.
         .          .   1002:	assistG := deductAssistCredit(size)
         .          .   1003:
         .          .   1004:	// Set mp.mallocing to keep from being preempted by GC.
         .          .   1005:	mp := acquirem()
         .          .   1006:	if mp.mallocing != 0 {
         .          .   1007:		throw("malloc deadlock")
         .          .   1008:	}
         .          .   1009:	if mp.gsignal == getg() {
         .          .   1010:		throw("malloc during signal")
         .          .   1011:	}
         .          .   1012:	mp.mallocing = 1
         .          .   1013:
         .          .   1014:	shouldhelpgc := false
         .          .   1015:	dataSize := userSize
         .          .   1016:	c := getMCache(mp)
         .          .   1017:	if c == nil {
         .          .   1018:		throw("mallocgc called without a P or outside bootstrapping")
         .          .   1019:	}
         .          .   1020:	var span *mspan
         .          .   1021:	var x unsafe.Pointer
         .          .   1022:	noscan := typ == nil || typ.PtrBytes == 0
         .          .   1023:	// In some cases block zeroing can profitably (for latency reduction purposes)
         .          .   1024:	// be delayed till preemption is possible; delayedZeroing tracks that state.
         .          .   1025:	delayedZeroing := false
         .          .   1026:	if size <= maxSmallSize {
         .          .   1027:		if noscan && size < maxTinySize {
         .          .   1028:			// Tiny allocator.
         .          .   1029:			//
         .          .   1030:			// Tiny allocator combines several tiny allocation requests
         .          .   1031:			// into a single memory block. The resulting memory block
         .          .   1032:			// is freed when all subobjects are unreachable. The subobjects
         .          .   1033:			// must be noscan (don't have pointers), this ensures that
         .          .   1034:			// the amount of potentially wasted memory is bounded.
         .          .   1035:			//
         .          .   1036:			// Size of the memory block used for combining (maxTinySize) is tunable.
         .          .   1037:			// Current setting is 16 bytes, which relates to 2x worst case memory
         .          .   1038:			// wastage (when all but one subobjects are unreachable).
         .          .   1039:			// 8 bytes would result in no wastage at all, but provides less
         .          .   1040:			// opportunities for combining.
         .          .   1041:			// 32 bytes provides more opportunities for combining,
         .          .   1042:			// but can lead to 4x worst case wastage.
         .          .   1043:			// The best case winning is 8x regardless of block size.
         .          .   1044:			//
         .          .   1045:			// Objects obtained from tiny allocator must not be freed explicitly.
         .          .   1046:			// So when an object will be freed explicitly, we ensure that
         .          .   1047:			// its size >= maxTinySize.
         .          .   1048:			//
         .          .   1049:			// SetFinalizer has a special case for objects potentially coming
         .          .   1050:			// from tiny allocator, it such case it allows to set finalizers
         .          .   1051:			// for an inner byte of a memory block.
         .          .   1052:			//
         .          .   1053:			// The main targets of tiny allocator are small strings and
         .          .   1054:			// standalone escaping variables. On a json benchmark
         .          .   1055:			// the allocator reduces number of allocations by ~12% and
         .          .   1056:			// reduces heap size by ~20%.
         .          .   1057:			off := c.tinyoffset
         .          .   1058:			// Align tiny pointer for required (conservative) alignment.
         .          .   1059:			if size&7 == 0 {
         .          .   1060:				off = alignUp(off, 8)
         .          .   1061:			} else if goarch.PtrSize == 4 && size == 12 {
         .          .   1062:				// Conservatively align 12-byte objects to 8 bytes on 32-bit
         .          .   1063:				// systems so that objects whose first field is a 64-bit
         .          .   1064:				// value is aligned to 8 bytes and does not cause a fault on
         .          .   1065:				// atomic access. See issue 37262.
         .          .   1066:				// TODO(mknyszek): Remove this workaround if/when issue 36606
         .          .   1067:				// is resolved.
         .          .   1068:				off = alignUp(off, 8)
         .          .   1069:			} else if size&3 == 0 {
         .          .   1070:				off = alignUp(off, 4)
         .          .   1071:			} else if size&1 == 0 {
         .          .   1072:				off = alignUp(off, 2)
         .          .   1073:			}
         .          .   1074:			if off+size <= maxTinySize && c.tiny != 0 {
         .          .   1075:				// The object fits into existing tiny block.
         .          .   1076:				x = unsafe.Pointer(c.tiny + off)
         .          .   1077:				c.tinyoffset = off + size
         .          .   1078:				c.tinyAllocs++
         .          .   1079:				mp.mallocing = 0
         .          .   1080:				releasem(mp)
         .          .   1081:				return x
         .          .   1082:			}
         .          .   1083:			// Allocate a new maxTinySize block.
         .          .   1084:			span = c.alloc[tinySpanClass]
         .          .   1085:			v := nextFreeFast(span)
         .          .   1086:			if v == 0 {
         .          .   1087:				v, span, shouldhelpgc = c.nextFree(tinySpanClass)
         .          .   1088:			}
         .          .   1089:			x = unsafe.Pointer(v)
         .          .   1090:			(*[2]uint64)(x)[0] = 0
         .          .   1091:			(*[2]uint64)(x)[1] = 0
         .          .   1092:			// See if we need to replace the existing tiny block with the new one
         .          .   1093:			// based on amount of remaining free space.
         .          .   1094:			if !raceenabled && (size < c.tinyoffset || c.tiny == 0) {
         .          .   1095:				// Note: disabled when race detector is on, see comment near end of this function.
         .          .   1096:				c.tiny = uintptr(x)
         .          .   1097:				c.tinyoffset = size
         .          .   1098:			}
         .          .   1099:			size = maxTinySize
         .          .   1100:		} else {
         .          .   1101:			var sizeclass uint8
         .          .   1102:			if size <= smallSizeMax-8 {
         .          .   1103:				sizeclass = size_to_class8[divRoundUp(size, smallSizeDiv)]
         .          .   1104:			} else {
         .          .   1105:				sizeclass = size_to_class128[divRoundUp(size-smallSizeMax, largeSizeDiv)]
         .          .   1106:			}
         .          .   1107:			size = uintptr(class_to_size[sizeclass])
         .          .   1108:			spc := makeSpanClass(sizeclass, noscan)
         .          .   1109:			span = c.alloc[spc]
         .          .   1110:			v := nextFreeFast(span)
         .          .   1111:			if v == 0 {
         .       40ms   1112:				v, span, shouldhelpgc = c.nextFree(spc)
         .          .   1113:			}
         .          .   1114:			x = unsafe.Pointer(v)
         .          .   1115:			if needzero && span.needzero != 0 {
         .          .   1116:				memclrNoHeapPointers(x, size)
         .          .   1117:			}
         .          .   1118:		}
         .          .   1119:	} else {
         .          .   1120:		shouldhelpgc = true
         .          .   1121:		// For large allocations, keep track of zeroed state so that
         .          .   1122:		// bulk zeroing can be happen later in a preemptible context.
         .          .   1123:		span = c.allocLarge(size, noscan)
         .          .   1124:		span.freeindex = 1
         .          .   1125:		span.allocCount = 1
         .          .   1126:		size = span.elemsize
         .          .   1127:		x = unsafe.Pointer(span.base())
         .          .   1128:		if needzero && span.needzero != 0 {
         .          .   1129:			if noscan {
         .          .   1130:				delayedZeroing = true
         .          .   1131:			} else {
         .          .   1132:				memclrNoHeapPointers(x, size)
         .          .   1133:				// We've in theory cleared almost the whole span here,
         .          .   1134:				// and could take the extra step of actually clearing
         .          .   1135:				// the whole thing. However, don't. Any GC bits for the
         .          .   1136:				// uncleared parts will be zero, and it's just going to
         .          .   1137:				// be needzero = 1 once freed anyway.
         .          .   1138:			}
         .          .   1139:		}
         .          .   1140:	}
         .          .   1141:
         .          .   1142:	if !noscan {
         .          .   1143:		var scanSize uintptr
         .       30ms   1144:		heapBitsSetType(uintptr(x), size, dataSize, typ)
         .          .   1145:		if dataSize > typ.Size_ {
         .          .   1146:			// Array allocation. If there are any
         .          .   1147:			// pointers, GC has to scan to the last
         .          .   1148:			// element.
         .          .   1149:			if typ.PtrBytes != 0 {
         .          .   1150:				scanSize = dataSize - typ.Size_ + typ.PtrBytes
         .          .   1151:			}
         .          .   1152:		} else {
         .          .   1153:			scanSize = typ.PtrBytes
         .          .   1154:		}
         .          .   1155:		c.scanAlloc += scanSize
         .          .   1156:	}
         .          .   1157:
         .          .   1158:	// Ensure that the stores above that initialize x to
         .          .   1159:	// type-safe memory and set the heap bits occur before
         .          .   1160:	// the caller can make x observable to the garbage
         .          .   1161:	// collector. Otherwise, on weakly ordered machines,
         .          .   1162:	// the garbage collector could follow a pointer to x,
         .          .   1163:	// but see uninitialized memory or stale heap bits.
         .          .   1164:	publicationBarrier()
         .          .   1165:	// As x and the heap bits are initialized, update
         .          .   1166:	// freeIndexForScan now so x is seen by the GC
         .          .   1167:	// (including conservative scan) as an allocated object.
         .          .   1168:	// While this pointer can't escape into user code as a
         .          .   1169:	// _live_ pointer until we return, conservative scanning
         .          .   1170:	// may find a dead pointer that happens to point into this
         .          .   1171:	// object. Delaying this update until now ensures that
         .          .   1172:	// conservative scanning considers this pointer dead until
         .          .   1173:	// this point.
         .          .   1174:	span.freeIndexForScan = span.freeindex
         .          .   1175:
         .          .   1176:	// Allocate black during GC.
         .          .   1177:	// All slots hold nil so no scanning is needed.
         .          .   1178:	// This may be racing with GC so do it atomically if there can be
         .          .   1179:	// a race marking the bit.
         .          .   1180:	if gcphase != _GCoff {
         .          .   1181:		gcmarknewobject(span, uintptr(x), size)
         .          .   1182:	}
         .          .   1183:
         .          .   1184:	if raceenabled {
         .          .   1185:		racemalloc(x, size)
         .          .   1186:	}
         .          .   1187:
         .          .   1188:	if msanenabled {
         .          .   1189:		msanmalloc(x, size)
         .          .   1190:	}
         .          .   1191:
         .          .   1192:	if asanenabled {
         .          .   1193:		// We should only read/write the memory with the size asked by the user.
         .          .   1194:		// The rest of the allocated memory should be poisoned, so that we can report
         .          .   1195:		// errors when accessing poisoned memory.
         .          .   1196:		// The allocated memory is larger than required userSize, it will also include
         .          .   1197:		// redzone and some other padding bytes.
         .          .   1198:		rzBeg := unsafe.Add(x, userSize)
         .          .   1199:		asanpoison(rzBeg, size-userSize)
         .          .   1200:		asanunpoison(x, userSize)
         .          .   1201:	}
         .          .   1202:
         .          .   1203:	if rate := MemProfileRate; rate > 0 {
         .          .   1204:		// Note cache c only valid while m acquired; see #47302
         .          .   1205:		if rate != 1 && size < c.nextSample {
      10ms       10ms   1206:			c.nextSample -= size
         .          .   1207:		} else {
         .       10ms   1208:			profilealloc(mp, x, size)
         .          .   1209:		}
         .          .   1210:	}
         .          .   1211:	mp.mallocing = 0
         .          .   1212:	releasem(mp)
         .          .   1213:
ROUTINE ======================== runtime.mcall in /usr/local/go/src/runtime/asm_amd64.s
      40ms      2.50s (flat, cum) 17.53% of Total
      10ms       10ms    428:TEXT runtime·mcall<ABIInternal>(SB), NOSPLIT, $0-8
         .          .    429:	MOVQ	AX, DX	// DX = fn
         .          .    430:
         .          .    431:	// Save state in g->sched. The caller's SP and PC are restored by gogo to
         .          .    432:	// resume execution in the caller's frame (implicit return). The caller's BP
         .          .    433:	// is also restored to support frame pointer unwinding.
         .          .    434:	MOVQ	SP, BX	// hide (SP) reads from vet
         .          .    435:	MOVQ	8(BX), BX	// caller's PC
         .          .    436:	MOVQ	BX, (g_sched+gobuf_pc)(R14)
         .          .    437:	LEAQ	fn+0(FP), BX	// caller's SP
         .          .    438:	MOVQ	BX, (g_sched+gobuf_sp)(R14)
         .          .    439:	// Get the caller's frame pointer by dereferencing BP. Storing BP as it is
         .          .    440:	// can cause a frame pointer cycle, see CL 476235.
         .          .    441:	MOVQ	(BP), BX // caller's BP
         .          .    442:	MOVQ	BX, (g_sched+gobuf_bp)(R14)
         .          .    443:
         .          .    444:	// switch to m->g0 & its stack, call fn
         .          .    445:	MOVQ	g_m(R14), BX
         .          .    446:	MOVQ	m_g0(BX), SI	// SI = g.m.g0
      10ms       10ms    447:	CMPQ	SI, R14	// if g == m->g0 call badmcall
         .          .    448:	JNE	goodm
         .          .    449:	JMP	runtime·badmcall(SB)
         .          .    450:goodm:
         .          .    451:	MOVQ	R14, AX		// AX (and arg 0) = g
         .          .    452:	MOVQ	SI, R14		// g = g.m.g0
         .          .    453:	get_tls(CX)		// Set G in TLS
         .          .    454:	MOVQ	R14, g(CX)
         .          .    455:	MOVQ	(g_sched+gobuf_sp)(R14), SP	// sp = g0.sched.sp
      10ms       10ms    456:	PUSHQ	AX	// open up space for fn's arg spill slot
      10ms       10ms    457:	MOVQ	0(DX), R12
         .      2.46s    458:	CALL	R12		// fn(g)
         .          .    459:	POPQ	AX
         .          .    460:	JMP	runtime·badmcall2(SB)
         .          .    461:	RET
         .          .    462:
         .          .    463:// systemstack_switch is a dummy routine that systemstack leaves at the bottom
ROUTINE ======================== runtime.nanotime in /usr/local/go/src/runtime/time_nofake.go
      30ms       30ms (flat, cum)  0.21% of Total
         .          .     18:func nanotime() int64 {
      30ms       30ms     19:	return nanotime1()
         .          .     20:}
         .          .     21:
         .          .     22:var overrideWrite func(fd uintptr, p unsafe.Pointer, n int32) int32
         .          .     23:
         .          .     24:// write must be nosplit on Windows (see write1)
ROUTINE ======================== runtime.nanotime1 in /usr/local/go/src/runtime/sys_linux_amd64.s
      30ms       30ms (flat, cum)  0.21% of Total
         .          .    223:TEXT runtime·nanotime1(SB),NOSPLIT,$16-8
         .          .    224:	// We don't know how much stack space the VDSO code will need,
         .          .    225:	// so switch to g0.
         .          .    226:	// In particular, a kernel configured with CONFIG_OPTIMIZE_INLINING=n
         .          .    227:	// and hardening can use a full page of stack space in gettime_sym
         .          .    228:	// due to stack probes inserted to avoid stack/heap collisions.
         .          .    229:	// See issue #20427.
         .          .    230:
         .          .    231:	MOVQ	SP, R12	// Save old SP; R12 unchanged by C code.
         .          .    232:
         .          .    233:	MOVQ	g_m(R14), BX // BX unchanged by C code.
         .          .    234:
         .          .    235:	// Set vdsoPC and vdsoSP for SIGPROF traceback.
         .          .    236:	// Save the old values on stack and restore them on exit,
         .          .    237:	// so this function is reentrant.
         .          .    238:	MOVQ	m_vdsoPC(BX), CX
      10ms       10ms    239:	MOVQ	m_vdsoSP(BX), DX
         .          .    240:	MOVQ	CX, 0(SP)
         .          .    241:	MOVQ	DX, 8(SP)
         .          .    242:
      20ms       20ms    243:	LEAQ	ret+0(FP), DX
         .          .    244:	MOVQ	-8(DX), CX
         .          .    245:	MOVQ	CX, m_vdsoPC(BX)
         .          .    246:	MOVQ	DX, m_vdsoSP(BX)
         .          .    247:
         .          .    248:	CMPQ	R14, m_curg(BX)	// Only switch if on curg.
ROUTINE ======================== runtime.newobject in /usr/local/go/src/runtime/malloc.go
         0       90ms (flat, cum)  0.63% of Total
         .          .   1323:func newobject(typ *_type) unsafe.Pointer {
         .       90ms   1324:	return mallocgc(typ.Size_, typ, true)
         .          .   1325:}
         .          .   1326:
         .          .   1327://go:linkname reflect_unsafe_New reflect.unsafe_New
         .          .   1328:func reflect_unsafe_New(typ *_type) unsafe.Pointer {
         .          .   1329:	return mallocgc(typ.Size_, typ, true)
ROUTINE ======================== runtime.newproc in /usr/local/go/src/runtime/proc.go
         0      1.35s (flat, cum)  9.47% of Total
         .          .   4477:func newproc(fn *funcval) {
         .          .   4478:	gp := getg()
         .          .   4479:	pc := getcallerpc()
         .      1.35s   4480:	systemstack(func() {
         .          .   4481:		newg := newproc1(fn, gp, pc)
         .          .   4482:
         .          .   4483:		pp := getg().m.p.ptr()
         .          .   4484:		runqput(pp, newg, true)
         .          .   4485:
ROUTINE ======================== runtime.newproc.func1 in /usr/local/go/src/runtime/proc.go
         0      1.34s (flat, cum)  9.40% of Total
         .          .   4480:	systemstack(func() {
         .      1.20s   4481:		newg := newproc1(fn, gp, pc)
         .          .   4482:
         .          .   4483:		pp := getg().m.p.ptr()
         .      100ms   4484:		runqput(pp, newg, true)
         .          .   4485:
         .          .   4486:		if mainStarted {
         .       40ms   4487:			wakep()
         .          .   4488:		}
         .          .   4489:	})
         .          .   4490:}
         .          .   4491:
         .          .   4492:// Create a new g in state _Grunnable, starting at fn. callerpc is the
ROUTINE ======================== runtime.newproc1 in /usr/local/go/src/runtime/proc.go
      40ms      1.20s (flat, cum)  8.42% of Total
      10ms       10ms   4495:func newproc1(fn *funcval, callergp *g, callerpc uintptr) *g {
         .          .   4496:	if fn == nil {
         .          .   4497:		fatal("go of nil func value")
         .          .   4498:	}
         .          .   4499:
         .          .   4500:	mp := acquirem() // disable preemption because we hold M and P in local vars.
         .          .   4501:	pp := mp.p.ptr()
         .      770ms   4502:	newg := gfget(pp)
         .          .   4503:	if newg == nil {
         .       20ms   4504:		newg = malg(stackMin)
         .          .   4505:		casgstatus(newg, _Gidle, _Gdead)
         .          .   4506:		allgadd(newg) // publishes with a g->status of Gdead so GC scanner doesn't look at uninitialized stack.
         .          .   4507:	}
         .          .   4508:	if newg.stack.hi == 0 {
         .          .   4509:		throw("newproc1: newg missing stack")
         .          .   4510:	}
         .          .   4511:
      10ms       10ms   4512:	if readgstatus(newg) != _Gdead {
         .          .   4513:		throw("newproc1: new g is not Gdead")
         .          .   4514:	}
         .          .   4515:
         .          .   4516:	totalSize := uintptr(4*goarch.PtrSize + sys.MinFrameSize) // extra space in case of reads slightly beyond frame
         .          .   4517:	totalSize = alignUp(totalSize, sys.StackAlign)
         .          .   4518:	sp := newg.stack.hi - totalSize
         .          .   4519:	spArg := sp
         .          .   4520:	if usesLR {
         .          .   4521:		// caller's LR
         .          .   4522:		*(*uintptr)(unsafe.Pointer(sp)) = 0
         .          .   4523:		prepGoExitFrame(sp)
         .          .   4524:		spArg += sys.MinFrameSize
         .          .   4525:	}
         .          .   4526:
         .          .   4527:	memclrNoHeapPointers(unsafe.Pointer(&newg.sched), unsafe.Sizeof(newg.sched))
         .          .   4528:	newg.sched.sp = sp
         .          .   4529:	newg.stktopsp = sp
         .          .   4530:	newg.sched.pc = abi.FuncPCABI0(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function
      10ms       10ms   4531:	newg.sched.g = guintptr(unsafe.Pointer(newg))
         .      210ms   4532:	gostartcallfn(&newg.sched, fn)
         .          .   4533:	newg.parentGoid = callergp.goid
         .          .   4534:	newg.gopc = callerpc
         .          .   4535:	newg.ancestors = saveAncestors(callergp)
         .          .   4536:	newg.startpc = fn.fn
         .       50ms   4537:	if isSystemGoroutine(newg, false) {
         .          .   4538:		sched.ngsys.Add(1)
         .          .   4539:	} else {
         .          .   4540:		// Only user goroutines inherit pprof labels.
         .          .   4541:		if mp.curg != nil {
         .          .   4542:			newg.labels = mp.curg.labels
         .          .   4543:		}
         .          .   4544:		if goroutineProfile.active {
         .          .   4545:			// A concurrent goroutine profile is running. It should include
         .          .   4546:			// exactly the set of goroutines that were alive when the goroutine
         .          .   4547:			// profiler first stopped the world. That does not include newg, so
         .          .   4548:			// mark it as not needing a profile before transitioning it from
         .          .   4549:			// _Gdead.
         .          .   4550:			newg.goroutineProfiled.Store(goroutineProfileSatisfied)
         .          .   4551:		}
         .          .   4552:	}
         .          .   4553:	// Track initial transition?
         .       10ms   4554:	newg.trackingSeq = uint8(fastrand())
         .          .   4555:	if newg.trackingSeq%gTrackingPeriod == 0 {
         .          .   4556:		newg.tracking = true
         .          .   4557:	}
      10ms       60ms   4558:	casgstatus(newg, _Gdead, _Grunnable)
         .       50ms   4559:	gcController.addScannableStack(pp, int64(newg.stack.hi-newg.stack.lo))
         .          .   4560:
         .          .   4561:	if pp.goidcache == pp.goidcacheend {
         .          .   4562:		// Sched.goidgen is the last allocated id,
         .          .   4563:		// this batch must be [sched.goidgen+1, sched.goidgen+GoidCacheBatch].
         .          .   4564:		// At startup sched.goidgen=0, so main goroutine receives goid=1.
ROUTINE ======================== runtime.notewakeup in /usr/local/go/src/runtime/lock_futex.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    139:func notewakeup(n *note) {
         .          .    140:	old := atomic.Xchg(key32(&n.key), 1)
         .          .    141:	if old != 0 {
         .          .    142:		print("notewakeup - double wakeup (", old, ")\n")
         .          .    143:		throw("notewakeup - double wakeup")
         .          .    144:	}
         .       10ms    145:	futexwakeup(key32(&n.key), 1)
         .          .    146:}
         .          .    147:
         .          .    148:func notesleep(n *note) {
         .          .    149:	gp := getg()
         .          .    150:	if gp != gp.m.g0 {
ROUTINE ======================== runtime.osyield in /usr/local/go/src/runtime/sys_linux_amd64.s
      40ms       40ms (flat, cum)  0.28% of Total
         .          .    653:TEXT runtime·osyield(SB),NOSPLIT,$0
         .          .    654:	MOVL	$SYS_sched_yield, AX
         .          .    655:	SYSCALL
      40ms       40ms    656:	RET
         .          .    657:
         .          .    658:TEXT runtime·sched_getaffinity(SB),NOSPLIT,$0
         .          .    659:	MOVQ	pid+0(FP), DI
         .          .    660:	MOVQ	len+8(FP), SI
         .          .    661:	MOVQ	buf+16(FP), DX
ROUTINE ======================== runtime.park_m in /usr/local/go/src/runtime/proc.go
         0      400ms (flat, cum)  2.81% of Total
         .          .   3721:func park_m(gp *g) {
         .          .   3722:	mp := getg().m
         .          .   3723:
         .          .   3724:	if traceEnabled() {
         .          .   3725:		traceGoPark(mp.waitTraceBlockReason, mp.waitTraceSkip)
         .          .   3726:	}
         .          .   3727:
         .          .   3728:	// N.B. Not using casGToWaiting here because the waitreason is
         .          .   3729:	// set by park_m's caller.
         .          .   3730:	casgstatus(gp, _Grunning, _Gwaiting)
         .          .   3731:	dropg()
         .          .   3732:
         .          .   3733:	if fn := mp.waitunlockf; fn != nil {
         .      350ms   3734:		ok := fn(gp, mp.waitlock)
         .          .   3735:		mp.waitunlockf = nil
         .          .   3736:		mp.waitlock = nil
         .          .   3737:		if !ok {
         .          .   3738:			if traceEnabled() {
         .          .   3739:				traceGoUnpark(gp, 2)
         .          .   3740:			}
         .          .   3741:			casgstatus(gp, _Gwaiting, _Grunnable)
         .          .   3742:			execute(gp, true) // Schedule it back, never returns.
         .          .   3743:		}
         .          .   3744:	}
         .       50ms   3745:	schedule()
         .          .   3746:}
         .          .   3747:
         .          .   3748:func goschedImpl(gp *g) {
         .          .   3749:	status := readgstatus(gp)
         .          .   3750:	if status&^_Gscan != _Grunning {
ROUTINE ======================== runtime.pcvalue in /usr/local/go/src/runtime/symtab.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    846:func pcvalue(f funcInfo, off uint32, targetpc uintptr, cache *pcvalueCache, strict bool) (int32, uintptr) {
         .          .    847:	if off == 0 {
         .          .    848:		return -1, 0
         .          .    849:	}
         .          .    850:
         .          .    851:	// Check the cache. This speeds up walks of deep stacks, which
         .          .    852:	// tend to have the same recursive functions over and over.
         .          .    853:	//
         .          .    854:	// This cache is small enough that full associativity is
         .          .    855:	// cheaper than doing the hashing for a less associative
         .          .    856:	// cache.
         .          .    857:	if cache != nil {
         .          .    858:		x := pcvalueCacheKey(targetpc)
         .          .    859:		for i := range cache.entries[x] {
         .          .    860:			// We check off first because we're more
         .          .    861:			// likely to have multiple entries with
         .          .    862:			// different offsets for the same targetpc
         .          .    863:			// than the other way around, so we'll usually
         .          .    864:			// fail in the first clause.
         .          .    865:			ent := &cache.entries[x][i]
         .          .    866:			if ent.off == off && ent.targetpc == targetpc {
         .          .    867:				return ent.val, 0
         .          .    868:			}
         .          .    869:		}
         .          .    870:	}
         .          .    871:
         .          .    872:	if !f.valid() {
         .          .    873:		if strict && panicking.Load() == 0 {
         .          .    874:			println("runtime: no module data for", hex(f.entry()))
         .          .    875:			throw("no module data")
         .          .    876:		}
         .          .    877:		return -1, 0
         .          .    878:	}
         .          .    879:	datap := f.datap
         .          .    880:	p := datap.pctab[off:]
         .          .    881:	pc := f.entry()
         .          .    882:	prevpc := pc
         .          .    883:	val := int32(-1)
         .          .    884:	for {
         .          .    885:		var ok bool
         .       10ms    886:		p, ok = step(p, &pc, &val, pc == f.entry())
         .          .    887:		if !ok {
         .          .    888:			break
         .          .    889:		}
         .          .    890:		if targetpc < pc {
         .          .    891:			// Replace a random entry in the cache. Random
ROUTINE ======================== runtime.preemptM in /usr/local/go/src/runtime/signal_unix.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    368:func preemptM(mp *m) {
         .          .    369:	// On Darwin, don't try to preempt threads during exec.
         .          .    370:	// Issue #41702.
         .          .    371:	if GOOS == "darwin" || GOOS == "ios" {
         .          .    372:		execLock.rlock()
         .          .    373:	}
         .          .    374:
         .          .    375:	if mp.signalPending.CompareAndSwap(0, 1) {
         .          .    376:		if GOOS == "darwin" || GOOS == "ios" {
         .          .    377:			pendingPreemptSignals.Add(1)
         .          .    378:		}
         .          .    379:
         .          .    380:		// If multiple threads are preempting the same M, it may send many
         .          .    381:		// signals to the same M such that it hardly make progress, causing
         .          .    382:		// live-lock problem. Apparently this could happen on darwin. See
         .          .    383:		// issue #37741.
         .          .    384:		// Only send a signal if there isn't already one pending.
         .       10ms    385:		signalM(mp, sigPreempt)
         .          .    386:	}
         .          .    387:
         .          .    388:	if GOOS == "darwin" || GOOS == "ios" {
         .          .    389:		execLock.runlock()
         .          .    390:	}
ROUTINE ======================== runtime.preemptone in /usr/local/go/src/runtime/proc.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   5769:func preemptone(pp *p) bool {
         .          .   5770:	mp := pp.m.ptr()
         .          .   5771:	if mp == nil || mp == getg().m {
         .          .   5772:		return false
         .          .   5773:	}
         .          .   5774:	gp := mp.curg
         .          .   5775:	if gp == nil || gp == mp.g0 {
         .          .   5776:		return false
         .          .   5777:	}
         .          .   5778:
         .          .   5779:	gp.preempt = true
         .          .   5780:
         .          .   5781:	// Every call in a goroutine checks for stack overflow by
         .          .   5782:	// comparing the current stack pointer to gp->stackguard0.
         .          .   5783:	// Setting gp->stackguard0 to StackPreempt folds
         .          .   5784:	// preemption into the normal stack overflow check.
         .          .   5785:	gp.stackguard0 = stackPreempt
         .          .   5786:
         .          .   5787:	// Request an async preemption of this P.
         .          .   5788:	if preemptMSupported && debug.asyncpreemptoff == 0 {
         .          .   5789:		pp.preempt = true
         .       10ms   5790:		preemptM(mp)
         .          .   5791:	}
         .          .   5792:
         .          .   5793:	return true
         .          .   5794:}
         .          .   5795:
ROUTINE ======================== runtime.procyield in /usr/local/go/src/runtime/asm_amd64.s
     350ms      350ms (flat, cum)  2.45% of Total
         .          .    775:TEXT runtime·procyield(SB),NOSPLIT,$0-0
         .          .    776:	MOVL	cycles+0(FP), AX
         .          .    777:again:
         .          .    778:	PAUSE
     340ms      340ms    779:	SUBL	$1, AX
      10ms       10ms    780:	JNZ	again
         .          .    781:	RET
         .          .    782:
         .          .    783:
         .          .    784:TEXT ·publicationBarrier<ABIInternal>(SB),NOSPLIT,$0-0
         .          .    785:	// Stores are already ordered on x86, so this is just a
ROUTINE ======================== runtime.profilealloc in /usr/local/go/src/runtime/malloc.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   1354:func profilealloc(mp *m, x unsafe.Pointer, size uintptr) {
         .          .   1355:	c := getMCache(mp)
         .          .   1356:	if c == nil {
         .          .   1357:		throw("profilealloc called without a P or outside bootstrapping")
         .          .   1358:	}
         .          .   1359:	c.nextSample = nextSample()
         .       10ms   1360:	mProf_Malloc(x, size)
         .          .   1361:}
         .          .   1362:
         .          .   1363:// nextSample returns the next sampling point for heap profiling. The goal is
         .          .   1364:// to sample allocations on average every MemProfileRate bytes, but with a
         .          .   1365:// completely random distribution over the allocation timeline; this
ROUTINE ======================== runtime.resetspinning in /usr/local/go/src/runtime/proc.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   3453:func resetspinning() {
         .          .   3454:	gp := getg()
         .          .   3455:	if !gp.m.spinning {
         .          .   3456:		throw("resetspinning: not a spinning m")
         .          .   3457:	}
         .          .   3458:	gp.m.spinning = false
         .          .   3459:	nmspinning := sched.nmspinning.Add(-1)
         .          .   3460:	if nmspinning < 0 {
         .          .   3461:		throw("findrunnable: negative nmspinning")
         .          .   3462:	}
         .          .   3463:	// M wakeup policy is deliberately somewhat conservative, so check if we
         .          .   3464:	// need to wakeup another P here. See "Worker thread parking/unparking"
         .          .   3465:	// comment at the top of the file for details.
         .       10ms   3466:	wakep()
         .          .   3467:}
         .          .   3468:
         .          .   3469:// injectglist adds each runnable G on the list to some run queue,
         .          .   3470:// and clears glist. If there is no current P, they are added to the
         .          .   3471:// global queue, and up to npidle M's are started to run them.
ROUTINE ======================== runtime.runqget in /usr/local/go/src/runtime/proc.go
      20ms       20ms (flat, cum)  0.14% of Total
         .          .   6311:func runqget(pp *p) (gp *g, inheritTime bool) {
         .          .   6312:	// If there's a runnext, it's the next G to run.
         .          .   6313:	next := pp.runnext
         .          .   6314:	// If the runnext is non-0 and the CAS fails, it could only have been stolen by another P,
         .          .   6315:	// because other Ps can race to set runnext to 0, but only the current P can set it to non-0.
         .          .   6316:	// Hence, there's no need to retry this CAS if it fails.
         .          .   6317:	if next != 0 && pp.runnext.cas(next, 0) {
         .          .   6318:		return next.ptr(), true
         .          .   6319:	}
         .          .   6320:
         .          .   6321:	for {
         .          .   6322:		h := atomic.LoadAcq(&pp.runqhead) // load-acquire, synchronize with other consumers
         .          .   6323:		t := pp.runqtail
         .          .   6324:		if t == h {
         .          .   6325:			return nil, false
         .          .   6326:		}
         .          .   6327:		gp := pp.runq[h%uint32(len(pp.runq))].ptr()
      20ms       20ms   6328:		if atomic.CasRel(&pp.runqhead, h, h+1) { // cas-release, commits consume
         .          .   6329:			return gp, false
         .          .   6330:		}
         .          .   6331:	}
         .          .   6332:}
         .          .   6333:
ROUTINE ======================== runtime.runqput in /usr/local/go/src/runtime/proc.go
      60ms      150ms (flat, cum)  1.05% of Total
         .          .   6199:func runqput(pp *p, gp *g, next bool) {
         .          .   6200:	if randomizeScheduler && next && fastrandn(2) == 0 {
         .          .   6201:		next = false
         .          .   6202:	}
         .          .   6203:
         .          .   6204:	if next {
         .          .   6205:	retryNext:
         .          .   6206:		oldnext := pp.runnext
         .       60ms   6207:		if !pp.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) {
         .          .   6208:			goto retryNext
         .          .   6209:		}
         .          .   6210:		if oldnext == 0 {
         .          .   6211:			return
         .          .   6212:		}
         .          .   6213:		// Kick the old runnext out to the regular run queue.
         .          .   6214:		gp = oldnext.ptr()
         .          .   6215:	}
         .          .   6216:
         .          .   6217:retry:
         .          .   6218:	h := atomic.LoadAcq(&pp.runqhead) // load-acquire, synchronize with consumers
         .          .   6219:	t := pp.runqtail
         .          .   6220:	if t-h < uint32(len(pp.runq)) {
         .          .   6221:		pp.runq[t%uint32(len(pp.runq))].set(gp)
      10ms       10ms   6222:		atomic.StoreRel(&pp.runqtail, t+1) // store-release, makes the item available for consumption
      50ms       50ms   6223:		return
         .          .   6224:	}
         .       30ms   6225:	if runqputslow(pp, gp, h, t) {
         .          .   6226:		return
         .          .   6227:	}
         .          .   6228:	// the queue is not full, now the put above must succeed
         .          .   6229:	goto retry
         .          .   6230:}
ROUTINE ======================== runtime.runqputslow in /usr/local/go/src/runtime/proc.go
      10ms       30ms (flat, cum)  0.21% of Total
         .          .   6234:func runqputslow(pp *p, gp *g, h, t uint32) bool {
         .          .   6235:	var batch [len(pp.runq)/2 + 1]*g
         .          .   6236:
         .          .   6237:	// First, grab a batch from local queue.
         .          .   6238:	n := t - h
         .          .   6239:	n = n / 2
         .          .   6240:	if n != uint32(len(pp.runq)/2) {
         .          .   6241:		throw("runqputslow: queue is not full")
         .          .   6242:	}
         .          .   6243:	for i := uint32(0); i < n; i++ {
         .          .   6244:		batch[i] = pp.runq[(h+i)%uint32(len(pp.runq))].ptr()
         .          .   6245:	}
         .          .   6246:	if !atomic.CasRel(&pp.runqhead, h, h+n) { // cas-release, commits consume
         .          .   6247:		return false
         .          .   6248:	}
         .          .   6249:	batch[n] = gp
         .          .   6250:
         .          .   6251:	if randomizeScheduler {
         .          .   6252:		for i := uint32(1); i <= n; i++ {
         .          .   6253:			j := fastrandn(i + 1)
         .          .   6254:			batch[i], batch[j] = batch[j], batch[i]
         .          .   6255:		}
         .          .   6256:	}
         .          .   6257:
         .          .   6258:	// Link the goroutines.
         .          .   6259:	for i := uint32(0); i < n; i++ {
      10ms       10ms   6260:		batch[i].schedlink.set(batch[i+1])
         .          .   6261:	}
         .          .   6262:	var q gQueue
         .          .   6263:	q.head.set(batch[0])
         .          .   6264:	q.tail.set(batch[n])
         .          .   6265:
         .          .   6266:	// Now put the batch on global queue.
         .       10ms   6267:	lock(&sched.lock)
         .          .   6268:	globrunqputbatch(&q, int32(n+1))
         .       10ms   6269:	unlock(&sched.lock)
         .          .   6270:	return true
         .          .   6271:}
         .          .   6272:
         .          .   6273:// runqputbatch tries to put all the G's on q on the local runnable queue.
         .          .   6274:// If the queue is full, they are put on the global queue; in that case
ROUTINE ======================== runtime.scanobject in /usr/local/go/src/runtime/mgcmark.go
      20ms       80ms (flat, cum)  0.56% of Total
         .          .   1256:func scanobject(b uintptr, gcw *gcWork) {
         .          .   1257:	// Prefetch object before we scan it.
         .          .   1258:	//
         .          .   1259:	// This will overlap fetching the beginning of the object with initial
         .          .   1260:	// setup before we start scanning the object.
         .          .   1261:	sys.Prefetch(b)
         .          .   1262:
         .          .   1263:	// Find the bits for b and the size of the object at b.
         .          .   1264:	//
         .          .   1265:	// b is either the beginning of an object, in which case this
         .          .   1266:	// is the size of the object to scan, or it points to an
         .          .   1267:	// oblet, in which case we compute the size to scan below.
         .          .   1268:	s := spanOfUnchecked(b)
         .          .   1269:	n := s.elemsize
         .          .   1270:	if n == 0 {
         .          .   1271:		throw("scanobject n == 0")
         .          .   1272:	}
         .          .   1273:	if s.spanclass.noscan() {
         .          .   1274:		// Correctness-wise this is ok, but it's inefficient
         .          .   1275:		// if noscan objects reach here.
         .          .   1276:		throw("scanobject of a noscan object")
         .          .   1277:	}
         .          .   1278:
         .          .   1279:	if n > maxObletBytes {
         .          .   1280:		// Large object. Break into oblets for better
         .          .   1281:		// parallelism and lower latency.
         .          .   1282:		if b == s.base() {
         .          .   1283:			// Enqueue the other oblets to scan later.
         .          .   1284:			// Some oblets may be in b's scalar tail, but
         .          .   1285:			// these will be marked as "no more pointers",
         .          .   1286:			// so we'll drop out immediately when we go to
         .          .   1287:			// scan those.
         .          .   1288:			for oblet := b + maxObletBytes; oblet < s.base()+s.elemsize; oblet += maxObletBytes {
         .          .   1289:				if !gcw.putFast(oblet) {
         .          .   1290:					gcw.put(oblet)
         .          .   1291:				}
         .          .   1292:			}
         .          .   1293:		}
         .          .   1294:
         .          .   1295:		// Compute the size of the oblet. Since this object
         .          .   1296:		// must be a large object, s.base() is the beginning
         .          .   1297:		// of the object.
         .          .   1298:		n = s.base() + s.elemsize - b
         .          .   1299:		if n > maxObletBytes {
         .          .   1300:			n = maxObletBytes
         .          .   1301:		}
         .          .   1302:	}
         .          .   1303:
         .          .   1304:	hbits := heapBitsForAddr(b, n)
         .          .   1305:	var scanSize uintptr
         .          .   1306:	for {
         .          .   1307:		var addr uintptr
         .       10ms   1308:		if hbits, addr = hbits.nextFast(); addr == 0 {
         .          .   1309:			if hbits, addr = hbits.next(); addr == 0 {
         .          .   1310:				break
         .          .   1311:			}
         .          .   1312:		}
         .          .   1313:
         .          .   1314:		// Keep track of farthest pointer we found, so we can
         .          .   1315:		// update heapScanWork. TODO: is there a better metric,
         .          .   1316:		// now that we can skip scalar portions pretty efficiently?
      20ms       20ms   1317:		scanSize = addr - b + goarch.PtrSize
         .          .   1318:
         .          .   1319:		// Work here is duplicated in scanblock and above.
         .          .   1320:		// If you make changes here, make changes there too.
         .          .   1321:		obj := *(*uintptr)(unsafe.Pointer(addr))
         .          .   1322:
         .          .   1323:		// At this point we have extracted the next potential pointer.
         .          .   1324:		// Quickly filter out nil and pointers back to the current object.
         .          .   1325:		if obj != 0 && obj-b >= n {
         .          .   1326:			// Test if obj points into the Go heap and, if so,
         .          .   1327:			// mark the object.
         .          .   1328:			//
         .          .   1329:			// Note that it's possible for findObject to
         .          .   1330:			// fail if obj points to a just-allocated heap
         .          .   1331:			// object because of a race with growing the
         .          .   1332:			// heap. In this case, we know the object was
         .          .   1333:			// just allocated and hence will be marked by
         .          .   1334:			// allocation itself.
         .       50ms   1335:			if obj, span, objIndex := findObject(obj, b, addr-b); obj != 0 {
         .          .   1336:				greyobject(obj, b, addr-b, span, gcw, objIndex)
         .          .   1337:			}
         .          .   1338:		}
         .          .   1339:	}
         .          .   1340:	gcw.bytesMarked += uint64(n)
ROUTINE ======================== runtime.schedule in /usr/local/go/src/runtime/proc.go
     180ms      1.86s (flat, cum) 13.04% of Total
         .          .   3553:func schedule() {
         .          .   3554:	mp := getg().m
         .          .   3555:
         .          .   3556:	if mp.locks != 0 {
         .          .   3557:		throw("schedule: holding locks")
         .          .   3558:	}
         .          .   3559:
         .          .   3560:	if mp.lockedg != 0 {
         .          .   3561:		stoplockedm()
         .          .   3562:		execute(mp.lockedg.ptr(), false) // Never returns.
         .          .   3563:	}
         .          .   3564:
         .          .   3565:	// We should not schedule away from a g that is executing a cgo call,
         .          .   3566:	// since the cgo call is using the m's g0 stack.
         .          .   3567:	if mp.incgo {
         .          .   3568:		throw("schedule: in cgo")
         .          .   3569:	}
         .          .   3570:
         .          .   3571:top:
         .          .   3572:	pp := mp.p.ptr()
         .          .   3573:	pp.preempt = false
         .          .   3574:
         .          .   3575:	// Safety check: if we are spinning, the run queue should be empty.
         .          .   3576:	// Check this before calling checkTimers, as that might call
         .          .   3577:	// goready to put a ready goroutine on the local run queue.
         .          .   3578:	if mp.spinning && (pp.runnext != 0 || pp.runqhead != pp.runqtail) {
         .          .   3579:		throw("schedule: spinning with local work")
         .          .   3580:	}
         .          .   3581:
      20ms      1.44s   3582:	gp, inheritTime, tryWakeP := findRunnable() // blocks until work is available
         .          .   3583:
         .          .   3584:	if debug.dontfreezetheworld > 0 && freezing.Load() {
         .          .   3585:		// See comment in freezetheworld. We don't want to perturb
         .          .   3586:		// scheduler state, so we didn't gcstopm in findRunnable, but
         .          .   3587:		// also don't want to allow new goroutines to run.
         .          .   3588:		//
         .          .   3589:		// Deadlock here rather than in the findRunnable loop so if
         .          .   3590:		// findRunnable is stuck in a loop we don't perturb that
         .          .   3591:		// either.
         .          .   3592:		lock(&deadlock)
         .          .   3593:		lock(&deadlock)
         .          .   3594:	}
         .          .   3595:
         .          .   3596:	// This thread is going to run a goroutine and is not spinning anymore,
         .          .   3597:	// so if it was marked as spinning we need to reset it now and potentially
         .          .   3598:	// start a new spinning M.
         .          .   3599:	if mp.spinning {
         .       10ms   3600:		resetspinning()
         .          .   3601:	}
         .          .   3602:
      10ms       10ms   3603:	if sched.disable.user && !schedEnabled(gp) {
         .          .   3604:		// Scheduling of this goroutine is disabled. Put it on
         .          .   3605:		// the list of pending runnable goroutines for when we
         .          .   3606:		// re-enable user scheduling and look again.
         .          .   3607:		lock(&sched.lock)
         .          .   3608:		if schedEnabled(gp) {
         .          .   3609:			// Something re-enabled scheduling while we
         .          .   3610:			// were acquiring the lock.
         .          .   3611:			unlock(&sched.lock)
         .          .   3612:		} else {
         .          .   3613:			sched.disable.runnable.pushBack(gp)
         .          .   3614:			sched.disable.n++
         .          .   3615:			unlock(&sched.lock)
         .          .   3616:			goto top
         .          .   3617:		}
         .          .   3618:	}
         .          .   3619:
         .          .   3620:	// If about to schedule a not-normal goroutine (a GCworker or tracereader),
         .          .   3621:	// wake a P if there is one.
         .          .   3622:	if tryWakeP {
         .          .   3623:		wakep()
         .          .   3624:	}
     150ms      150ms   3625:	if gp.lockedm != 0 {
         .          .   3626:		// Hands off own p to the locked m,
         .          .   3627:		// then blocks waiting for a new p.
         .          .   3628:		startlockedm(gp)
         .          .   3629:		goto top
         .          .   3630:	}
         .          .   3631:
         .      250ms   3632:	execute(gp, inheritTime)
         .          .   3633:}
         .          .   3634:
         .          .   3635:// dropg removes the association between m and the current goroutine m->curg (gp for short).
         .          .   3636:// Typically a caller sets gp's status away from Grunning and then
         .          .   3637:// immediately calls dropg to finish the job. The caller is also responsible
ROUTINE ======================== runtime.selectgo in /usr/local/go/src/runtime/select.go
      30ms      350ms (flat, cum)  2.45% of Total
         .          .    121:func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool) {
         .          .    122:	if debugSelect {
         .          .    123:		print("select: cas0=", cas0, "\n")
         .          .    124:	}
         .          .    125:
         .          .    126:	// NOTE: In order to maintain a lean stack size, the number of scases
         .          .    127:	// is capped at 65536.
         .          .    128:	cas1 := (*[1 << 16]scase)(unsafe.Pointer(cas0))
         .          .    129:	order1 := (*[1 << 17]uint16)(unsafe.Pointer(order0))
         .          .    130:
         .          .    131:	ncases := nsends + nrecvs
         .          .    132:	scases := cas1[:ncases:ncases]
         .          .    133:	pollorder := order1[:ncases:ncases]
      10ms       10ms    134:	lockorder := order1[ncases:][:ncases:ncases]
         .          .    135:	// NOTE: pollorder/lockorder's underlying array was not zero-initialized by compiler.
         .          .    136:
         .          .    137:	// Even when raceenabled is true, there might be select
         .          .    138:	// statements in packages compiled without -race (e.g.,
         .          .    139:	// ensureSigM in runtime/signal_unix.go).
         .          .    140:	var pcs []uintptr
         .          .    141:	if raceenabled && pc0 != nil {
         .          .    142:		pc1 := (*[1 << 16]uintptr)(unsafe.Pointer(pc0))
         .          .    143:		pcs = pc1[:ncases:ncases]
         .          .    144:	}
         .          .    145:	casePC := func(casi int) uintptr {
         .          .    146:		if pcs == nil {
         .          .    147:			return 0
         .          .    148:		}
         .          .    149:		return pcs[casi]
         .          .    150:	}
         .          .    151:
         .          .    152:	var t0 int64
         .          .    153:	if blockprofilerate > 0 {
         .          .    154:		t0 = cputicks()
         .          .    155:	}
         .          .    156:
         .          .    157:	// The compiler rewrites selects that statically have
         .          .    158:	// only 0 or 1 cases plus default into simpler constructs.
         .          .    159:	// The only way we can end up with such small sel.ncase
         .          .    160:	// values here is for a larger select in which most channels
         .          .    161:	// have been nilled out. The general code handles those
         .          .    162:	// cases correctly, and they are rare enough not to bother
         .          .    163:	// optimizing (and needing to test).
         .          .    164:
         .          .    165:	// generate permuted order
         .          .    166:	norder := 0
         .          .    167:	for i := range scases {
         .          .    168:		cas := &scases[i]
         .          .    169:
         .          .    170:		// Omit cases without channels from the poll and lock orders.
         .          .    171:		if cas.c == nil {
         .          .    172:			cas.elem = nil // allow GC
         .          .    173:			continue
         .          .    174:		}
         .          .    175:
         .          .    176:		j := fastrandn(uint32(norder + 1))
         .          .    177:		pollorder[norder] = pollorder[j]
         .          .    178:		pollorder[j] = uint16(i)
         .          .    179:		norder++
         .          .    180:	}
         .          .    181:	pollorder = pollorder[:norder]
         .          .    182:	lockorder = lockorder[:norder]
         .          .    183:
         .          .    184:	// sort the cases by Hchan address to get the locking order.
         .          .    185:	// simple heap sort, to guarantee n log n time and constant stack footprint.
         .          .    186:	for i := range lockorder {
         .          .    187:		j := i
         .          .    188:		// Start with the pollorder to permute cases on the same channel.
         .          .    189:		c := scases[pollorder[i]].c
         .          .    190:		for j > 0 && scases[lockorder[(j-1)/2]].c.sortkey() < c.sortkey() {
         .          .    191:			k := (j - 1) / 2
         .          .    192:			lockorder[j] = lockorder[k]
         .          .    193:			j = k
         .          .    194:		}
         .          .    195:		lockorder[j] = pollorder[i]
         .          .    196:	}
         .          .    197:	for i := len(lockorder) - 1; i >= 0; i-- {
         .          .    198:		o := lockorder[i]
         .          .    199:		c := scases[o].c
         .          .    200:		lockorder[i] = lockorder[0]
         .          .    201:		j := 0
         .          .    202:		for {
         .          .    203:			k := j*2 + 1
         .          .    204:			if k >= i {
         .          .    205:				break
         .          .    206:			}
         .          .    207:			if k+1 < i && scases[lockorder[k]].c.sortkey() < scases[lockorder[k+1]].c.sortkey() {
         .          .    208:				k++
         .          .    209:			}
         .          .    210:			if c.sortkey() < scases[lockorder[k]].c.sortkey() {
         .          .    211:				lockorder[j] = lockorder[k]
         .          .    212:				j = k
         .          .    213:				continue
         .          .    214:			}
         .          .    215:			break
         .          .    216:		}
         .          .    217:		lockorder[j] = o
         .          .    218:	}
         .          .    219:
         .          .    220:	if debugSelect {
         .          .    221:		for i := 0; i+1 < len(lockorder); i++ {
         .          .    222:			if scases[lockorder[i]].c.sortkey() > scases[lockorder[i+1]].c.sortkey() {
         .          .    223:				print("i=", i, " x=", lockorder[i], " y=", lockorder[i+1], "\n")
         .          .    224:				throw("select: broken sort")
         .          .    225:			}
         .          .    226:		}
         .          .    227:	}
         .          .    228:
         .          .    229:	// lock all the channels involved in the select
         .      160ms    230:	sellock(scases, lockorder)
         .          .    231:
         .          .    232:	var (
         .          .    233:		gp     *g
         .          .    234:		sg     *sudog
         .          .    235:		c      *hchan
         .          .    236:		k      *scase
         .          .    237:		sglist *sudog
         .          .    238:		sgnext *sudog
         .          .    239:		qp     unsafe.Pointer
         .          .    240:		nextp  **sudog
         .          .    241:	)
         .          .    242:
         .          .    243:	// pass 1 - look for something already waiting
         .          .    244:	var casi int
         .          .    245:	var cas *scase
         .          .    246:	var caseSuccess bool
         .          .    247:	var caseReleaseTime int64 = -1
         .          .    248:	var recvOK bool
         .          .    249:	for _, casei := range pollorder {
         .          .    250:		casi = int(casei)
         .          .    251:		cas = &scases[casi]
         .          .    252:		c = cas.c
         .          .    253:
         .          .    254:		if casi >= nsends {
         .          .    255:			sg = c.sendq.dequeue()
         .          .    256:			if sg != nil {
         .          .    257:				goto recv
         .          .    258:			}
         .          .    259:			if c.qcount > 0 {
         .          .    260:				goto bufrecv
         .          .    261:			}
         .          .    262:			if c.closed != 0 {
         .          .    263:				goto rclose
         .          .    264:			}
         .          .    265:		} else {
         .          .    266:			if raceenabled {
         .          .    267:				racereadpc(c.raceaddr(), casePC(casi), chansendpc)
         .          .    268:			}
         .          .    269:			if c.closed != 0 {
         .          .    270:				goto sclose
         .          .    271:			}
         .          .    272:			sg = c.recvq.dequeue()
         .          .    273:			if sg != nil {
         .          .    274:				goto send
         .          .    275:			}
         .          .    276:			if c.qcount < c.dataqsiz {
         .          .    277:				goto bufsend
         .          .    278:			}
         .          .    279:		}
         .          .    280:	}
         .          .    281:
         .          .    282:	if !block {
         .          .    283:		selunlock(scases, lockorder)
         .          .    284:		casi = -1
         .          .    285:		goto retc
         .          .    286:	}
         .          .    287:
         .          .    288:	// pass 2 - enqueue on all chans
         .          .    289:	gp = getg()
         .          .    290:	if gp.waiting != nil {
         .          .    291:		throw("gp.waiting != nil")
         .          .    292:	}
         .          .    293:	nextp = &gp.waiting
         .          .    294:	for _, casei := range lockorder {
         .          .    295:		casi = int(casei)
         .          .    296:		cas = &scases[casi]
         .          .    297:		c = cas.c
         .       20ms    298:		sg := acquireSudog()
         .          .    299:		sg.g = gp
         .          .    300:		sg.isSelect = true
         .          .    301:		// No stack splits between assigning elem and enqueuing
         .          .    302:		// sg on gp.waiting where copystack can find it.
         .          .    303:		sg.elem = cas.elem
         .          .    304:		sg.releasetime = 0
         .          .    305:		if t0 != 0 {
         .          .    306:			sg.releasetime = -1
         .          .    307:		}
         .          .    308:		sg.c = c
         .          .    309:		// Construct waiting list in lock order.
         .          .    310:		*nextp = sg
         .          .    311:		nextp = &sg.waitlink
         .          .    312:
         .          .    313:		if casi < nsends {
         .          .    314:			c.sendq.enqueue(sg)
         .          .    315:		} else {
         .          .    316:			c.recvq.enqueue(sg)
         .          .    317:		}
         .          .    318:	}
         .          .    319:
         .          .    320:	// wait for someone to wake us up
         .          .    321:	gp.param = nil
         .          .    322:	// Signal to anyone trying to shrink our stack that we're about
         .          .    323:	// to park on a channel. The window between when this G's status
         .          .    324:	// changes and when we set gp.activeStackChans is not safe for
         .          .    325:	// stack shrinking.
         .          .    326:	gp.parkingOnChan.Store(true)
         .          .    327:	gopark(selparkcommit, nil, waitReasonSelect, traceBlockSelect, 1)
      10ms       10ms    328:	gp.activeStackChans = false
         .          .    329:
         .      110ms    330:	sellock(scases, lockorder)
         .          .    331:
         .          .    332:	gp.selectDone.Store(0)
         .          .    333:	sg = (*sudog)(gp.param)
         .          .    334:	gp.param = nil
         .          .    335:
         .          .    336:	// pass 3 - dequeue from unsuccessful chans
         .          .    337:	// otherwise they stack up on quiet channels
         .          .    338:	// record the successful case, if any.
         .          .    339:	// We singly-linked up the SudoGs in lock order.
         .          .    340:	casi = -1
         .          .    341:	cas = nil
         .          .    342:	caseSuccess = false
         .          .    343:	sglist = gp.waiting
         .          .    344:	// Clear all elem before unlinking from gp.waiting.
         .          .    345:	for sg1 := gp.waiting; sg1 != nil; sg1 = sg1.waitlink {
         .          .    346:		sg1.isSelect = false
         .          .    347:		sg1.elem = nil
         .          .    348:		sg1.c = nil
         .          .    349:	}
         .          .    350:	gp.waiting = nil
         .          .    351:
         .          .    352:	for _, casei := range lockorder {
         .          .    353:		k = &scases[casei]
         .          .    354:		if sg == sglist {
         .          .    355:			// sg has already been dequeued by the G that woke us up.
         .          .    356:			casi = int(casei)
         .          .    357:			cas = k
         .          .    358:			caseSuccess = sglist.success
         .          .    359:			if sglist.releasetime > 0 {
         .          .    360:				caseReleaseTime = sglist.releasetime
         .          .    361:			}
         .          .    362:		} else {
         .          .    363:			c = k.c
         .          .    364:			if int(casei) < nsends {
         .       10ms    365:				c.sendq.dequeueSudoG(sglist)
         .          .    366:			} else {
         .          .    367:				c.recvq.dequeueSudoG(sglist)
         .          .    368:			}
         .          .    369:		}
         .          .    370:		sgnext = sglist.waitlink
         .          .    371:		sglist.waitlink = nil
         .          .    372:		releaseSudog(sglist)
         .          .    373:		sglist = sgnext
         .          .    374:	}
         .          .    375:
         .          .    376:	if cas == nil {
         .          .    377:		throw("selectgo: bad wakeup")
         .          .    378:	}
         .          .    379:
         .          .    380:	c = cas.c
         .          .    381:
         .          .    382:	if debugSelect {
         .          .    383:		print("wait-return: cas0=", cas0, " c=", c, " cas=", cas, " send=", casi < nsends, "\n")
         .          .    384:	}
         .          .    385:
         .          .    386:	if casi < nsends {
         .          .    387:		if !caseSuccess {
         .          .    388:			goto sclose
         .          .    389:		}
         .          .    390:	} else {
         .          .    391:		recvOK = caseSuccess
         .          .    392:	}
         .          .    393:
         .          .    394:	if raceenabled {
         .          .    395:		if casi < nsends {
         .          .    396:			raceReadObjectPC(c.elemtype, cas.elem, casePC(casi), chansendpc)
         .          .    397:		} else if cas.elem != nil {
         .          .    398:			raceWriteObjectPC(c.elemtype, cas.elem, casePC(casi), chanrecvpc)
         .          .    399:		}
         .          .    400:	}
         .          .    401:	if msanenabled {
         .          .    402:		if casi < nsends {
         .          .    403:			msanread(cas.elem, c.elemtype.Size_)
         .          .    404:		} else if cas.elem != nil {
         .          .    405:			msanwrite(cas.elem, c.elemtype.Size_)
         .          .    406:		}
         .          .    407:	}
         .          .    408:	if asanenabled {
         .          .    409:		if casi < nsends {
         .          .    410:			asanread(cas.elem, c.elemtype.Size_)
         .          .    411:		} else if cas.elem != nil {
         .          .    412:			asanwrite(cas.elem, c.elemtype.Size_)
         .          .    413:		}
         .          .    414:	}
         .          .    415:
         .       10ms    416:	selunlock(scases, lockorder)
         .          .    417:	goto retc
         .          .    418:
         .          .    419:bufrecv:
         .          .    420:	// can receive from buffer
         .          .    421:	if raceenabled {
         .          .    422:		if cas.elem != nil {
         .          .    423:			raceWriteObjectPC(c.elemtype, cas.elem, casePC(casi), chanrecvpc)
         .          .    424:		}
         .          .    425:		racenotify(c, c.recvx, nil)
         .          .    426:	}
         .          .    427:	if msanenabled && cas.elem != nil {
         .          .    428:		msanwrite(cas.elem, c.elemtype.Size_)
         .          .    429:	}
         .          .    430:	if asanenabled && cas.elem != nil {
         .          .    431:		asanwrite(cas.elem, c.elemtype.Size_)
         .          .    432:	}
         .          .    433:	recvOK = true
         .          .    434:	qp = chanbuf(c, c.recvx)
         .          .    435:	if cas.elem != nil {
         .          .    436:		typedmemmove(c.elemtype, cas.elem, qp)
         .          .    437:	}
         .          .    438:	typedmemclr(c.elemtype, qp)
         .          .    439:	c.recvx++
         .          .    440:	if c.recvx == c.dataqsiz {
         .          .    441:		c.recvx = 0
         .          .    442:	}
         .          .    443:	c.qcount--
         .          .    444:	selunlock(scases, lockorder)
         .          .    445:	goto retc
         .          .    446:
         .          .    447:bufsend:
         .          .    448:	// can send to buffer
         .          .    449:	if raceenabled {
         .          .    450:		racenotify(c, c.sendx, nil)
         .          .    451:		raceReadObjectPC(c.elemtype, cas.elem, casePC(casi), chansendpc)
         .          .    452:	}
         .          .    453:	if msanenabled {
         .          .    454:		msanread(cas.elem, c.elemtype.Size_)
         .          .    455:	}
         .          .    456:	if asanenabled {
         .          .    457:		asanread(cas.elem, c.elemtype.Size_)
         .          .    458:	}
         .          .    459:	typedmemmove(c.elemtype, chanbuf(c, c.sendx), cas.elem)
         .          .    460:	c.sendx++
         .          .    461:	if c.sendx == c.dataqsiz {
         .          .    462:		c.sendx = 0
         .          .    463:	}
         .          .    464:	c.qcount++
         .          .    465:	selunlock(scases, lockorder)
         .          .    466:	goto retc
         .          .    467:
         .          .    468:recv:
         .          .    469:	// can receive from sleeping sender (sg)
         .          .    470:	recv(c, sg, cas.elem, func() { selunlock(scases, lockorder) }, 2)
         .          .    471:	if debugSelect {
         .          .    472:		print("syncrecv: cas0=", cas0, " c=", c, "\n")
         .          .    473:	}
         .          .    474:	recvOK = true
         .          .    475:	goto retc
         .          .    476:
         .          .    477:rclose:
         .          .    478:	// read at end of closed channel
         .       10ms    479:	selunlock(scases, lockorder)
         .          .    480:	recvOK = false
         .          .    481:	if cas.elem != nil {
         .          .    482:		typedmemclr(c.elemtype, cas.elem)
         .          .    483:	}
         .          .    484:	if raceenabled {
         .          .    485:		raceacquire(c.raceaddr())
         .          .    486:	}
         .          .    487:	goto retc
         .          .    488:
         .          .    489:send:
         .          .    490:	// can send to a sleeping receiver (sg)
         .          .    491:	if raceenabled {
         .          .    492:		raceReadObjectPC(c.elemtype, cas.elem, casePC(casi), chansendpc)
         .          .    493:	}
         .          .    494:	if msanenabled {
         .          .    495:		msanread(cas.elem, c.elemtype.Size_)
         .          .    496:	}
         .          .    497:	if asanenabled {
         .          .    498:		asanread(cas.elem, c.elemtype.Size_)
         .          .    499:	}
         .          .    500:	send(c, sg, cas.elem, func() { selunlock(scases, lockorder) }, 2)
         .          .    501:	if debugSelect {
         .          .    502:		print("syncsend: cas0=", cas0, " c=", c, "\n")
         .          .    503:	}
         .          .    504:	goto retc
         .          .    505:
         .          .    506:retc:
         .          .    507:	if caseReleaseTime > 0 {
      10ms       10ms    508:		blockevent(caseReleaseTime-t0, 1)
         .          .    509:	}
         .          .    510:	return casi, recvOK
         .          .    511:
         .          .    512:sclose:
         .          .    513:	// send on closed channel
ROUTINE ======================== runtime.sellock in /usr/local/go/src/runtime/select.go
      20ms      270ms (flat, cum)  1.89% of Total
         .          .     33:func sellock(scases []scase, lockorder []uint16) {
         .          .     34:	var c *hchan
      10ms       10ms     35:	for _, o := range lockorder {
      10ms       10ms     36:		c0 := scases[o].c
         .          .     37:		if c0 != c {
         .          .     38:			c = c0
         .      250ms     39:			lock(&c.lock)
         .          .     40:		}
         .          .     41:	}
         .          .     42:}
         .          .     43:
         .          .     44:func selunlock(scases []scase, lockorder []uint16) {
ROUTINE ======================== runtime.selparkcommit in /usr/local/go/src/runtime/select.go
         0      350ms (flat, cum)  2.45% of Total
         .          .     62:func selparkcommit(gp *g, _ unsafe.Pointer) bool {
         .          .     63:	// There are unlocked sudogs that point into gp's stack. Stack
         .          .     64:	// copying must lock the channels of those sudogs.
         .          .     65:	// Set activeStackChans here instead of before we try parking
         .          .     66:	// because we could self-deadlock in stack growth on a
         .          .     67:	// channel lock.
         .          .     68:	gp.activeStackChans = true
         .          .     69:	// Mark that it's safe for stack shrinking to occur now,
         .          .     70:	// because any thread acquiring this G's stack for shrinking
         .          .     71:	// is guaranteed to observe activeStackChans after this store.
         .          .     72:	gp.parkingOnChan.Store(false)
         .          .     73:	// Make sure we unlock after setting activeStackChans and
         .          .     74:	// unsetting parkingOnChan. The moment we unlock any of the
         .          .     75:	// channel locks we risk gp getting readied by a channel operation
         .          .     76:	// and so gp could continue running before everything before the
         .          .     77:	// unlock is visible (even to gp itself).
         .          .     78:
         .          .     79:	// This must not access gp's stack (see gopark). In
         .          .     80:	// particular, it must not access the *hselect. That's okay,
         .          .     81:	// because by the time this is called, gp.waiting has all
         .          .     82:	// channels in lock order.
         .          .     83:	var lastc *hchan
         .          .     84:	for sg := gp.waiting; sg != nil; sg = sg.waitlink {
         .          .     85:		if sg.c != lastc && lastc != nil {
         .          .     86:			// As soon as we unlock the channel, fields in
         .          .     87:			// any sudog with that channel may change,
         .          .     88:			// including c and waitlink. Since multiple
         .          .     89:			// sudogs may have the same channel, we unlock
         .          .     90:			// only after we've passed the last instance
         .          .     91:			// of a channel.
         .      230ms     92:			unlock(&lastc.lock)
         .          .     93:		}
         .          .     94:		lastc = sg.c
         .          .     95:	}
         .          .     96:	if lastc != nil {
         .      120ms     97:		unlock(&lastc.lock)
         .          .     98:	}
         .          .     99:	return true
         .          .    100:}
         .          .    101:
         .          .    102:func block() {
ROUTINE ======================== runtime.selunlock in /usr/local/go/src/runtime/select.go
         0       20ms (flat, cum)  0.14% of Total
         .          .     44:func selunlock(scases []scase, lockorder []uint16) {
         .          .     45:	// We must be very careful here to not touch sel after we have unlocked
         .          .     46:	// the last lock, because sel can be freed right after the last unlock.
         .          .     47:	// Consider the following situation.
         .          .     48:	// First M calls runtime·park() in runtime·selectgo() passing the sel.
         .          .     49:	// Once runtime·park() has unlocked the last lock, another M makes
         .          .     50:	// the G that calls select runnable again and schedules it for execution.
         .          .     51:	// When the G runs on another M, it locks all the locks and frees sel.
         .          .     52:	// Now if the first M touches sel, it will access freed memory.
         .          .     53:	for i := len(lockorder) - 1; i >= 0; i-- {
         .          .     54:		c := scases[lockorder[i]].c
         .          .     55:		if i > 0 && c == scases[lockorder[i-1]].c {
         .          .     56:			continue // will unlock it on the next iteration
         .          .     57:		}
         .       20ms     58:		unlock(&c.lock)
         .          .     59:	}
         .          .     60:}
         .          .     61:
         .          .     62:func selparkcommit(gp *g, _ unsafe.Pointer) bool {
         .          .     63:	// There are unlocked sudogs that point into gp's stack. Stack
ROUTINE ======================== runtime.signalM in /usr/local/go/src/runtime/os_linux.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    575:func signalM(mp *m, sig int) {
         .       10ms    576:	tgkill(getpid(), int(mp.procid), sig)
         .          .    577:}
         .          .    578:
         .          .    579:// validSIGPROF compares this signal delivery's code against the signal sources
         .          .    580:// that the profiler uses, returning whether the delivery should be processed.
         .          .    581:// To be processed, a signal delivery from a known profiling mechanism should
ROUTINE ======================== runtime.spanOf in /usr/local/go/src/runtime/mheap.go
      40ms       40ms (flat, cum)  0.28% of Total
         .          .    682:func spanOf(p uintptr) *mspan {
         .          .    683:	// This function looks big, but we use a lot of constant
         .          .    684:	// folding around arenaL1Bits to get it under the inlining
         .          .    685:	// budget. Also, many of the checks here are safety checks
         .          .    686:	// that Go needs to do anyway, so the generated code is quite
         .          .    687:	// short.
         .          .    688:	ri := arenaIndex(p)
         .          .    689:	if arenaL1Bits == 0 {
         .          .    690:		// If there's no L1, then ri.l1() can't be out of bounds but ri.l2() can.
      10ms       10ms    691:		if ri.l2() >= uint(len(mheap_.arenas[0])) {
         .          .    692:			return nil
         .          .    693:		}
         .          .    694:	} else {
         .          .    695:		// If there's an L1, then ri.l1() can be out of bounds but ri.l2() can't.
         .          .    696:		if ri.l1() >= uint(len(mheap_.arenas)) {
         .          .    697:			return nil
         .          .    698:		}
         .          .    699:	}
         .          .    700:	l2 := mheap_.arenas[ri.l1()]
         .          .    701:	if arenaL1Bits != 0 && l2 == nil { // Should never happen if there's no L1.
         .          .    702:		return nil
         .          .    703:	}
      20ms       20ms    704:	ha := l2[ri.l2()]
         .          .    705:	if ha == nil {
         .          .    706:		return nil
         .          .    707:	}
      10ms       10ms    708:	return ha.spans[(p/pageSize)%pagesPerArena]
         .          .    709:}
         .          .    710:
         .          .    711:// spanOfUnchecked is equivalent to spanOf, but the caller must ensure
         .          .    712:// that p points into an allocated heap arena.
         .          .    713://
ROUTINE ======================== runtime.startm in /usr/local/go/src/runtime/proc.go
         0       10ms (flat, cum)  0.07% of Total
         .          .   2563:func startm(pp *p, spinning, lockheld bool) {
         .          .   2564:	// Disable preemption.
         .          .   2565:	//
         .          .   2566:	// Every owned P must have an owner that will eventually stop it in the
         .          .   2567:	// event of a GC stop request. startm takes transient ownership of a P
         .          .   2568:	// (either from argument or pidleget below) and transfers ownership to
         .          .   2569:	// a started M, which will be responsible for performing the stop.
         .          .   2570:	//
         .          .   2571:	// Preemption must be disabled during this transient ownership,
         .          .   2572:	// otherwise the P this is running on may enter GC stop while still
         .          .   2573:	// holding the transient P, leaving that P in limbo and deadlocking the
         .          .   2574:	// STW.
         .          .   2575:	//
         .          .   2576:	// Callers passing a non-nil P must already be in non-preemptible
         .          .   2577:	// context, otherwise such preemption could occur on function entry to
         .          .   2578:	// startm. Callers passing a nil P may be preemptible, so we must
         .          .   2579:	// disable preemption before acquiring a P from pidleget below.
         .          .   2580:	mp := acquirem()
         .          .   2581:	if !lockheld {
         .          .   2582:		lock(&sched.lock)
         .          .   2583:	}
         .          .   2584:	if pp == nil {
         .          .   2585:		if spinning {
         .          .   2586:			// TODO(prattmic): All remaining calls to this function
         .          .   2587:			// with _p_ == nil could be cleaned up to find a P
         .          .   2588:			// before calling startm.
         .          .   2589:			throw("startm: P required for spinning=true")
         .          .   2590:		}
         .          .   2591:		pp, _ = pidleget(0)
         .          .   2592:		if pp == nil {
         .          .   2593:			if !lockheld {
         .          .   2594:				unlock(&sched.lock)
         .          .   2595:			}
         .          .   2596:			releasem(mp)
         .          .   2597:			return
         .          .   2598:		}
         .          .   2599:	}
         .          .   2600:	nmp := mget()
         .          .   2601:	if nmp == nil {
         .          .   2602:		// No M is available, we must drop sched.lock and call newm.
         .          .   2603:		// However, we already own a P to assign to the M.
         .          .   2604:		//
         .          .   2605:		// Once sched.lock is released, another G (e.g., in a syscall),
         .          .   2606:		// could find no idle P while checkdead finds a runnable G but
         .          .   2607:		// no running M's because this new M hasn't started yet, thus
         .          .   2608:		// throwing in an apparent deadlock.
         .          .   2609:		// This apparent deadlock is possible when startm is called
         .          .   2610:		// from sysmon, which doesn't count as a running M.
         .          .   2611:		//
         .          .   2612:		// Avoid this situation by pre-allocating the ID for the new M,
         .          .   2613:		// thus marking it as 'running' before we drop sched.lock. This
         .          .   2614:		// new M will eventually run the scheduler to execute any
         .          .   2615:		// queued G's.
         .          .   2616:		id := mReserveID()
         .          .   2617:		unlock(&sched.lock)
         .          .   2618:
         .          .   2619:		var fn func()
         .          .   2620:		if spinning {
         .          .   2621:			// The caller incremented nmspinning, so set m.spinning in the new M.
         .          .   2622:			fn = mspinning
         .          .   2623:		}
         .          .   2624:		newm(fn, pp, id)
         .          .   2625:
         .          .   2626:		if lockheld {
         .          .   2627:			lock(&sched.lock)
         .          .   2628:		}
         .          .   2629:		// Ownership transfer of pp committed by start in newm.
         .          .   2630:		// Preemption is now safe.
         .          .   2631:		releasem(mp)
         .          .   2632:		return
         .          .   2633:	}
         .          .   2634:	if !lockheld {
         .          .   2635:		unlock(&sched.lock)
         .          .   2636:	}
         .          .   2637:	if nmp.spinning {
         .          .   2638:		throw("startm: m is spinning")
         .          .   2639:	}
         .          .   2640:	if nmp.nextp != 0 {
         .          .   2641:		throw("startm: m has p")
         .          .   2642:	}
         .          .   2643:	if spinning && !runqempty(pp) {
         .          .   2644:		throw("startm: p has runnable gs")
         .          .   2645:	}
         .          .   2646:	// The caller incremented nmspinning, so set m.spinning in the new M.
         .          .   2647:	nmp.spinning = spinning
         .          .   2648:	nmp.nextp.set(pp)
         .       10ms   2649:	notewakeup(&nmp.park)
         .          .   2650:	// Ownership transfer of pp committed by wakeup. Preemption is now
         .          .   2651:	// safe.
         .          .   2652:	releasem(mp)
         .          .   2653:}
         .          .   2654:
ROUTINE ======================== runtime.step in /usr/local/go/src/runtime/symtab.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .   1071:func step(p []byte, pc *uintptr, val *int32, first bool) (newp []byte, ok bool) {
         .          .   1072:	// For both uvdelta and pcdelta, the common case (~70%)
         .          .   1073:	// is that they are a single byte. If so, avoid calling readvarint.
         .          .   1074:	uvdelta := uint32(p[0])
      10ms       10ms   1075:	if uvdelta == 0 && !first {
         .          .   1076:		return nil, false
         .          .   1077:	}
         .          .   1078:	n := uint32(1)
         .          .   1079:	if uvdelta&0x80 != 0 {
         .          .   1080:		n, uvdelta = readvarint(p)
ROUTINE ======================== runtime.sweepone in /usr/local/go/src/runtime/mgcsweep.go
         0       10ms (flat, cum)  0.07% of Total
         .          .    356:func sweepone() uintptr {
         .          .    357:	gp := getg()
         .          .    358:
         .          .    359:	// Increment locks to ensure that the goroutine is not preempted
         .          .    360:	// in the middle of sweep thus leaving the span in an inconsistent state for next GC
         .          .    361:	gp.m.locks++
         .          .    362:
         .          .    363:	// TODO(austin): sweepone is almost always called in a loop;
         .          .    364:	// lift the sweepLocker into its callers.
         .          .    365:	sl := sweep.active.begin()
         .          .    366:	if !sl.valid {
         .          .    367:		gp.m.locks--
         .          .    368:		return ^uintptr(0)
         .          .    369:	}
         .          .    370:
         .          .    371:	// Find a span to sweep.
         .          .    372:	npages := ^uintptr(0)
         .          .    373:	var noMoreWork bool
         .          .    374:	for {
         .          .    375:		s := mheap_.nextSpanForSweep()
         .          .    376:		if s == nil {
         .          .    377:			noMoreWork = sweep.active.markDrained()
         .          .    378:			break
         .          .    379:		}
         .          .    380:		if state := s.state.get(); state != mSpanInUse {
         .          .    381:			// This can happen if direct sweeping already
         .          .    382:			// swept this span, but in that case the sweep
         .          .    383:			// generation should always be up-to-date.
         .          .    384:			if !(s.sweepgen == sl.sweepGen || s.sweepgen == sl.sweepGen+3) {
         .          .    385:				print("runtime: bad span s.state=", state, " s.sweepgen=", s.sweepgen, " sweepgen=", sl.sweepGen, "\n")
         .          .    386:				throw("non in-use span in unswept list")
         .          .    387:			}
         .          .    388:			continue
         .          .    389:		}
         .          .    390:		if s, ok := sl.tryAcquire(s); ok {
         .          .    391:			// Sweep the span we found.
         .          .    392:			npages = s.npages
         .       10ms    393:			if s.sweep(false) {
         .          .    394:				// Whole span was freed. Count it toward the
         .          .    395:				// page reclaimer credit since these pages can
         .          .    396:				// now be used for span allocation.
         .          .    397:				mheap_.reclaimCredit.Add(npages)
         .          .    398:			} else {
ROUTINE ======================== runtime.systemstack in /usr/local/go/src/runtime/asm_amd64.s
      20ms      1.49s (flat, cum) 10.45% of Total
         .          .    478:TEXT runtime·systemstack(SB), NOSPLIT, $0-8
      10ms       10ms    479:	MOVQ	fn+0(FP), DI	// DI = fn
         .          .    480:	get_tls(CX)
         .          .    481:	MOVQ	g(CX), AX	// AX = g
         .          .    482:	MOVQ	g_m(AX), BX	// BX = m
         .          .    483:
         .          .    484:	CMPQ	AX, m_gsignal(BX)
         .          .    485:	JEQ	noswitch
         .          .    486:
         .          .    487:	MOVQ	m_g0(BX), DX	// DX = g0
         .          .    488:	CMPQ	AX, DX
         .          .    489:	JEQ	noswitch
         .          .    490:
         .          .    491:	CMPQ	AX, m_curg(BX)
         .          .    492:	JNE	bad
         .          .    493:
         .          .    494:	// Switch stacks.
         .          .    495:	// The original frame pointer is stored in BP,
         .          .    496:	// which is useful for stack unwinding.
         .          .    497:	// Save our state in g->sched. Pretend to
         .          .    498:	// be systemstack_switch if the G stack is scanned.
         .          .    499:	CALL	gosave_systemstack_switch<>(SB)
         .          .    500:
         .          .    501:	// switch to g0
         .          .    502:	MOVQ	DX, g(CX)
         .          .    503:	MOVQ	DX, R14 // set the g register
         .          .    504:	MOVQ	(g_sched+gobuf_sp)(DX), SP
         .          .    505:
         .          .    506:	// call target function
      10ms       10ms    507:	MOVQ	DI, DX
         .          .    508:	MOVQ	0(DI), DI
         .      1.47s    509:	CALL	DI
         .          .    510:
         .          .    511:	// switch back to g
         .          .    512:	get_tls(CX)
         .          .    513:	MOVQ	g(CX), AX
         .          .    514:	MOVQ	g_m(AX), BX
ROUTINE ======================== runtime.tgkill in /usr/local/go/src/runtime/sys_linux_amd64.s
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    171:TEXT ·tgkill(SB),NOSPLIT,$0
         .          .    172:	MOVQ	tgid+0(FP), DI
         .          .    173:	MOVQ	tid+8(FP), SI
         .          .    174:	MOVQ	sig+16(FP), DX
         .          .    175:	MOVL	$SYS_tgkill, AX
         .          .    176:	SYSCALL
      10ms       10ms    177:	RET
         .          .    178:
         .          .    179:TEXT runtime·setitimer(SB),NOSPLIT,$0-24
         .          .    180:	MOVL	mode+0(FP), DI
         .          .    181:	MOVQ	new+8(FP), SI
         .          .    182:	MOVQ	old+16(FP), DX
ROUTINE ======================== runtime.traceEnabled in /usr/local/go/src/runtime/trace.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    262:func traceEnabled() bool {
      10ms       10ms    263:	return trace.enabled
         .          .    264:}
         .          .    265:
         .          .    266:// traceShuttingDown returns true if the trace is currently shutting down.
         .          .    267://
         .          .    268://go:nosplit
ROUTINE ======================== runtime.unlock in /usr/local/go/src/runtime/lock_futex.go
         0      920ms (flat, cum)  6.45% of Total
         .          .    111:func unlock(l *mutex) {
         .      920ms    112:	unlockWithRank(l)
         .          .    113:}
         .          .    114:
         .          .    115:func unlock2(l *mutex) {
         .          .    116:	v := atomic.Xchg(key32(&l.key), mutex_unlocked)
         .          .    117:	if v == mutex_unlocked {
ROUTINE ======================== runtime.unlock2 in /usr/local/go/src/runtime/lock_futex.go
         0      920ms (flat, cum)  6.45% of Total
         .          .    115:func unlock2(l *mutex) {
         .          .    116:	v := atomic.Xchg(key32(&l.key), mutex_unlocked)
         .          .    117:	if v == mutex_unlocked {
         .          .    118:		throw("unlock of unlocked lock")
         .          .    119:	}
         .          .    120:	if v == mutex_sleeping {
         .      920ms    121:		futexwakeup(key32(&l.key), 1)
         .          .    122:	}
         .          .    123:
         .          .    124:	gp := getg()
         .          .    125:	gp.m.locks--
         .          .    126:	if gp.m.locks < 0 {
ROUTINE ======================== runtime.unlockWithRank in /usr/local/go/src/runtime/lockrank_off.go
         0      920ms (flat, cum)  6.45% of Total
         .          .     33:func unlockWithRank(l *mutex) {
         .      920ms     34:	unlock2(l)
         .          .     35:}
         .          .     36:
         .          .     37:// This function may be called in nosplit context and thus must be nosplit.
         .          .     38://
         .          .     39://go:nosplit
ROUTINE ======================== runtime.wakep in /usr/local/go/src/runtime/proc.go
      20ms       50ms (flat, cum)  0.35% of Total
         .          .   2729:func wakep() {
         .          .   2730:	// Be conservative about spinning threads, only start one if none exist
         .          .   2731:	// already.
      20ms       20ms   2732:	if sched.nmspinning.Load() != 0 || !sched.nmspinning.CompareAndSwap(0, 1) {
         .          .   2733:		return
         .          .   2734:	}
         .          .   2735:
         .          .   2736:	// Disable preemption until ownership of pp transfers to the next M in
         .          .   2737:	// startm. Otherwise preemption here would leave pp stuck waiting to
         .          .   2738:	// enter _Pgcstop.
         .          .   2739:	//
         .          .   2740:	// See preemption comment on acquirem in startm for more details.
         .          .   2741:	mp := acquirem()
         .          .   2742:
         .          .   2743:	var pp *p
         .       20ms   2744:	lock(&sched.lock)
         .          .   2745:	pp, _ = pidlegetSpinning(0)
         .          .   2746:	if pp == nil {
         .          .   2747:		if sched.nmspinning.Add(-1) < 0 {
         .          .   2748:			throw("wakep: negative nmspinning")
         .          .   2749:		}
         .          .   2750:		unlock(&sched.lock)
         .          .   2751:		releasem(mp)
         .          .   2752:		return
         .          .   2753:	}
         .          .   2754:	// Since we always have a P, the race in the "No M is available"
         .          .   2755:	// comment in startm doesn't apply during the small window between the
         .          .   2756:	// unlock here and lock in startm. A checkdead in between will always
         .          .   2757:	// see at least one running M (ours).
         .          .   2758:	unlock(&sched.lock)
         .          .   2759:
         .       10ms   2760:	startm(pp, true, false)
         .          .   2761:
         .          .   2762:	releasem(mp)
         .          .   2763:}
         .          .   2764:
         .          .   2765:// Stops execution of the current m that is locked to a g until the g is runnable again.
ROUTINE ======================== runtime.writeHeapBits.flush in /usr/local/go/src/runtime/mbitmap.go
      20ms       20ms (flat, cum)  0.14% of Total
         .          .    847:func (h writeHeapBits) flush(addr, size uintptr) {
         .          .    848:	// zeros counts the number of bits needed to represent the object minus the
         .          .    849:	// number of bits we've already written. This is the number of 0 bits
         .          .    850:	// that need to be added.
         .          .    851:	zeros := (addr+size-h.addr)/goarch.PtrSize - h.valid
         .          .    852:
         .          .    853:	// Add zero bits up to the bitmap word boundary
         .          .    854:	if zeros > 0 {
         .          .    855:		z := ptrBits - h.valid
         .          .    856:		if z > zeros {
         .          .    857:			z = zeros
         .          .    858:		}
         .          .    859:		h.valid += z
         .          .    860:		zeros -= z
         .          .    861:	}
         .          .    862:
         .          .    863:	// Find word in bitmap that we're going to write.
      10ms       10ms    864:	ai := arenaIndex(h.addr)
         .          .    865:	ha := mheap_.arenas[ai.l1()][ai.l2()]
         .          .    866:	idx := h.addr / (ptrBits * goarch.PtrSize) % heapArenaBitmapWords
         .          .    867:
         .          .    868:	// Write remaining bits.
         .          .    869:	if h.valid != h.low {
         .          .    870:		m := uintptr(1)<<h.low - 1      // don't clear existing bits below "low"
         .          .    871:		m |= ^(uintptr(1)<<h.valid - 1) // don't clear existing bits above "valid"
      10ms       10ms    872:		ha.bitmap[idx] = ha.bitmap[idx]&m | h.mask
         .          .    873:	}
         .          .    874:	if zeros == 0 {
         .          .    875:		return
         .          .    876:	}
         .          .    877:
ROUTINE ======================== runtime.writeHeapBits.write in /usr/local/go/src/runtime/mbitmap.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    795:func (h writeHeapBits) write(bits, valid uintptr) writeHeapBits {
         .          .    796:	if h.valid+valid <= ptrBits {
         .          .    797:		// Fast path - just accumulate the bits.
      10ms       10ms    798:		h.mask |= bits << h.valid
         .          .    799:		h.valid += valid
         .          .    800:		return h
         .          .    801:	}
         .          .    802:	// Too many bits to fit in this word. Write the current word
         .          .    803:	// out and move on to the next word.
ROUTINE ======================== runtime/internal/atomic.(*Uint64).Add in /usr/local/go/src/runtime/internal/atomic/types.go
      10ms       10ms (flat, cum)  0.07% of Total
         .          .    343:func (u *Uint64) Add(delta int64) uint64 {
      10ms       10ms    344:	return Xadd64(&u.value, delta)
         .          .    345:}
         .          .    346:
         .          .    347:// Uintptr is an atomically accessed uintptr value.
         .          .    348://
         .          .    349:// A Uintptr must not be copied.
